[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Differentiable Epidemiology",
    "section": "",
    "text": "0.1 Who this is for\nThis handbook introduces practical inference workflows for infectious-disease models, with an emphasis on:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#who-this-is-for",
    "href": "index.html#who-this-is-for",
    "title": "Differentiable Epidemiology",
    "section": "",
    "text": "Researchers and practitioners who want to understand inference for epidemic models without relying on opaque tooling.\nReaders comfortable with basic probability and willing to work through small, executable examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#reading-paths",
    "href": "index.html#reading-paths",
    "title": "Differentiable Epidemiology",
    "section": "0.2 Reading paths",
    "text": "0.2 Reading paths\n\nPractical path: running-example → classical-baselines → likelihood-free-baselines → validation-and-robustness.\nGradient-first path: autodiff-basics → differentiability-axis → gradient-estimators → modern-sbi.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "Differentiable Epidemiology",
    "section": "0.3 Reproducibility",
    "text": "0.3 Reproducibility\n\nThe companion package is intentionally lightweight and tested.\nThe same source renders to both HTML and PDF.\nCI executes the HTML build to keep examples reproducible over time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "running-example.html",
    "href": "running-example.html",
    "title": "2  Running example: SEIR + reporting",
    "section": "",
    "text": "This page executes a minimal deterministic SEIR + reporting pipeline using the companion Python package.\nScope:\n\nDeterministic SEIR simulation (Euler discretisation).\nA paired stochastic SEIR variant (tau-leaping; non-differentiable).\nAn observation model: incidence derived from the drop in susceptibles, then reported cases via under-reporting, a delay distribution, and NB noise.\nA tiny paired pipeline that produces TimeSeriesDataset objects for both deterministic and stochastic simulations.\n\nAssumptions / parameterisation:\n\nCompartments are continuous (real-valued) and closed: births/deaths are ignored, so total population is conserved.\nbeta is the effective transmission rate, sigma = 1 / latent_period, gamma = 1 / infectious_period.\nThe solver is a simple forward Euler discretisation with step size dt (smaller dt is more stable).\n\nObservation model (per time step):\n\nIncidence: \\(i_t = \\max(S_t - S_{t+1}, 0)\\)\nUnder-reporting: \\(\\tilde{i}_t = \\rho\\, i_t\\)\nDelay: \\(\\mu_t = \\sum_{d\\ge 0} \\tilde{i}_{t-d} w_d\\), where \\(w_d\\) is a discrete delay PMF.\nNoise: \\(y_t \\sim \\text{NegBin}(\\mu_t, \\kappa)\\) with \\(\\mathrm{Var}(y_t)=\\mu_t + \\mu_t^2/\\kappa\\).\n\n\nimport numpy as np\n\nfrom diff_epi_inference import (\n    SEIRParams,\n    TimeSeriesDataset,\n    discrete_gamma_delay_pmf,\n    expected_reported_cases_delayed,\n    incidence_from_susceptibles,\n    nbinom_loglik,\n    plot_observation_overlay,\n    plot_seir_compartments,\n    plot_two_timeseries,\n    sample_nbinom_reports,\n    simulate_seir_and_report_deterministic,\n    simulate_seir_and_report_stochastic,\n    simulate_seir_euler,\n)\n\n\n# Use a higher transmission rate and a longer horizon so the epidemic wave\n# rises and declines within the simulation window.\nsim_steps = 300\nparams = SEIRParams(beta=2.2, sigma=1 / 5, gamma=1 / 7)\nout = simulate_seir_euler(\n    params=params,\n    s0=999.0,\n    e0=0.0,\n    i0=1.0,\n    r0=0.0,\n    dt=0.2,\n    steps=sim_steps,\n)\n\nt = out[\"t\"]\nS, E, I, R = out[\"S\"], out[\"E\"], out[\"I\"], out[\"R\"]\n\nfig, ax = plot_seir_compartments(\n    t=t,\n    S=S,\n    E=E,\n    I_series=I,\n    R=R,\n    title=\"SEIR (Euler discretisation)\",\n)\nfig\n\nassert np.allclose(S + E + I + R, S[0] + E[0] + I[0] + R[0])\n\n\n\n\n\n\n\n\n\ninc = incidence_from_susceptibles(S)\n\nrho = 0.3\nw = discrete_gamma_delay_pmf(shape=2.0, scale=1.0, max_delay=20)\nmu = expected_reported_cases_delayed(incidence=inc, reporting_rate=rho, delay_pmf=w)\n\ndispersion = 20.0\nrng = np.random.default_rng(0)\ny = sample_nbinom_reports(expected=mu, dispersion=dispersion, rng=rng)\nll = nbinom_loglik(y=y, mu=mu, dispersion=dispersion)\n\nds_det = TimeSeriesDataset(t=t[1:], y=y, name=\"reported (deterministic)\")\n\nfig, ax = plot_observation_overlay(\n    t=ds_det.t,\n    observed=ds_det.y,\n    expected=mu,\n    observed_label=ds_det.name,\n    expected_label=\"E[reported] (delay + rho)\",\n    title=\"Observation model\",\n)\nfig\n\nassert np.isfinite(ll)\nassert inc.shape == mu.shape == ds_det.y.shape\n\n\n\n\n\n\n\n\n\nds_det2 = simulate_seir_and_report_deterministic(\n    params=params,\n    s0=999.0,\n    e0=0.0,\n    i0=1.0,\n    r0=0.0,\n    dt=0.2,\n    steps=sim_steps,\n    reporting_rate=rho,\n    rng=np.random.default_rng(1),\n    name=\"reported (det pipeline)\",\n)\n\nds_sto2 = simulate_seir_and_report_stochastic(\n    params=params,\n    s0=999,\n    e0=0,\n    i0=1,\n    r0=0,\n    dt=0.2,\n    steps=sim_steps,\n    reporting_rate=rho,\n    rng=np.random.default_rng(2),\n    name=\"reported (stoch pipeline)\",\n)\n\nfig, ax = plot_two_timeseries(\n    t=ds_det2.t,\n    y1=ds_det2.y,\n    y2=ds_sto2.y,\n    label1=ds_det2.name,\n    label2=ds_sto2.name,\n    title=\"Paired deterministic vs stochastic reported cases\",\n    ylabel=\"cases / step\",\n)\nfig\n\nassert ds_det2.t.shape == ds_det2.y.shape == ds_sto2.t.shape == ds_sto2.y.shape",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Running example: SEIR + reporting</span>"
    ]
  },
  {
    "objectID": "classical-baselines.html",
    "href": "classical-baselines.html",
    "title": "3  Classical baselines",
    "section": "",
    "text": "3.1 Chapter map\nThis chapter introduces amortisation-free inference baselines for the SEIR running example. These serve as anchors for later SBI/VI methods.\nThe diagnostics and algorithm choices here follow standard MCMC references for HMC/NUTS and convergence assessment (Neal 2011; Hoffman and Gelman 2014; Vehtari et al. 2021).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical baselines</span>"
    ]
  },
  {
    "objectID": "classical-baselines.html#chapter-map",
    "href": "classical-baselines.html#chapter-map",
    "title": "3  Classical baselines",
    "section": "",
    "text": "Metropolis-Hastings baseline for a one-parameter posterior.\nFinite-difference HMC as a pedagogical gradient bridge.\nOptional autodiff NUTS baseline (BlackJAX).\nShared validation pieces: traces, PPC, and small coverage checks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical baselines</span>"
    ]
  },
  {
    "objectID": "classical-baselines.html#minimal-mh-demo-sampling-a-1d-gaussian",
    "href": "classical-baselines.html#minimal-mh-demo-sampling-a-1d-gaussian",
    "title": "3  Classical baselines",
    "section": "3.2 Minimal MH demo: sampling a 1D Gaussian",
    "text": "3.2 Minimal MH demo: sampling a 1D Gaussian\nBefore applying MH to the SEIR model, it helps to sanity-check the implementation on a distribution where we know the answer. Below we sample from a standard normal target density using a Gaussian random-walk proposal.\n\nimport os\nimport numpy as np\n\nCI_PROFILE = os.environ.get(\"QUARTO_PROFILE\") == \"ci\" or os.environ.get(\"CI\") == \"true\"\n\n\ndef ci(default: int, ci_value: int) -&gt; int:\n    \"\"\"Use smaller settings when rendering under the Quarto CI profile.\"\"\"\n    return ci_value if CI_PROFILE else default\n\n\nfrom diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings\n\n\ndef log_prob_standard_normal(x: np.ndarray) -&gt; float:\n    # Up to an additive constant: log N(x; 0, 1)\n    return -0.5 * float(np.sum(x**2))\n\n\nrng = np.random.default_rng(0)\nres = random_walk_metropolis_hastings(\n    log_prob_standard_normal,\n    x0=np.array([5.0]),\n    proposal_std=1.0,\n    n_steps=ci(5_000, 1_000),\n    rng=rng,\n)\n\naccept_rate = res.accept_rate\nmean = float(np.mean(res.chain))\nstd = float(np.std(res.chain))\n\naccept_rate, mean, std\n\n(0.678, 0.07636268305930825, 1.1065715301835828)\n\n\nThe acceptance rate depends strongly on the proposal scale; in 1D, a value around 20–60% is typical for a reasonable random-walk step size.\n\n3.2.1 Tuning proposal_std (random-walk MH)\nFor a Gaussian random-walk MH proposal, the proposal scale controls the trade-off between:\n\nToo small proposal_std → very high acceptance but tiny moves (high autocorrelation).\nToo large proposal_std → very low acceptance (many repeats).\n\nPractical tuning guidance:\n\nIn 1D, an acceptance rate around 0.3–0.6 is often fine; the asymptotic optimum for a Gaussian target is about 0.44.\nIn higher dimensions, the optimal random-walk acceptance rate tends to be lower (often quoted around 0.2–0.3, with ~0.234 as a classical asymptotic result for i.i.d. Gaussian targets).\nA simple workflow is:\n\nRun a short warmup (e.g. 500–2,000 steps).\nAdjust proposal_std up/down to hit a rough target acceptance.\nFreeze the proposal scale for the main run (avoid adapting during sampling if you want a strictly Markov chain).\n\n\nIn this chapter we keep tuning manual and lightweight; later chapters can revisit more systematic adaptation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical baselines</span>"
    ]
  },
  {
    "objectID": "classical-baselines.html#minimal-mh-demo-infer-only-beta-in-the-seir-running-example",
    "href": "classical-baselines.html#minimal-mh-demo-infer-only-beta-in-the-seir-running-example",
    "title": "3  Classical baselines",
    "section": "3.3 Minimal MH demo: infer only beta in the SEIR running example",
    "text": "3.3 Minimal MH demo: infer only beta in the SEIR running example\nAs a first epidemiological baseline, we infer only the transmission rate beta, keeping the remaining parameters fixed. This is intentionally a small, well-posed problem: a 1D posterior that still exercises the full simulation + observation likelihood.\nWe will:\n\nsimulate a synthetic observed time series y from a known beta_true\ndefine a log posterior in terms of log_beta = log(beta) (so proposals live on ℝ)\nrun random-walk MH and inspect the resulting posterior for beta\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom diff_epi_inference import (\n    SEIRParams,\n    discrete_gamma_delay_pmf,\n    expected_reported_cases_delayed,\n    incidence_from_susceptibles,\n    nbinom_loglik,\n    plot_observation_overlay,\n    sample_nbinom_reports,\n    simulate_seir_euler,\n)\nfrom diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings\n\n# --- Fixed settings (match the running example defaults) ---\n\nsigma_fixed = 1 / 5\ngamma_fixed = 1 / 7\n\ns0, e0, i0, r0 = 999.0, 0.0, 1.0, 0.0\n\ndt = 0.2\nsteps = ci(200, 120)\n\nreporting_rate = 0.3\nw = discrete_gamma_delay_pmf(shape=2.0, scale=1.0, max_delay=20)\ndispersion = 20.0\n\n# --- Synthetic data ---\n\nbeta_true = 0.6\nparams_true = SEIRParams(beta=beta_true, sigma=sigma_fixed, gamma=gamma_fixed)\n\nout_true = simulate_seir_euler(\n    params=params_true,\n    s0=s0,\n    e0=e0,\n    i0=i0,\n    r0=r0,\n    dt=dt,\n    steps=steps,\n)\n\ninc_true = incidence_from_susceptibles(out_true[\"S\"])\nmu_true = expected_reported_cases_delayed(\n    incidence=inc_true,\n    reporting_rate=reporting_rate,\n    delay_pmf=w,\n)\n\nrng = np.random.default_rng(0)\ny_obs = sample_nbinom_reports(expected=mu_true, dispersion=dispersion, rng=rng)\n\n\n# --- Log posterior for log(beta) ---\n\n# Prior parameterisation note\n#\n# We place a Gaussian prior directly on log_beta = log(beta):\n#   log_beta ~ Normal(logbeta_prior_mean, logbeta_prior_sd)\n# This implies a lognormal prior on beta on its constrained (positive) support.\n# Because we sample in log space, we evaluate the prior density in terms of\n# log_beta, not beta; no extra Jacobian term is needed in the log posterior.\n#\n# In both MH and HMC, the algorithms only use *differences* in log density, so\n# we routinely drop additive normalisation constants.\n\nfrom diff_epi_inference.models.seir_numpy_beta_only import make_log_post_logbeta_numpy\n\nlogbeta_prior_mean = float(np.log(0.5))\nlogbeta_prior_sd = 0.5\n\nlog_post_logbeta = make_log_post_logbeta_numpy(\n    y_obs=y_obs,\n    w_delay_pmf=w,\n    sigma=sigma_fixed,\n    gamma=gamma_fixed,\n    s0=s0,\n    e0=e0,\n    i0=i0,\n    r0=r0,\n    dt=dt,\n    steps=steps,\n    reporting_rate=reporting_rate,\n    dispersion=dispersion,\n    logbeta_prior_mean=logbeta_prior_mean,\n    logbeta_prior_sd=logbeta_prior_sd,\n)\n\n\nres = random_walk_metropolis_hastings(\n    log_post_logbeta,\n    x0=np.array([np.log(0.3)]),\n    proposal_std=0.10,\n    n_steps=ci(3_000, 800),\n    rng=np.random.default_rng(1),\n)\n\nbeta_chain = np.exp(res.chain[:, 0])\n# Crude burn-in for this demo (keep it robust to shorter CI runs).\nburn = min(500, beta_chain.shape[0] // 2)\nbeta_chain = beta_chain[burn:]\n\naccept_rate = res.accept_rate\nbeta_mean = float(np.mean(beta_chain))\nbeta_q05, beta_q95 = [float(q) for q in np.quantile(beta_chain, [0.05, 0.95])]\n\naccept_rate, beta_mean, beta_q05, beta_q95\n\n(0.4975, 0.5988469198925787, 0.5507517063915722, 0.6461143508343143)\n\n\nA quick visual check:\n\nfrom diff_epi_inference.plotting.mcmc import plot_trace_and_acf\n\nfig, _ = plot_trace_and_acf(\n    beta_chain,\n    true_value=beta_true,\n    max_lag=60,\n    title=\"MH trace for beta (others fixed)\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n3.3.1 Posterior predictive check (MH)\nA minimal posterior predictive check samples simulated observations from the posterior draws, then overlays predictive bands against the observed time series.\n\ndef ppc_from_beta_draws(beta_draws: np.ndarray, rng: np.random.Generator, n_draws: int = 50):\n    beta_draws = np.asarray(beta_draws)\n    if beta_draws.ndim != 1:\n        raise ValueError(\"beta_draws must be 1D\")\n\n    n_draws = int(min(n_draws, beta_draws.shape[0]))\n    idx = rng.choice(beta_draws.shape[0], size=n_draws, replace=False)\n\n    y_rep = []\n    for beta in beta_draws[idx]:\n        params = SEIRParams(beta=float(beta), sigma=sigma_fixed, gamma=gamma_fixed)\n        out = simulate_seir_euler(\n            params=params,\n            s0=s0,\n            e0=e0,\n            i0=i0,\n            r0=r0,\n            dt=dt,\n            steps=steps,\n        )\n        inc = incidence_from_susceptibles(out[\"S\"])\n        mu = expected_reported_cases_delayed(\n            incidence=inc,\n            reporting_rate=reporting_rate,\n            delay_pmf=w,\n        )\n        y_rep.append(sample_nbinom_reports(expected=mu, dispersion=dispersion, rng=rng))\n\n    y_rep = np.asarray(y_rep)\n    q05, q50, q95 = np.quantile(y_rep, [0.05, 0.5, 0.95], axis=0)\n    return q05, q50, q95\n\n\nq05_mh, q50_mh, q95_mh = ppc_from_beta_draws(\n    beta_chain,\n    rng=np.random.default_rng(123),\n    n_draws=ci(60, 20),\n)\n\nt = np.arange(len(y_obs)) * dt\n\nfig, ax = plot_observation_overlay(\n    t=t,\n    observed=y_obs,\n    expected=q50_mh,\n    observed_label=\"observed y\",\n    expected_label=\"posterior predictive median\",\n    title=\"PPC (MH; beta only)\",\n)\nax.fill_between(t, q05_mh, q95_mh, alpha=0.2, label=\"posterior predictive 90%\")\nax.legend(loc=\"upper right\")\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 2) Random-walk Metropolis–Hastings\n\nAlgorithm: Gaussian random-walk proposals in unconstrained space.\nImplementation notes:\n\nTransform constrained parameters (positivity, simplex) to ℝ.\nAdapt proposal scale during warmup only; then fix.\n\nOutputs:\n\nAcceptance rate\nAutocorrelation / effective sample size",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical baselines</span>"
    ]
  },
  {
    "objectID": "classical-baselines.html#minimal-hmc-demo-infer-only-beta-finite-difference-gradients",
    "href": "classical-baselines.html#minimal-hmc-demo-infer-only-beta-finite-difference-gradients",
    "title": "3  Classical baselines",
    "section": "3.4 Minimal HMC demo: infer only beta (finite-difference gradients)",
    "text": "3.4 Minimal HMC demo: infer only beta (finite-difference gradients)\nFor a first gradient-based baseline, we can run HMC on the same 1D posterior as above. In a production setting, you would normally use autodiff + NUTS (e.g. NumPyro/BlackJAX) so that gradients are exact (up to solver tolerance) and step sizes / trajectory lengths are adapted automatically.\nHere we use a deliberately minimal HMC implementation that estimates gradients by central finite differences.\nLimitations of this baseline (important):\n\nComputational cost: finite-difference gradients require 2 evaluations per parameter per leapfrog step. Even in 1D this adds up; in higher dimensions it quickly becomes impractical.\nNumerical sensitivity: the finite-difference step size (grad_eps) must be tuned; too small → cancellation / noise, too large → biased gradients.\nNo adaptation / diagnostics: this minimal HMC does not adapt step sizes or trajectory lengths and does not report NUTS-style diagnostics (divergences, tree depth, energy).\n\n\n3.4.1 Tuning grad_eps (finite-difference gradient step size)\ngrad_eps controls the central-difference approximation\n\\[\n\\nabla f(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2\\varepsilon}.\n\\]\nA practical workflow:\n\nWork in unconstrained parameter space (we do: x = log_beta). grad_eps is measured in that space.\nStart with a conservative default like 1e-4 to 1e-3.\nSanity-check sensitivity by rerunning a short chain with grad_eps scaled by 10× (e.g. 1e-5, 1e-4, 1e-3) and verifying that posterior summaries (mean/interval) and acceptance rate are broadly stable.\n\nHow to spot problems:\n\nToo small: gradients become dominated by numerical cancellation/roundoff. You may see very noisy acceptance rates, sticky chains, or you need an implausibly tiny step_size to avoid rejections.\nToo large: gradients are biased (you are differentiating a smoothed/linearised version of the log density). Chains may mix but settle on the wrong posterior (summaries shift noticeably as you change grad_eps).\n\nDespite these caveats, it is a useful pedagogical bridge between MH and a full autodiff + NUTS workflow.\n\n\n3.4.2 Tuning step_size and n_leapfrog (HMC)\nHMC performance is controlled mainly by two hyperparameters:\n\nstep_size (“epsilon”): the size of each leapfrog integrator step.\nn_leapfrog (“L”): the number of leapfrog steps per proposal (trajectory length is roughly step_size * n_leapfrog).\n\nA lightweight manual tuning workflow:\n\nStart with a short warmup (e.g. 200–1,000 iterations).\nAdjust step_size to hit a reasonable acceptance rate.\n\nIn practice, a target around 0.6–0.9 is common for HMC.\nToo low → reduce step_size (integration error is too large).\nToo high with slow exploration → increase step_size slightly or increase n_leapfrog.\n\nSet n_leapfrog to get proposals that move across the posterior without “U-turning” too quickly. Very large n_leapfrog increases compute cost and can lead to wasted looping.\n\nFor NUTS, you typically target an acceptance probability (often called target_accept) around 0.8 by default, and rely on adaptation to set step sizes.\n\n3.4.2.1 Divergences (NUTS-specific warning sign)\nWhen using NUTS implementations (e.g. NumPyro/Stan/BlackJAX), always check for divergences. Divergences indicate that the integrator is failing in parts of the posterior (geometry is too sharp / step size too large), and the resulting draws may be biased. A common first response is to increase target_accept (smaller step sizes) and/or reparameterise.\n\nfrom diff_epi_inference.mcmc.hmc import hamiltonian_monte_carlo\n\nres_hmc = hamiltonian_monte_carlo(\n    log_post_logbeta,\n    x0=np.array([np.log(0.3)]),\n    n_steps=ci(2_000, 250),\n    step_size=0.02,\n    n_leapfrog=ci(25, 12),\n    grad_eps=1e-4,\n    rng=np.random.default_rng(2),\n)\n\nbeta_chain_hmc = np.exp(res_hmc.chain[:, 0])\n# Crude burn-in (robust to shorter CI runs).\nburn_hmc = min(500, beta_chain_hmc.shape[0] // 2)\nbeta_chain_hmc = beta_chain_hmc[burn_hmc:]\n\naccept_rate_hmc = res_hmc.accept_rate\nbeta_mean_hmc = float(np.mean(beta_chain_hmc))\nbeta_q05_hmc, beta_q95_hmc = [float(q) for q in np.quantile(beta_chain_hmc, [0.05, 0.95])]\n\naccept_rate_hmc, beta_mean_hmc, beta_q05_hmc, beta_q95_hmc\n\n(0.98, 0.6045650427730066, 0.5574625949880155, 0.6538450521443441)\n\n\nA quick trace plot:\n\nfrom diff_epi_inference.plotting.mcmc import plot_trace_and_acf\n\nfig, _ = plot_trace_and_acf(\n    beta_chain_hmc,\n    true_value=beta_true,\n    max_lag=60,\n    title=\"HMC trace for beta (finite-difference gradients; others fixed)\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Posterior predictive check (HMC)\nUsing the same PPC helper as above:\n\nq05_hmc, q50_hmc, q95_hmc = ppc_from_beta_draws(\n    beta_chain_hmc,\n    rng=np.random.default_rng(456),\n    n_draws=ci(60, 20),\n)\n\nt = np.arange(len(y_obs)) * dt\n\nfig, ax = plot_observation_overlay(\n    t=t,\n    observed=y_obs,\n    expected=q50_hmc,\n    observed_label=\"observed y\",\n    expected_label=\"posterior predictive median\",\n    title=\"PPC (HMC; beta only)\",\n)\nax.fill_between(t, q05_hmc, q95_hmc, alpha=0.2, label=\"posterior predictive 90%\")\nax.legend(loc=\"upper right\")\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 3) HMC / NUTS on the ODE model (recommended path)\n\nUse autodiff through the ODE solver + observation model.\nStart with a small, well-posed parameterisation (avoid pathological geometry).\nOutputs:\n\nStep size / tree depth (NUTS)\nDivergences / energy diagnostics\n\n\n\n3.4.4.1 Autodiff + NUTS via BlackJAX\nBelow is a minimal sanity check on a 1D standard normal target.\n\nimport numpy as np\nimport jax.numpy as jnp\n\nfrom diff_epi_inference.mcmc.nuts_blackjax import run_blackjax_nuts\n\n\ndef log_prob_standard_normal(x):\n    return -0.5 * jnp.sum(x**2)\n\n\nres_nuts = run_blackjax_nuts(\n    log_prob_standard_normal,\n    x0=np.array([5.0]),\n    num_warmup=ci(200, 50),\n    num_samples=ci(500, 150),\n    seed=0,\n)\n\nfloat(res_nuts.accept_rate), float(np.mean(res_nuts.chain[:, 0])), float(np.std(res_nuts.chain[:, 0]))\n\n(0.5177280902862549, 0.014052588492631912, 1.0223132371902466)\n\n\n\n\n3.4.4.2 Optional: NUTS for the SEIR beta-only posterior (JAX reimplementation)\nTo use NUTS on the SEIR running example, the log posterior must be JAX-traceable. The core simulator in this repo is NumPy-based, so below we re-implement the minimal beta-only likelihood in jax.numpy (Euler step + delay convolution + NB likelihood).\nThis is intentionally scoped to the beta-only demo so we can keep the path self-contained.\nNote: there is also an optional calibration/coverage smoke test for this NUTS path in tests/test_blackjax_nuts_seir_calibration_optional.py. It is skipped unless the nuts extra is installed (and is meant to catch obvious regressions in the JAX reimplementation).\n\nimport numpy as np\n\nfrom diff_epi_inference.mcmc.nuts_blackjax import run_blackjax_nuts\n\nfrom diff_epi_inference.models.seir_jax_beta_only import make_log_post_logbeta_jax\n\nlog_post_logbeta_jax = make_log_post_logbeta_jax(\n    y_obs=np.asarray(y_obs),\n    w_delay_pmf=np.asarray(w, dtype=float),\n    sigma=float(sigma_fixed),\n    gamma=float(gamma_fixed),\n    s0=float(s0),\n    e0=float(e0),\n    i0=float(i0),\n    r0=float(r0),\n    dt=float(dt),\n    steps=int(steps),\n    reporting_rate=float(reporting_rate),\n    dispersion=float(dispersion),\n    logbeta_prior_mean=float(logbeta_prior_mean),\n    logbeta_prior_sd=float(logbeta_prior_sd),\n)\n\nres_nuts_seir = run_blackjax_nuts(\n    log_post_logbeta_jax,\n    x0=np.array([np.log(0.3)]),\n    num_warmup=ci(400, 100),\n    num_samples=ci(800, 250),\n    seed=3,\n)\n\nbeta_chain_nuts = np.exp(res_nuts_seir.chain[:, 0])\naccept_rate_nuts = float(res_nuts_seir.accept_rate)\nbeta_mean_nuts = float(np.mean(beta_chain_nuts))\nbeta_q05_nuts, beta_q95_nuts = [float(q) for q in np.quantile(beta_chain_nuts, [0.05, 0.95])]\n\naccept_rate_nuts, beta_mean_nuts, beta_q05_nuts, beta_q95_nuts\n\n(0.8622193336486816,\n 0.6017205119132996,\n 0.5520565986633301,\n 0.6556851714849471)\n\n\nA quick posterior predictive check:\n\nq05_nuts, q50_nuts, q95_nuts = ppc_from_beta_draws(\n    beta_chain_nuts,\n    rng=np.random.default_rng(789),\n    n_draws=ci(60, 20),\n)\n\nt = np.arange(len(y_obs)) * dt\n\nfig, ax = plot_observation_overlay(\n    t=t,\n    observed=y_obs,\n    expected=q50_nuts,\n    observed_label=\"observed y\",\n    expected_label=\"posterior predictive median\",\n    title=\"PPC (NUTS via BlackJAX; beta only)\",\n)\nax.fill_between(t, q05_nuts, q95_nuts, alpha=0.2, label=\"posterior predictive 90%\")\nax.legend(loc=\"upper right\")\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.5 4) Diagnostics\nIn this project we use a small “core” of diagnostics repeatedly:\n\nTrace plots (and where helpful, autocorrelation) to catch obvious mixing / non-stationarity problems.\nR-hat (split \\(\\\\hat{R}\\)) and ESS as rough, chain-level summaries.\nPosterior predictive checks (PPC) to ensure the fitted model can reproduce key features of the data.\nCalibration / coverage smoke tests on synthetic data.\n\n\n3.4.5.1 Trace plots\nA trace plot is the cheapest sanity check: it should look like a stationary “hairy caterpillar” rather than a drifting trend. In the examples above we used a single chain and inspected the trace visually.\n\n\n3.4.5.2 ESS and R-hat\nEffective sample size (ESS) and split \\(\\\\hat{R}\\) are most meaningful when you run multiple independent chains. With ArviZ we can compute these summaries with a few lines.\n\nimport numpy as np\nimport arviz as az\n\nfrom diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings\n\nn_chains = 4\nn_steps = 2_000\nburn = 500\n\nchains = []\nfor seed in range(n_chains):\n    res_c = random_walk_metropolis_hastings(\n        log_post_logbeta,\n        x0=np.array([np.log(0.3)]),\n        proposal_std=0.10,\n        n_steps=n_steps,\n        rng=np.random.default_rng(100 + seed),\n    )\n    chains.append(np.exp(res_c.chain[burn:, 0]))\n\nbeta_chains = np.stack(chains, axis=0)  # (chain, draw)\n\n# ArviZ expects an InferenceData or an array with chain/draw dims.\nrhat = float(az.rhat(beta_chains))\ness = float(az.ess(beta_chains))\n\nrhat, ess\n\n(1.0032371688840005, 1404.1491120860317)\n\n\nInterpretation pointers (rules of thumb, not laws):\n\n\\(\\\\hat{R}\\) close to 1.00 (e.g. &lt; 1.01–1.05 depending on context) suggests chains are mixing and agree.\nLarger ESS is better; very small ESS indicates heavy autocorrelation and overconfident summaries.\n\nThese summaries do not replace PPC / calibration: it is possible for chains to mix well while the model is wrong.\n\n\n3.4.5.3 Calibration smoke test (MH; beta only)\nA minimal calibration check is to repeat the synthetic-data experiment many times and ask:\n\nIf we generate data at beta_true, does our nominal 90% posterior interval contain beta_true about 90% of the time?\n\nThis is not full simulation-based calibration (SBC), but it is a fast “is anything horribly wrong?” smoke test.\n\nfrom dataclasses import dataclass\n\nfrom diff_epi_inference.models.seir_numpy_beta_only import make_log_post_logbeta_numpy\n\n\n@dataclass\nclass CoverageResult:\n    beta_true: float\n    contained: bool\n    q05: float\n    q95: float\n    accept_rate: float\n\n\ndef make_log_post_logbeta_for_y(y_obs_local: np.ndarray):\n    return make_log_post_logbeta_numpy(\n        y_obs=y_obs_local,\n        w_delay_pmf=w,\n        sigma=sigma_fixed,\n        gamma=gamma_fixed,\n        s0=s0,\n        e0=e0,\n        i0=i0,\n        r0=r0,\n        dt=dt,\n        steps=steps,\n        reporting_rate=reporting_rate,\n        dispersion=dispersion,\n        logbeta_prior_mean=logbeta_prior_mean,\n        logbeta_prior_sd=logbeta_prior_sd,\n    )\n\n\nrng = np.random.default_rng(2026)\n\nbeta_grid = np.array([0.30, 0.40, 0.50, 0.60, 0.70])\nreps_per_beta = ci(2, 1)  # 5 * reps_per_beta datasets\n\nresults: list[CoverageResult] = []\n\nfor beta_true_i in beta_grid:\n    for _ in range(reps_per_beta):\n        params_true_i = SEIRParams(beta=float(beta_true_i), sigma=sigma_fixed, gamma=gamma_fixed)\n        out_true_i = simulate_seir_euler(\n            params=params_true_i,\n            s0=s0,\n            e0=e0,\n            i0=i0,\n            r0=r0,\n            dt=dt,\n            steps=steps,\n        )\n\n        inc_true_i = incidence_from_susceptibles(out_true_i[\"S\"])\n        mu_true_i = expected_reported_cases_delayed(\n            incidence=inc_true_i,\n            reporting_rate=reporting_rate,\n            delay_pmf=w,\n        )\n\n        y_obs_i = sample_nbinom_reports(expected=mu_true_i, dispersion=dispersion, rng=rng)\n        log_post_i = make_log_post_logbeta_for_y(y_obs_i)\n\n        # Short chain: this is a smoke test, not a production run.\n        res_i = random_walk_metropolis_hastings(\n            log_post_i,\n            x0=np.array([np.log(0.5)]),\n            proposal_std=0.10,\n            n_steps=ci(800, 250),\n            rng=rng,\n        )\n\n        beta_chain_i = np.exp(res_i.chain[:, 0])\n        burn_i = min(200, beta_chain_i.shape[0] // 2)\n        beta_chain_i = beta_chain_i[burn_i:]\n        q05_i, q95_i = [float(q) for q in np.quantile(beta_chain_i, [0.05, 0.95])]\n        contained_i = (q05_i &lt;= float(beta_true_i) &lt;= q95_i)\n\n        results.append(\n            CoverageResult(\n                beta_true=float(beta_true_i),\n                contained=bool(contained_i),\n                q05=q05_i,\n                q95=q95_i,\n                accept_rate=float(res_i.accept_rate),\n            )\n        )\n\ncoverage = float(np.mean([r.contained for r in results]))\ncoverage\n\n0.8\n\n\nWe can also break down coverage by beta_true:\n\nfrom collections import defaultdict\n\nby_beta = defaultdict(list)\nfor r in results:\n    by_beta[r.beta_true].append(r.contained)\n\ncoverage_by_beta = {beta: float(np.mean(v)) for beta, v in sorted(by_beta.items())}\ncoverage_by_beta\n\n{0.3: 1.0, 0.4: 0.0, 0.5: 1.0, 0.6: 1.0, 0.7: 1.0}\n\n\n\n\n3.4.5.4 Calibration smoke test (HMC; beta only)\nWe can repeat the same coverage experiment for the minimal HMC baseline. Because HMC here uses finite-difference gradients, we keep the chains very short.\n\nfrom diff_epi_inference.mcmc.hmc import hamiltonian_monte_carlo\n\n\nresults_hmc: list[CoverageResult] = []\n\nfor beta_true_i in beta_grid:\n    for _ in range(reps_per_beta):\n        params_true_i = SEIRParams(beta=float(beta_true_i), sigma=sigma_fixed, gamma=gamma_fixed)\n        out_true_i = simulate_seir_euler(\n            params=params_true_i,\n            s0=s0,\n            e0=e0,\n            i0=i0,\n            r0=r0,\n            dt=dt,\n            steps=steps,\n        )\n\n        inc_true_i = incidence_from_susceptibles(out_true_i[\"S\"])\n        mu_true_i = expected_reported_cases_delayed(\n            incidence=inc_true_i,\n            reporting_rate=reporting_rate,\n            delay_pmf=w,\n        )\n\n        y_obs_i = sample_nbinom_reports(expected=mu_true_i, dispersion=dispersion, rng=rng)\n        log_post_i = make_log_post_logbeta_for_y(y_obs_i)\n\n        res_i = hamiltonian_monte_carlo(\n            log_post_i,\n            x0=np.array([np.log(0.5)]),\n            n_steps=ci(200, 80),\n            step_size=0.02,\n            n_leapfrog=10,\n            grad_eps=1e-4,\n            rng=rng,\n        )\n\n        beta_chain_i = np.exp(res_i.chain[:, 0])\n        burn_i = min(100, beta_chain_i.shape[0] // 2)\n        beta_chain_i = beta_chain_i[burn_i:]\n        q05_i, q95_i = [float(q) for q in np.quantile(beta_chain_i, [0.05, 0.95])]\n        contained_i = (q05_i &lt;= float(beta_true_i) &lt;= q95_i)\n\n        results_hmc.append(\n            CoverageResult(\n                beta_true=float(beta_true_i),\n                contained=bool(contained_i),\n                q05=q05_i,\n                q95=q95_i,\n                accept_rate=float(res_i.accept_rate),\n            )\n        )\n\ncoverage_hmc = float(np.mean([r.contained for r in results_hmc]))\ncoverage_hmc\n\n0.8\n\n\nBreak down coverage by beta_true:\n\nby_beta_hmc = defaultdict(list)\nfor r in results_hmc:\n    by_beta_hmc[r.beta_true].append(r.contained)\n\ncoverage_by_beta_hmc = {beta: float(np.mean(v)) for beta, v in sorted(by_beta_hmc.items())}\ncoverage_by_beta_hmc\n\n{0.3: 1.0, 0.4: 1.0, 0.5: 0.0, 0.6: 1.0, 0.7: 1.0}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical baselines</span>"
    ]
  },
  {
    "objectID": "classical-baselines.html#key-takeaways",
    "href": "classical-baselines.html#key-takeaways",
    "title": "3  Classical baselines",
    "section": "3.5 Key takeaways",
    "text": "3.5 Key takeaways\n\nMH and HMC provide strong non-amortised baselines for debugging model and likelihood behavior.\nEven in small examples, PPC and calibration smoke checks are essential alongside trace diagnostics.\nOptional NUTS (autodiff-based) is the most practical production baseline when dependencies are available.\n\n\n\n\n\nHoffman, Matthew D, and Andrew Gelman. 2014. “The No-u-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” Journal of Machine Learning Research 15 (47): 1593–623.\n\n\nNeal, Radford M. 2011. MCMC Using Hamiltonian Dynamics. Edited by Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Chapman; Hall/CRC.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved \\(\\hat{R}\\) for Assessing Convergence of MCMC.” Bayesian Analysis 16 (2): 667–718.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Classical baselines</span>"
    ]
  },
  {
    "objectID": "variational-inference.html",
    "href": "variational-inference.html",
    "title": "4  Variational inference",
    "section": "",
    "text": "4.1 Chapter map\nThis chapter introduces variational inference (VI): an optimisation-based alternative to MCMC.\nIn differentiable epidemiology, VI is useful when you have a differentiable (approximate) likelihood and want:\nThis chapter follows the standard VI treatment in (Blei, Kucukelbir, and McAuliffe 2017; Kucukelbir et al. 2017).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#chapter-map",
    "href": "variational-inference.html#chapter-map",
    "title": "4  Variational inference",
    "section": "",
    "text": "VI in one diagram\nELBO derivation and interpretation\nVariational-family choices\nOptimisation strategies\nNormal-normal worked example\nSEIR beta-only mean-field VI example\nPlacement relative to MCMC and SBI",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#vi-in-one-diagram",
    "href": "variational-inference.html#vi-in-one-diagram",
    "title": "4  Variational inference",
    "section": "4.2 1) VI in one diagram",
    "text": "4.2 1) VI in one diagram\nWe want a posterior\n\\[p(\\theta\\mid y) \\propto p(y\\mid\\theta)\\,p(\\theta),\\]\nbut direct computation is hard. In VI we pick a tractable family of densities \\(\\{q_\\phi(\\theta)\\}\\) and solve\n\\[q_{\\phi^*}(\\theta) \\approx p(\\theta\\mid y)\\]\nby optimisation.\nA common objective is to minimise the reverse KL:\n\\[\\phi^* = \\arg\\min_\\phi\\; \\mathrm{KL}\\bigl(q_\\phi(\\theta)\\;\\|\\;p(\\theta\\mid y)\\bigr).\\]\n\n\n\n\n\n\nNote\n\n\n\nReverse KL typically prefers mode-seeking approximations. This can be a feature (fast, stable) or a bug (under-dispersed posteriors), depending on the task.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#the-elbo",
    "href": "variational-inference.html#the-elbo",
    "title": "4  Variational inference",
    "section": "4.3 2) The ELBO",
    "text": "4.3 2) The ELBO\nStart from:\n\\[\\log p(y) = \\log \\int p(y,\\theta)\\,d\\theta.\\]\nMultiply and divide by \\(q_\\phi(\\theta)\\):\n\\[\\log p(y) = \\log \\int q_\\phi(\\theta)\\,\\frac{p(y,\\theta)}{q_\\phi(\\theta)}\\,d\\theta\n= \\log\\,\\mathbb{E}_{q_\\phi}\\left[\\frac{p(y,\\theta)}{q_\\phi(\\theta)}\\right].\\]\nApplying Jensen’s inequality gives the evidence lower bound (ELBO):\n\\[\\log p(y) \\ge \\mathcal{L}(\\phi)\n= \\mathbb{E}_{q_\\phi}\\bigl[\\log p(y,\\theta) - \\log q_\\phi(\\theta)\\bigr].\\]\nEquivalently:\n\\[\\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}[\\log p(y\\mid\\theta)] - \\mathrm{KL}\\bigl(q_\\phi(\\theta)\\;\\|\\;p(\\theta)\\bigr).\\]\nMaximising the ELBO is the same as minimising \\(\\mathrm{KL}(q_\\phi\\|p(\\theta\\mid y))\\) up to the constant \\(\\log p(y)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#variational-families",
    "href": "variational-inference.html#variational-families",
    "title": "4  Variational inference",
    "section": "4.4 3) Variational families",
    "text": "4.4 3) Variational families\nA few common choices:\n\nMean-field: \\(q(\\theta)=\\prod_i q_i(\\theta_i)\\). Cheap, but can miss correlations.\nFull-covariance Gaussian: captures correlations but scales poorly as \\(d^2\\).\nNormalising flows: expressive approximations; connects directly to the flow material in the modern SBI chapter.\nStructured VI: keep some dependencies (e.g. blockwise factorisation) to match the model.\n\nIn differentiable epidemiology, mean-field (or blockwise) Gaussian VI is often a practical default:\n\nyou get uncertainty estimates,\noptimisation is stable,\ngradients can be obtained through autodiff if the likelihood is differentiable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#optimisation-strategies-brief",
    "href": "variational-inference.html#optimisation-strategies-brief",
    "title": "4  Variational inference",
    "section": "4.5 4) Optimisation strategies (brief)",
    "text": "4.5 4) Optimisation strategies (brief)\n\n4.5.1 Coordinate ascent VI (CAVI)\nFor conjugate exponential-family models, the ELBO can be optimised by updating one factor \\(q_i\\) at a time. This is fast and exact within the chosen factorisation, but less flexible.\n\n\n4.5.2 Gradient-based VI (“black-box” VI)\nWhen conjugacy is unavailable, we optimise \\(\\mathcal{L}(\\phi)\\) with gradients. Two key gradient estimators are:\n\nScore-function / REINFORCE: works broadly, can have high variance.\nReparameterisation trick: lower variance, but requires a differentiable transform.\n\nIn practice, most modern VI for continuous parameters uses reparameterisation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#worked-example-normalnormal-model-numpy",
    "href": "variational-inference.html#worked-example-normalnormal-model-numpy",
    "title": "4  Variational inference",
    "section": "4.6 5) Worked example: normal–normal model (NumPy)",
    "text": "4.6 5) Worked example: normal–normal model (NumPy)\nWe observe \\(y_1,\\dots,y_n\\) with\n\\[y_i \\mid \\mu \\sim \\mathcal{N}(\\mu,\\,\\sigma^2),\\qquad \\mu\\sim\\mathcal{N}(\\mu_0,\\,\\tau_0^2),\\]\nwith known \\(\\sigma\\). The true posterior is Gaussian, so this is a nice sandbox.\nWe choose a variational family\n\\[q_\\phi(\\mu)=\\mathcal{N}(m,\\,s^2),\\qquad \\phi=(m,\\log s).\\]\nEven though the posterior is exactly Gaussian here, the point is to show:\n\nhow to write the ELBO,\nhow to optimise it,\nhow the ELBO trades data-fit vs KL-to-prior.\n\n\nimport numpy as np\n\nfrom diff_epi_inference import plot_distribution_comparison, plot_loss_curve\n\nrng = np.random.default_rng(0)\n\n# Data generating process\nmu_true = 1.3\nsigma = 0.7\nn = 50\n\ny = rng.normal(mu_true, sigma, size=n)\n\n# Prior\nmu0 = 0.0\ntau0 = 2.0\n\n# Closed-form posterior for reference\n# (normal prior + normal likelihood with known sigma)\nprec0 = 1.0 / tau0**2\nprec = prec0 + n / sigma**2\n\ntau_post = np.sqrt(1.0 / prec)\nmu_post = (mu0 * prec0 + y.sum() / sigma**2) / prec\n\nmu_post, tau_post\n\n(np.float64(1.3868996255448498), np.float64(0.0988739029306126))\n\n\n\n4.6.1 ELBO for \\(q(\\mu)=\\mathcal{N}(m,s^2)\\)\nWe can compute the ELBO in closed form. Write \\(s=\\exp(\\ell)\\) where \\(\\ell=\\log s\\) is unconstrained.\n\ndef elbo(m: float, log_s: float) -&gt; float:\n    s2 = float(np.exp(2.0 * log_s))\n\n    # E_q[log p(y|mu)] for normal likelihood with known sigma\n    # sum_i E[ -0.5 ((y_i - mu)^2 / sigma^2) - 0.5 log(2 pi sigma^2) ]\n    # where E[(y_i - mu)^2] = (y_i - m)^2 + s^2\n    const = -0.5 * n * np.log(2.0 * np.pi * sigma**2)\n    quad = -0.5 / sigma**2 * np.sum((y - m) ** 2 + s2)\n    eq_loglik = const + quad\n\n    # KL(q || p) for univariate Gaussians\n    # q=N(m,s^2), p=N(mu0,tau0^2)\n    kl = 0.5 * (\n        (s2 + (m - mu0) ** 2) / tau0**2\n        - 1.0\n        + 2.0 * (np.log(tau0) - log_s)\n    )\n\n    return float(eq_loglik - kl)\n\n\nprint({\n    \"ELBO at prior\": elbo(m=mu0, log_s=np.log(tau0)),\n    \"ELBO near posterior\": elbo(m=mu_post, log_s=np.log(tau_post)),\n})\n\n{'ELBO at prior': -351.56756387547426, 'ELBO near posterior': -52.1152779822524}\n\n\n\n\n4.6.2 Optimising the ELBO (simple gradient ascent)\nBecause the ELBO is smooth here, we can optimise it with hand-derived gradients.\n\ndef elbo_grad(m: float, log_s: float) -&gt; tuple[float, float]:\n    # Derivatives of the closed-form ELBO above.\n    s2 = float(np.exp(2.0 * log_s))\n\n    # d/dm E_q[log p(y|mu)] = (1/sigma^2) * sum_i (y_i - m)\n    d_m_loglik = (1.0 / sigma**2) * float(np.sum(y - m))\n\n    # d/dm KL = (m - mu0)/tau0^2\n    d_m_kl = (m - mu0) / tau0**2\n\n    d_m = d_m_loglik - d_m_kl\n\n    # d/d(log s) E_q[log p(y|mu)] = d/d(log s) [ -0.5/sigma^2 * n * s^2 ]\n    # since s^2 = exp(2 log_s), d s^2 / d log_s = 2 s^2\n    d_ls_loglik = -0.5 / sigma**2 * n * (2.0 * s2)\n\n    # KL includes: 0.5*(s^2/tau0^2) - log s  (up to constants)\n    d_ls_kl = 0.5 * (2.0 * s2 / tau0**2) - (-1.0)  # derivative of +log_s in KL? careful below\n\n    # Let's recompute d/d log_s of KL exactly:\n    # KL = 0.5*((s2 + (m-mu0)^2)/tau0^2 - 1 + 2(log tau0 - log_s))\n    # d/d log_s KL = 0.5*( (2 s2)/tau0^2  + 2*(0 - 1) )\n    d_ls_kl = 0.5 * ((2.0 * s2) / tau0**2 - 2.0)\n\n    d_ls = d_ls_loglik - d_ls_kl\n\n    return float(d_m), float(d_ls)\n\n\nm, log_s = 0.0, np.log(1.0)\n\n# The gradients can be quite large (they scale with n / sigma^2),\n# so we use a conservative learning rate and clamp log_s for stability.\nlr = 1e-3\nlog_s_min, log_s_max = np.log(0.05), np.log(5.0)\n\nfor t in range(2_000):\n    gm, gls = elbo_grad(m, log_s)\n    m += lr * gm\n    log_s = float(np.clip(log_s + lr * gls, log_s_min, log_s_max))\n\nprint(\n    {\n        \"m_vi\": float(m),\n        \"s_vi\": float(np.exp(log_s)),\n        \"m_posterior\": float(mu_post),\n        \"s_posterior\": float(tau_post),\n        \"elbo_final\": float(elbo(m, log_s)),\n    }\n)\n\n{'m_vi': 1.3868996255448491, 's_vi': 0.09977492671991484, 'm_posterior': 1.3868996255448498, 's_posterior': 0.0988739029306126, 'elbo_final': -52.11536077587953}\n\n\n\nposterior_draws = rng.normal(loc=mu_post, scale=tau_post, size=5_000)\nvi_draws = rng.normal(loc=m, scale=np.exp(log_s), size=5_000)\n\nfig, ax = plot_distribution_comparison(\n    a=posterior_draws,\n    b=vi_draws,\n    label_a=\"Exact posterior draws\",\n    label_b=\"VI approximation draws\",\n    title=\"Normal-normal example: posterior vs VI approximation\",\n    xlabel=\"mu\",\n)\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn higher dimensions you typically work with a vector mean and a parameterisation of the covariance (e.g. diagonal in mean-field VI). The same ELBO pattern applies.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#worked-example-mean-field-vi-for-the-seir-beta-only-model-jax",
    "href": "variational-inference.html#worked-example-mean-field-vi-for-the-seir-beta-only-model-jax",
    "title": "4  Variational inference",
    "section": "4.7 6) Worked example: mean-field VI for the SEIR beta-only model (JAX)",
    "text": "4.7 6) Worked example: mean-field VI for the SEIR beta-only model (JAX)\nWe now connect VI back to the running SEIR example. We reuse the beta-only JAX log-posterior from the classical-baselines chapter and fit a mean-field Gaussian variational approximation for \\(\\log \\beta\\).\n\n\n\n\n\n\nNote\n\n\n\nThis is an intentionally minimal implementation:\n\nvariational family: \\(q(\\log\\beta)=\\mathcal{N}(m, s^2)\\)\nELBO estimator: Monte Carlo + reparameterisation trick\noptimiser: Adam (implemented directly, to avoid extra dependencies)\n\n\n\n\nimport numpy as np\n\nfrom diff_epi_inference import SEIRParams, discrete_gamma_delay_pmf\nfrom diff_epi_inference import expected_reported_cases_delayed, incidence_from_susceptibles\nfrom diff_epi_inference import sample_nbinom_reports, simulate_seir_euler\nfrom diff_epi_inference.models.seir_jax_beta_only import make_log_post_logbeta_jax\nfrom diff_epi_inference.vi import fit_meanfield_gaussian_vi_jax\n\nrng = np.random.default_rng(2026)\n\n# Fixed model settings (small for a quick demo)\nsigma_fixed = 1 / 5\ngamma_fixed = 1 / 7\n\ns0, e0, i0, r0 = 999.0, 0.0, 1.0, 0.0\ndt = 0.5\nsteps = 60\n\nreporting_rate = 0.3\nw = discrete_gamma_delay_pmf(shape=2.0, scale=1.0, max_delay=15)\ndispersion = 20.0\n\n# Prior on log(beta)\nlogbeta_prior_mean = float(np.log(0.5))\nlogbeta_prior_sd = 0.5\n\n# Synthetic dataset\nbeta_true = 0.6\nparams_true = SEIRParams(beta=float(beta_true), sigma=sigma_fixed, gamma=gamma_fixed)\nout_true = simulate_seir_euler(\n    params=params_true,\n    s0=s0,\n    e0=e0,\n    i0=i0,\n    r0=r0,\n    dt=dt,\n    steps=steps,\n)\ninc_true = incidence_from_susceptibles(out_true[\"S\"])\nmu_true = expected_reported_cases_delayed(\n    incidence=inc_true,\n    reporting_rate=reporting_rate,\n    delay_pmf=w,\n)\ny_obs = sample_nbinom_reports(expected=mu_true, dispersion=dispersion, rng=rng)\n\nlog_post = make_log_post_logbeta_jax(\n    y_obs=y_obs,\n    w_delay_pmf=w,\n    sigma=sigma_fixed,\n    gamma=gamma_fixed,\n    s0=s0,\n    e0=e0,\n    i0=i0,\n    r0=r0,\n    dt=dt,\n    steps=steps,\n    reporting_rate=reporting_rate,\n    dispersion=dispersion,\n    logbeta_prior_mean=logbeta_prior_mean,\n    logbeta_prior_sd=logbeta_prior_sd,\n)\n\nvi = fit_meanfield_gaussian_vi_jax(\n    log_post,\n    dim=1,\n    seed=0,\n    num_steps=400,\n    lr=5e-2,\n    num_mc_samples=32,\n    init_mean=np.array([np.log(0.3)]),\n    init_log_std=np.array([np.log(0.5)]),\n)\n\nbeta_vi_mean = float(np.exp(vi.mean[0]))\nbeta_vi_sd = float(np.exp(vi.log_std[0])) * beta_vi_mean  # delta method-ish\n\n{\n    \"beta_true\": beta_true,\n    \"beta_vi_mean\": beta_vi_mean,\n    \"beta_vi_sd_approx\": beta_vi_sd,\n    \"elbo_start\": float(vi.elbo_history[0]),\n    \"elbo_end\": float(vi.elbo_history[-1]),\n}\n\n{'beta_true': 0.6,\n 'beta_vi_mean': 0.6060173511505127,\n 'beta_vi_sd_approx': 0.020674937580698938,\n 'elbo_start': -180.69752502441406,\n 'elbo_end': -64.12928771972656}\n\n\n\nfig, ax = plot_loss_curve(\n    np.asarray(vi.elbo_history),\n    title=\"SEIR beta-only VI: ELBO over optimisation\",\n    ylabel=\"ELBO\",\n)\nfig",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#where-vi-fits-in-differentiable-epidemiology",
    "href": "variational-inference.html#where-vi-fits-in-differentiable-epidemiology",
    "title": "4  Variational inference",
    "section": "4.8 7) Where VI fits in differentiable epidemiology",
    "text": "4.8 7) Where VI fits in differentiable epidemiology\nThink of three regimes:\n\nClassical likelihood-based inference (MCMC, Laplace, VI)\n\nWhen you have a differentiable (or at least evaluable) likelihood.\nVI can be a good default when MCMC is too slow.\n\nLikelihood-free baselines (ABC)\n\nWhen you cannot write/evaluate the likelihood.\nVI is not directly applicable unless you introduce a surrogate likelihood.\n\nModern SBI (flows, amortised inference)\n\nWhen you want fast inference across many datasets.\nYou can view some SBI methods as amortised VI with expressive conditional densities.\n\n\n\n4.8.1 Practical notes\n\nVI gives a lower bound on the evidence, but the bound itself is not a guarantee of posterior accuracy.\nAlways validate with diagnostics that matter for the downstream question:\n\nposterior predictive checks,\ncalibration checks (e.g. simulation-based calibration when possible),\nsensitivity to prior and variational family.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "variational-inference.html#exercises",
    "href": "variational-inference.html#exercises",
    "title": "4  Variational inference",
    "section": "4.9 Exercises",
    "text": "4.9 Exercises\n\nReplace the mean-field Gaussian with a full-covariance Gaussian in a 2D toy model.\nImplement a reparameterised Monte Carlo estimate of the ELBO for a non-conjugate likelihood.\nFor an epidemiological running example, decide which parameters should share a variational block (e.g. \\((\\beta,\\gamma)\\) together) and justify the factorisation.\n\n\n\n\n\nBlei, David M, Alp Kucukelbir, and Jon D McAuliffe. 2017. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association 112 (518): 859–77.\n\n\nKucukelbir, Alp, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. 2017. “Automatic Differentiation Variational Inference.” Journal of Machine Learning Research 18 (14): 1–45.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational inference</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html",
    "href": "autodiff-basics.html",
    "title": "5  Automatic differentiation basics",
    "section": "",
    "text": "5.1 Why autodiff matters for this handbook\nMuch of modern inference for epidemiological models relies on gradients (or approximations to gradients). Automatic differentiation (autodiff) is the workhorse that makes gradients practical in high-dimensional models and pipelines.\nThis chapter is intentionally conceptual-first. Runnable demos and helper code are included below. For deeper treatment, see (Griewank and Walther 2008; Baydin et al. 2018).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html#learning-goals",
    "href": "autodiff-basics.html#learning-goals",
    "title": "5  Automatic differentiation basics",
    "section": "5.2 Learning goals",
    "text": "5.2 Learning goals\nAfter this chapter you should be able to:\n\nDistinguish symbolic differentiation, numerical differentiation, and automatic differentiation.\nExplain the difference between forward-mode (JVP) and reverse-mode (VJP) autodiff.\nRecognise when reverse-mode is a good fit (scalar loss, many parameters) and when forward-mode helps.\nMap epidemiology modelling steps (simulator, observation model, loss) to a computational graph.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html#autodiff-in-one-picture-computational-graphs",
    "href": "autodiff-basics.html#autodiff-in-one-picture-computational-graphs",
    "title": "5  Automatic differentiation basics",
    "section": "5.3 Autodiff in one picture (computational graphs)",
    "text": "5.3 Autodiff in one picture (computational graphs)\nAutodiff works by applying the chain rule to a program represented as a graph of elementary operations. In practice, libraries like JAX, PyTorch, and TensorFlow build and/or trace these graphs for you.\n\n5.3.1 Notation\n\nParameters: \\(\\theta\\)\nLatent state trajectory (e.g. compartments over time): \\(x_{0:T}\\)\nSimulator: \\(x_{0:T} = \\mathrm{Sim}(\\theta)\\)\nObservations: \\(y \\sim p(y \\mid x_{0:T}, \\theta)\\)\nObjective (loss / negative log-likelihood): \\(\\mathcal{L}(\\theta)\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html#forward-mode-vs-reverse-mode",
    "href": "autodiff-basics.html#forward-mode-vs-reverse-mode",
    "title": "5  Automatic differentiation basics",
    "section": "5.4 Forward-mode vs reverse-mode",
    "text": "5.4 Forward-mode vs reverse-mode\n\n5.4.1 Forward-mode (JVP)\nForward-mode computes derivatives by pushing tangent information forward through the program. It is often efficient when the input dimension is small.\n\n\n5.4.2 Reverse-mode (VJP)\nReverse-mode computes derivatives by first running the program forward (saving intermediate values) and then pulling adjoint information backwards. It is often efficient for scalar objectives with many parameters, which is common in inference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html#runnable-demos-jax-path",
    "href": "autodiff-basics.html#runnable-demos-jax-path",
    "title": "5  Automatic differentiation basics",
    "section": "5.5 Runnable demos (JAX path)",
    "text": "5.5 Runnable demos (JAX path)\nThe cells below use JAX and produce concrete numerical output.\n\n5.5.1 Demo 1: scalar function + gradient\n\nimport matplotlib.pyplot as plt\n\nfrom diff_epi_inference.autodiff.demos import scalar_function_and_grad_demo\n\nres = scalar_function_and_grad_demo(n=200)\nprint({\"x_min\": float(res.x_grid.min()), \"x_max\": float(res.x_grid.max())})\n\nfig, ax = plt.subplots(figsize=(7, 3))\nax.plot(res.x_grid, res.f_vals, label=\"f(x)\")\nax.plot(res.x_grid, res.grad_vals, label=\"df/dx\")\nax.set_xlabel(\"x\")\nax.set_title(\"A tiny scalar function and its autodiff gradient\")\nax.legend(loc=\"best\")\nplt.show()\n\n{'x_min': -2.0, 'x_max': 2.0}\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 Demo 2: forward-mode JVP for a vector-valued function\n\nfrom diff_epi_inference.autodiff.demos import forward_mode_jvp_demo\n\nres = forward_mode_jvp_demo(m=20, x0=0.3)\nprint({\"x0\": res.x0, \"y_shape\": res.y.shape, \"jvp_shape\": res.jvp.shape})\nprint(\"First 5 outputs y:\", res.y[:5].round(4).tolist())\nprint(\"First 5 derivatives dy/dx:\", res.jvp[:5].round(4).tolist())\n\n{'x0': 0.3, 'y_shape': (20,), 'jvp_shape': (20,)}\nFirst 5 outputs y: [0.1517000049352646, 0.19189999997615814, 0.2321999967098236, 0.27239999175071716, 0.3125]\nFirst 5 derivatives dy/dx: [0.5094000101089478, 0.6442000269889832, 0.7781999707221985, 0.9107000231742859, 1.0413000583648682]\n\n\n\n\n5.5.3 Demo 3: reverse-mode gradient for a scalar loss with many parameters\n\nfrom diff_epi_inference.autodiff.demos import reverse_mode_vjp_demo\n\nres = reverse_mode_vjp_demo(dim=200, seed=0)\nprint({\"dim\": res.dim, \"loss0\": res.loss0, \"grad_norm\": res.grad_norm})\n\n{'dim': 200, 'loss0': 0.044367097318172455, 'grad_norm': 0.03124016895890236}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html#practical-checklist",
    "href": "autodiff-basics.html#practical-checklist",
    "title": "5  Automatic differentiation basics",
    "section": "5.6 Practical checklist",
    "text": "5.6 Practical checklist\n\nIs your objective scalar? Reverse-mode is usually the default choice.\nAre you computing derivatives w.r.t. many outputs? Consider forward-mode or batching.\nDo you have control-flow (loops/branches)? Ensure your autodiff library supports the pattern.\nAre there non-differentiable ops (thresholds, argmax, discrete sampling)? See the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "autodiff-basics.html#key-takeaways",
    "href": "autodiff-basics.html#key-takeaways",
    "title": "5  Automatic differentiation basics",
    "section": "5.7 Key takeaways",
    "text": "5.7 Key takeaways\n\nForward-mode and reverse-mode both apply the chain rule; they differ in traversal direction and cost profile.\nReverse-mode is usually the default for scalar objectives with many parameters.\nAutodiff is only as useful as the differentiability of your modeling pipeline.\n\n\n\n\n\nBaydin, Atilim Gunes, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2018. “Automatic Differentiation in Machine Learning: A Survey.” Journal of Machine Learning Research 18 (153): 1–43.\n\n\nGriewank, Andreas, and Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2nd ed. SIAM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic differentiation basics</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html",
    "href": "differentiability-axis.html",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "",
    "text": "6.1 Motivation\nIn infectious disease modelling we often start from a simulator:\nSome of these are naturally differentiable (or can be made differentiable), and some are not. Choosing an inference method is easier if you explicitly locate your model on a differentiability axis. This framing mirrors current simulation-based inference practice (Cranmer, Brehmer, and Louppe 2020).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#motivation",
    "href": "differentiability-axis.html#motivation",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "",
    "text": "an ODE model solved numerically,\na stochastic simulator (e.g. Gillespie / CTMC),\nan agent-based model,\na hybrid model with discrete events (interventions, thresholds, reporting delays).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#the-differentiability-axis",
    "href": "differentiability-axis.html#the-differentiability-axis",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.2 The differentiability axis",
    "text": "6.2 The differentiability axis\n\n6.2.1 1) Fully differentiable end-to-end\nEverything from parameters \\(\\theta\\) to objective \\(\\mathcal{L}(\\theta)\\) is differentiable. Examples include differentiable ODE solvers with smooth observation models.\nImplications: gradient-based optimisation and gradient-informed Bayesian methods are feasible.\n\n\n6.2.2 2) Differentiable core with non-differentiable edges\nThe simulator may be differentiable, but you have non-smooth components:\n\nthresholds / clipping / rounding,\ndiscrete interventions triggered by conditions,\ndiscontinuous likelihood approximations.\n\nImplications: you may need smoothing, continuous relaxations, or surrogate losses.\n\n\n6.2.3 3) Stochastic but reparameterisable\nRandomness is present, but can be written as a deterministic transform of noise: \\(z = g(\\theta, \\epsilon)\\) with \\(\\epsilon \\sim p(\\epsilon)\\).\nImplications: pathwise (reparameterisation) gradients can work.\n\n\n6.2.4 4) Black-box / non-differentiable simulator\nThe simulator is discrete, event-driven, or otherwise non-differentiable with respect to parameters.\nImplications: likelihood-free or gradient-free methods (ABC, synthetic likelihood, neural density estimators, score-based approaches, etc.) are often more appropriate.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#demo-1-finite-differences-can-be-unstable-at-discontinuities",
    "href": "differentiability-axis.html#demo-1-finite-differences-can-be-unstable-at-discontinuities",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.3 Demo 1: finite differences can be unstable at discontinuities",
    "text": "6.3 Demo 1: finite differences can be unstable at discontinuities\nA common first instinct is to “just do finite differences” when gradients are unavailable. This can fail dramatically if your objective is not smooth.\nHere is the simplest possible example: a step function.\n\nimport matplotlib.pyplot as plt\n\nfrom diff_epi_inference.differentiability.demos import (\n    discontinuous_finite_difference_instability_demo,\n)\n\nres = discontinuous_finite_difference_instability_demo(x0=0.0)\n\nfig, ax = plt.subplots(figsize=(6, 3))\nax.plot(res.eps, res.fd_estimates, marker=\"o\")\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.set_xlabel(\"finite-difference step size ε\")\nax.set_ylabel(\"central FD estimate\")\nax.set_title(\"Finite differences near a discontinuity explode as ε → 0\")\nplt.show()\n\nprint({\"x0\": res.x0, \"eps\": res.eps.tolist(), \"fd\": res.fd_estimates.round(2).tolist()})\n\n\n\n\n\n\n\n\n{'x0': 0.0, 'eps': [1.0, 0.3, 0.1, 0.03, 0.01, 0.003], 'fd': [0.5, 1.67, 5.0, 16.67, 50.0, 166.67]}\n\n\nTakeaway: if your simulator includes hard thresholds, conditionals that change behaviour, integer rounding, or discrete-event updates, finite-difference gradients can become meaningless (or wildly sensitive to your choice of \\(\\epsilon\\)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#demo-2-autodiff-on-a-smooth-function-is-stable",
    "href": "differentiability-axis.html#demo-2-autodiff-on-a-smooth-function-is-stable",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.4 Demo 2: autodiff on a smooth function is stable",
    "text": "6.4 Demo 2: autodiff on a smooth function is stable\nWhen your simulator and likelihood are smooth, autodiff gives you gradients that are:\n\nstable to step-size choices,\nexact up to floating point / control-flow choices,\ncheap compared to finite differences in high dimensions.\n\n\nfrom diff_epi_inference.differentiability.demos import smooth_function_jax_grad_demo\n\nres = smooth_function_jax_grad_demo(x0=0.3)\nprint(\n    {\n        \"x0\": res.x0,\n        \"f0\": round(res.f0, 6),\n        \"grad_jax\": round(res.grad0, 6),\n        \"grad_analytic\": round(res.grad0_analytic, 6),\n    }\n)\n\n{'x0': 0.3, 'f0': 0.30452, 'grad_jax': 1.015337, 'grad_analytic': 1.015336}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#demo-3-a-stochastic-tau-leap-seir-simulator-is-non-differentiable-as-implemented",
    "href": "differentiability-axis.html#demo-3-a-stochastic-tau-leap-seir-simulator-is-non-differentiable-as-implemented",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.5 Demo 3: a stochastic tau-leap SEIR simulator is non-differentiable as implemented",
    "text": "6.5 Demo 3: a stochastic tau-leap SEIR simulator is non-differentiable as implemented\nA tau-leaping simulator updates compartments using random integer transitions. For a fixed random seed, the mapping\n\\[\\beta \\mapsto R_T(\\beta)\\]\n(where \\(R_T\\) is the final recovered count) tends to be jumpy: flat for a while, then a sudden change when a random draw flips. That means a derivative in the usual sense does not exist for a single sample path.\n\nimport matplotlib.pyplot as plt\n\nfrom diff_epi_inference.differentiability.demos import tau_leap_seir_nondifferentiability_demo\n\nres = tau_leap_seir_nondifferentiability_demo(beta0=0.25, span=0.06, n=61, fd_eps=1e-3, seed=0)\n\nfig, ax = plt.subplots(figsize=(7, 3))\nax.plot(res.betas, res.final_size, lw=2)\nax.set_xlabel(\"beta\")\nax.set_ylabel(\"final size (R_T)\")\nax.set_title(\"Tau-leap SEIR: a single-seed objective is jumpy in beta\")\nplt.show()\n\nfig, ax = plt.subplots(figsize=(7, 3))\nax.plot(res.betas, res.fd_estimates, lw=1)\nax.axhline(0.0, color=\"k\", lw=1)\nax.set_xlabel(\"beta\")\nax.set_ylabel(\"central FD estimate\")\nax.set_title(\"Finite differences: mostly zero, punctuated by spikes\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe expectation \\(\\mathbb{E}[R_T(\\beta)]\\) may be smooth in \\(\\beta\\).\nBut the single-run simulator output is not (and most inference pipelines see the single-run output).\n\nThis is where you should consider:\n\naveraging / using common random numbers carefully,\nreparameterisable constructions (when possible),\nsynthetic likelihood / SBI / ABC,\ndifferentiable surrogates or relaxations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#a-method-selection-decision-tree",
    "href": "differentiability-axis.html#a-method-selection-decision-tree",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.6 A method-selection decision tree",
    "text": "6.6 A method-selection decision tree\nThis is deliberately simplified, but useful as a first pass.\n\nflowchart TD\n  A[Start: simulator + observation model] --&gt; B{Is everything differentiable\\nend-to-end?}\n  B -- Yes --&gt; C[Gradient-based inference\\n(HMC/NUTS, VI, MAP)]\n  B -- No --&gt; D{Is the core differentiable\\nbut edges are non-smooth?}\n  D -- Yes --&gt; E[Smoothing / relaxations\\n+ gradient-based methods]\n  D -- No --&gt; F{Is the simulator stochastic\\nbut reparameterisable?}\n  F -- Yes --&gt; G[Pathwise gradients\\n(reparameterisation trick)]\n  F -- No --&gt; H[Gradient-free / likelihood-free\\n(ABC, synthetic likelihood, SBI, surrogates)]\n\n\n\n\nflowchart TD\n  A[Start: simulator + observation model] --&gt; B{Is everything differentiable\\nend-to-end?}\n  B -- Yes --&gt; C[Gradient-based inference\\n(HMC/NUTS, VI, MAP)]\n  B -- No --&gt; D{Is the core differentiable\\nbut edges are non-smooth?}\n  D -- Yes --&gt; E[Smoothing / relaxations\\n+ gradient-based methods]\n  D -- No --&gt; F{Is the simulator stochastic\\nbut reparameterisable?}\n  F -- Yes --&gt; G[Pathwise gradients\\n(reparameterisation trick)]\n  F -- No --&gt; H[Gradient-free / likelihood-free\\n(ABC, synthetic likelihood, SBI, surrogates)]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#method-selection-checklist-end-of-chapter",
    "href": "differentiability-axis.html#method-selection-checklist-end-of-chapter",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.7 Method selection checklist (end-of-chapter)",
    "text": "6.7 Method selection checklist (end-of-chapter)\nWhen you are picking an inference approach, write down answers to these explicitly:\n\nSimulator type: ODE / SDE / discrete-event (CTMC, ABM) / hybrid?\nWhere is non-smoothness coming from? thresholds, rounding, discrete interventions, censoring?\nWhat gradients do you actually need? w.r.t. parameters only, or also w.r.t. latent trajectories / controls?\nIf stochastic: can you express randomness as \\(x = g(\\theta, \\epsilon)\\) with \\(\\epsilon\\) independent of \\(\\theta\\)?\nIf not differentiable: can you build a differentiable relaxation that is scientifically acceptable?\nCost profile: how expensive is one simulation? can you batch/parallelise? can you amortise with a surrogate?\nIdentifiability: even with gradients, do you have enough information in the data to pin down \\(\\theta\\)?\nValidation plan: what will you use (PPC, SBC, coverage checks) to detect when the method is lying to you?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "differentiability-axis.html#key-takeaways",
    "href": "differentiability-axis.html#key-takeaways",
    "title": "6  Differentiability axis: when your simulator is (not) differentiable",
    "section": "6.8 Key takeaways",
    "text": "6.8 Key takeaways\n\nAlways classify your simulator and observation pipeline by differentiability before choosing inference.\nFinite differences are unreliable around discontinuities and discrete-event dynamics.\nWhen exact gradients are unavailable, move deliberately to relaxations or likelihood-free methods.\n\n\n\n\n\nCranmer, Kyle, Johann Brehmer, and Gilles Louppe. 2020. “The Frontier of Simulation-Based Inference.” Proceedings of the National Academy of Sciences 117 (48): 30055–62.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentiability axis: when your simulator is (not) differentiable</span>"
    ]
  },
  {
    "objectID": "gradient-estimators.html",
    "href": "gradient-estimators.html",
    "title": "7  Gradient estimators & relaxations",
    "section": "",
    "text": "7.1 0) Finite differences (fragile)\nMany inference and learning methods in differentiable epidemiology rely on gradients. When your model has discrete components (discrete latent states, interventions, thresholds, reporting rules), naive differentiation breaks. The estimator families below follow the classical REINFORCE and Concrete/Gumbel-Softmax literature (Williams 1992; Jang, Gu, and Poole 2017; Maddison, Mnih, and Teh 2017).\nThis chapter collects a few practical tools:\nA first impulse is to approximate a gradient with finite differences:\n\\[\n\\frac{\\partial}{\\partial \\phi}\\,\\mathbb{E}[f(z)]\n\\approx \\frac{\\widehat{\\mathbb{E}}_{\\phi+\\epsilon}[f(z)]-\\widehat{\\mathbb{E}}_{\\phi-\\epsilon}[f(z)]}{2\\epsilon}.\n\\]\nThis can work for smooth, low-noise problems, but it is often fragile:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Gradient estimators & relaxations</span>"
    ]
  },
  {
    "objectID": "gradient-estimators.html#finite-differences-fragile",
    "href": "gradient-estimators.html#finite-differences-fragile",
    "title": "7  Gradient estimators & relaxations",
    "section": "",
    "text": "you must choose a step size \\(\\epsilon\\);\nMonte Carlo noise can dominate the difference;\nfor discontinuous / discrete models, the estimator can be extremely unstable.\n\n\n7.1.1 Demo: bias/variance trade-off on a toy Bernoulli “infection event”\nWe estimate the gradient of \\(\\mathbb{E}[z]\\) where \\(z\\sim \\mathrm{Bernoulli}(\\sigma(\\phi))\\). The true gradient is \\(\\sigma(\\phi)(1-\\sigma(\\phi))\\).\nWe compare three estimators:\n\nfinite differences (MC)\nREINFORCE (score-function)\na soft relaxation (Binary Concrete) with a pathwise derivative (biased at finite temperature)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom diff_epi_inference.gradients import bernoulli_concrete, reinforce_grad_logit_bernoulli\n\nrng = np.random.default_rng(0)\nlogit = 0.3\np = 1.0 / (1.0 + np.exp(-logit))\ntrue_grad = p * (1 - p)\n\ndef mc_mean_z(logit_value: float, *, rng: np.random.Generator, n: int) -&gt; float:\n    p = 1.0 / (1.0 + np.exp(-logit_value))\n    z = rng.binomial(n=1, p=p, size=n).astype(float)\n    return float(np.mean(z))\n\nn_samples = 400\nn_rep = 300\n\neps = 1e-2\ntau = 0.7\n\nfd = []\nreinforce = []\nrelaxed_pathwise = []\n\nfor _ in range(n_rep):\n    # 1) finite differences with two independent MC estimates\n    m_plus = mc_mean_z(logit + eps, rng=rng, n=n_samples)\n    m_minus = mc_mean_z(logit - eps, rng=rng, n=n_samples)\n    fd.append((m_plus - m_minus) / (2 * eps))\n\n    # 2) REINFORCE: unbiased but high variance\n    r = reinforce_grad_logit_bernoulli(lambda z: z, logit=logit, rng=rng, n_samples=n_samples)\n    reinforce.append(r.grad)\n\n    # 3) Binary Concrete relaxation: differentiable sample y in [0, 1]\n    y = bernoulli_concrete(logit, temperature=tau, rng=rng)\n    # pathwise derivative dy/dlogit for y = sigmoid((logit + noise)/tau)\n    relaxed_pathwise.append(float(y * (1 - y) / tau))\n\nfig, ax = plt.subplots(figsize=(7, 3.5))\nax.boxplot([fd, reinforce, relaxed_pathwise], labels=[\"finite diff\", \"REINFORCE\", f\"relax (tau={tau})\"], showfliers=False)\nax.axhline(true_grad, color=\"black\", linestyle=\"--\", linewidth=1, label=\"true\")\nax.set_ylabel(\"gradient estimate\")\nax.set_title(f\"Toy Bernoulli gradient at logit={logit} (n={n_samples}, reps={n_rep})\")\nax.legend(loc=\"upper right\")\nplt.show()\n\nnp.mean(fd), np.mean(reinforce), np.mean(relaxed_pathwise), true_grad\n\n\n\n\n\n\n\n\n(np.float64(0.3608333333333333),\n np.float64(0.2434422916666667),\n np.float64(0.18795659526175915),\n np.float64(0.24445831169074586))\n\n\nIn practice:\n\nfinite differences can be wildly noisy unless you spend a lot of compute;\nREINFORCE is unbiased but may require careful baselines / variance reduction;\nrelaxations can give stable gradients, but they change the model (bias).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Gradient estimators & relaxations</span>"
    ]
  },
  {
    "objectID": "gradient-estimators.html#score-function-gradients-reinforce",
    "href": "gradient-estimators.html#score-function-gradients-reinforce",
    "title": "7  Gradient estimators & relaxations",
    "section": "7.2 1) Score-function gradients (REINFORCE)",
    "text": "7.2 1) Score-function gradients (REINFORCE)\nSuppose we want gradients of an expectation\n\\[\n\\nabla_\\phi\\; \\mathbb{E}_{z\\sim p_\\phi(z)}[f(z)]\n\\]\nwhere \\(z\\) is discrete. We can use the identity\n\\[\n\\nabla_\\phi\\; \\mathbb{E}[f(z)] = \\mathbb{E}\\bigl[f(z)\\,\\nabla_\\phi \\log p_\\phi(z)\\bigr].\n\\]\nFor a Bernoulli variable parameterised by a logit \\(\\phi\\),\n\\[\n\\nabla_\\phi \\log p_\\phi(z) = z - \\sigma(\\phi).\n\\]\n\n7.2.1 Minimal demo: Bernoulli REINFORCE (NumPy)\n\nimport numpy as np\n\nfrom diff_epi_inference.gradients import reinforce_grad_logit_bernoulli\n\nrng = np.random.default_rng(0)\nlogit = 0.3\n\n# f(z)=z  =&gt; E[z]=p  =&gt; d/dlogit E[z] = p(1-p)\nres = reinforce_grad_logit_bernoulli(lambda z: z, logit=logit, rng=rng, n_samples=50_000)\n\np = 1.0 / (1.0 + np.exp(-logit))\nres.grad, p * (1 - p)\n\n(0.24469142040000003, np.float64(0.24445831169074586))\n\n\n\n\n7.2.2 Variance reduction: baselines\nA common control variate is to subtract a baseline \\(b\\):\n\\[\n\\mathbb{E}[(f(z)-b)\\nabla_\\phi\\log p_\\phi(z)]\n\\]\nAny baseline that does not depend on \\(z\\) keeps the estimator unbiased and can drastically reduce variance.\n(Our tiny implementation uses the sample mean of \\(f(z)\\) as a baseline option.)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Gradient estimators & relaxations</span>"
    ]
  },
  {
    "objectID": "gradient-estimators.html#pathwise-reparameterisation-gradients",
    "href": "gradient-estimators.html#pathwise-reparameterisation-gradients",
    "title": "7  Gradient estimators & relaxations",
    "section": "7.3 2) Pathwise (reparameterisation) gradients",
    "text": "7.3 2) Pathwise (reparameterisation) gradients\nWhen randomness can be expressed as a deterministic transform of parameter-free noise,\n\\[\n\\epsilon\\sim p(\\epsilon),\\qquad z = g(\\phi,\\epsilon),\n\\]\nwe can differentiate through \\(g\\) and estimate\n\\[\n\\nabla_\\phi\\,\\mathbb{E}[f(g(\\phi,\\epsilon))]\n\\approx \\frac{1}{S}\\sum_{s=1}^S \\nabla_\\phi f(g(\\phi,\\epsilon_s)).\n\\]\nThis usually has much lower variance than REINFORCE.\nIn epidemiology, this is most directly applicable when your simulator is differentiable and the noise is reparameterisable (e.g. Gaussian noise models).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Gradient estimators & relaxations</span>"
    ]
  },
  {
    "objectID": "gradient-estimators.html#relaxations-for-categorical-discrete-choices",
    "href": "gradient-estimators.html#relaxations-for-categorical-discrete-choices",
    "title": "7  Gradient estimators & relaxations",
    "section": "7.4 3) Relaxations for categorical / discrete choices",
    "text": "7.4 3) Relaxations for categorical / discrete choices\n\n7.4.1 Gumbel–Softmax (Concrete)\nFor a categorical variable with logits \\(\\ell\\in\\mathbb{R}^K\\), Gumbel–Softmax draws a soft one-hot vector on the simplex. As temperature \\(\\tau\\to 0\\), samples become closer to one-hot.\nFor \\(K=2\\) (a Bernoulli choice), this reduces to the Binary Concrete (a soft sample in \\([0,1]\\)).\n\nimport numpy as np\n\nfrom diff_epi_inference.gradients import gumbel_softmax\n\nrng = np.random.default_rng(0)\nlogits = np.array([0.0, 1.0, -0.5])\n\ny = gumbel_softmax(logits, temperature=0.7, rng=rng)\nfloat(np.sum(y)), y\n\n(1.0, array([0.51559895, 0.46900191, 0.01539914]))\n\n\n\n\n7.4.2 Straight-through estimators (heuristic)\nA common practical trick is:\n\nforward pass: use a hard discrete sample (argmax / threshold)\nbackward pass: pretend we used the soft sample\n\nThis enables optimisation but is biased.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Gradient estimators & relaxations</span>"
    ]
  },
  {
    "objectID": "gradient-estimators.html#practical-checklist",
    "href": "gradient-estimators.html#practical-checklist",
    "title": "7  Gradient estimators & relaxations",
    "section": "7.5 Practical checklist",
    "text": "7.5 Practical checklist\nWhen you reach for gradient estimators / relaxations:\n\nCan you rewrite the randomness in a reparameterised form (pathwise gradient)?\nIf you must use REINFORCE, can you add an effective baseline (and maybe Rao–Blackwellise parts)?\nIf using Gumbel–Softmax:\n\nchoose a temperature schedule,\ncheck sensitivity to \\(\\tau\\),\nvalidate downstream metrics (not just loss curves).\n\nAlways test for pathological gradients: exploding variance, vanishing gradients at low temperatures, or optimisation instability.\n\n\n\n\n\nJang, Eric, Shixiang Gu, and Ben Poole. 2017. “Categorical Reparameterization with Gumbel-Softmax.” International Conference on Learning Representations.\n\n\nMaddison, Chris J, Andriy Mnih, and Yee Whye Teh. 2017. “The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.” International Conference on Learning Representations.\n\n\nWilliams, Ronald J. 1992. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.” Machine Learning 8: 229–56.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Gradient estimators & relaxations</span>"
    ]
  },
  {
    "objectID": "toy-diff-epi-case-study.html",
    "href": "toy-diff-epi-case-study.html",
    "title": "8  Toy differentiable epidemiology case study",
    "section": "",
    "text": "8.1 A relaxed SIR-like simulator\nThis chapter is a small, pedagogical example of end-to-end differentiable inference in a toy epidemic model.\nThe point is not realism. The point is the pattern:\nA discrete-time SIR model typically has discrete transition counts:\nDirectly sampling discrete counts blocks gradients.\nA common trick is to replace the discrete sampling with a continuous relaxation (e.g. a Concrete / Gumbel-Softmax relaxation), which produces fractional transition counts but keeps the computation differentiable. The relaxation used here follows the same core idea as (Jang, Gu, and Poole 2017; Maddison, Mnih, and Teh 2017).\nIn this repo we implement a very small JAX-based toy simulator that uses a Binary-Concrete-style relaxation to produce differentiable “counts”.\nimport numpy as np\n\nfrom diff_epi_inference import plot_loss_curve, plot_series_comparison\nfrom diff_epi_inference.examples.toy_diff_epi_jax import fit_beta_by_gradient_descent, simulate_toy_relaxed_sir\nsteps = 25\ntrue_beta = 0.40\ngamma = 0.20\n\n# Generate a synthetic incidence curve\npath = simulate_toy_relaxed_sir(\n    seed=0,\n    beta=true_beta,\n    gamma=gamma,\n    s0=80.0,\n    i0=20.0,\n    r0=0.0,\n    steps=steps,\n    temperature=0.6,\n)\ny_obs = np.asarray(path.new_infections)\n\n# Fit beta by differentiating through the simulator\nres = fit_beta_by_gradient_descent(\n    y_obs=y_obs,\n    seed=0,\n    beta_init=0.15,\n    gamma=gamma,\n    s0=80.0,\n    i0=20.0,\n    r0=0.0,\n    steps=steps,\n    temperature=0.6,\n    lr=0.25,\n    iters=40,\n)\n\nprint(\"beta_init:\", res.beta_init)\nprint(\"beta_hat :\", round(res.beta_hat, 3))\nprint(\"loss[0]  :\", float(res.losses[0]))\nprint(\"loss[-1] :\", float(res.losses[-1]))\n\nbeta_init: 0.15\nbeta_hat : 0.0\nloss[0]  : 29.7530517578125\nloss[-1] : 40.00442123413086\nimport matplotlib.pyplot as plt\n\npath_hat = simulate_toy_relaxed_sir(\n    seed=0,\n    beta=float(res.beta_hat),\n    gamma=gamma,\n    s0=80.0,\n    i0=20.0,\n    r0=0.0,\n    steps=steps,\n    temperature=0.6,\n)\n\nfig, ax = plot_series_comparison(\n    t=np.arange(steps, dtype=float),\n    observed=y_obs,\n    fitted=np.asarray(path_hat.new_infections),\n    observed_label=\"observed new infections\",\n    fitted_label=\"fitted new infections\",\n    title=\"Toy relaxed-SIR: observed vs fitted incidence\",\n    ylabel=\"new infections / step\",\n)\nplt.show()\n\nfig, ax = plot_loss_curve(\n    np.asarray(res.losses),\n    title=\"Toy relaxed-SIR optimisation trajectory\",\n    ylabel=\"MSE loss\",\n)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Toy differentiable epidemiology case study</span>"
    ]
  },
  {
    "objectID": "toy-diff-epi-case-study.html#a-relaxed-sir-like-simulator",
    "href": "toy-diff-epi-case-study.html#a-relaxed-sir-like-simulator",
    "title": "8  Toy differentiable epidemiology case study",
    "section": "",
    "text": "new infections ~ Binomial(S, p_inf)\nnew recoveries ~ Binomial(I, p_rec)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Toy differentiable epidemiology case study</span>"
    ]
  },
  {
    "objectID": "toy-diff-epi-case-study.html#notes-and-caveats",
    "href": "toy-diff-epi-case-study.html#notes-and-caveats",
    "title": "8  Toy differentiable epidemiology case study",
    "section": "8.2 Notes and caveats",
    "text": "8.2 Notes and caveats\n\nThis is a toy relaxation. It is not a statistically principled replacement for exact Binomial transitions.\nRelaxations introduce bias: you gain gradients but you no longer simulate from the exact discrete model.\nIn realistic applications you would typically combine:\n\na better-motivated observation model,\nstronger priors/regularisation,\ndiagnostics (e.g. SBC/PPC), and\ncareful sensitivity analysis over the relaxation temperature.\n\n\n\n\n\n\nJang, Eric, Shixiang Gu, and Ben Poole. 2017. “Categorical Reparameterization with Gumbel-Softmax.” International Conference on Learning Representations.\n\n\nMaddison, Chris J, Andriy Mnih, and Yee Whye Teh. 2017. “The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.” International Conference on Learning Representations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Toy differentiable epidemiology case study</span>"
    ]
  },
  {
    "objectID": "likelihood-free-baselines.html",
    "href": "likelihood-free-baselines.html",
    "title": "9  Likelihood-free baselines",
    "section": "",
    "text": "9.1 Chapter map\nThis chapter introduces likelihood-free baselines for the running example. These methods are useful when the simulator is non-differentiable and/or when a tractable likelihood is unavailable.\nWe focus on three families that define the classical foundation for simulation-based inference: ABC rejection (Beaumont, Zhang, and Balding 2002), SMC-ABC, and synthetic likelihood (Wood 2010).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Likelihood-free baselines</span>"
    ]
  },
  {
    "objectID": "likelihood-free-baselines.html#chapter-map",
    "href": "likelihood-free-baselines.html#chapter-map",
    "title": "9  Likelihood-free baselines",
    "section": "",
    "text": "Build low-dimensional summaries and a scaled distance.\nRun ABC rejection for a first posterior approximation.\nTighten with SMC-ABC.\nBridge to synthetic likelihood on the same summary space.\n\n\n9.1.0.1 Demo: beta-only ABC on the stochastic simulator\nBelow we infer only \\(\\beta\\) (holding the other SEIR parameters fixed) using the stochastic simulator and a very small set of summary statistics. This is intentionally minimal: the goal is to show the ABC control flow and get a rough posterior-like set of accepted \\(\\beta\\) values.\n\nimport numpy as np\n\nfrom diff_epi_inference import SEIRParams\nfrom diff_epi_inference.abc import abc_rejection\nfrom diff_epi_inference.pipeline import simulate_seir_and_report_stochastic\n\nrng = np.random.default_rng(0)\n\n# --- \"Observed\" data (synthetic) ---\nbeta_true = 0.35\nparams_true = SEIRParams(beta=beta_true, sigma=1 / 4.0, gamma=1 / 6.0)\n\nsteps = 80\n\nds_obs = simulate_seir_and_report_stochastic(\n    params=params_true,\n    s0=10_000,\n    e0=3,\n    i0=2,\n    r0=0,\n    dt=1.0,\n    steps=steps,\n    reporting_rate=0.25,\n    rng=rng,\n)\n\ny_obs = ds_obs.y.astype(float)\n\n\n# --- Prior and simulator ---\n# We sample log(beta) ~ Normal, then exponentiate.\nlogbeta_prior_mean = float(np.log(0.3))\nlogbeta_prior_sd = 0.35\n\n\ndef prior_sample(rng: np.random.Generator) -&gt; np.ndarray:\n    logbeta = rng.normal(loc=logbeta_prior_mean, scale=logbeta_prior_sd)\n    return np.array([logbeta], dtype=float)\n\n\ndef simulate(theta: np.ndarray, rng: np.random.Generator) -&gt; np.ndarray:\n    (logbeta,) = np.asarray(theta, dtype=float)\n    beta = float(np.exp(logbeta))\n\n    params = SEIRParams(beta=beta, sigma=params_true.sigma, gamma=params_true.gamma)\n    ds = simulate_seir_and_report_stochastic(\n        params=params,\n        s0=10_000,\n        e0=3,\n        i0=2,\n        r0=0,\n        dt=1.0,\n        steps=steps,\n        reporting_rate=0.25,\n        rng=rng,\n    )\n    return ds.y.astype(float)\n\n\n# --- Summaries and distance ---\n# Keep summaries low-dimensional: (total cases, peak size, peak time).\n\ndef summary(y: np.ndarray) -&gt; np.ndarray:\n    y = np.asarray(y, dtype=float)\n    peak_t = int(np.argmax(y))\n    return np.array([np.sum(y), np.max(y), peak_t], dtype=float)\n\n\ndef distance(s_sim: np.ndarray, s_obs: np.ndarray) -&gt; float:\n    s_sim = np.asarray(s_sim, dtype=float)\n    s_obs = np.asarray(s_obs, dtype=float)\n    # Rough scaling so \"time\" doesn't dominate.\n    scale = np.array([1000.0, 50.0, 5.0])\n    return float(np.linalg.norm((s_sim - s_obs) / scale))\n\n\nres = abc_rejection(\n    prior_sample=prior_sample,\n    simulate=simulate,\n    distance=distance,\n    y_obs=y_obs,\n    summary=summary,\n    epsilon=1.5,\n    n_accept=200,\n    max_trials=50_000,\n    rng=rng,\n)\n\nbeta_accept = np.exp(res.thetas[:, 0])\n\nprint(f\"ABC trials: {res.n_trials}  (accept rate ~ {len(beta_accept)/res.n_trials:.3f})\")\nprint(f\"beta_true: {beta_true:.3f}\")\nprint(\n    \"accepted beta: \"\n    f\"mean={np.mean(beta_accept):.3f}, sd={np.std(beta_accept):.3f}, \"\n    f\"q10={np.quantile(beta_accept, 0.1):.3f}, q90={np.quantile(beta_accept, 0.9):.3f}\"\n)\n\nABC trials: 497  (accept rate ~ 0.402)\nbeta_true: 0.350\naccepted beta: mean=0.315, sd=0.044, q10=0.252, q90=0.370\n\n\n\nfrom diff_epi_inference.plotting import plot_hist_1d\n\nfig, ax = plot_hist_1d(\n    beta_accept,\n    true_value=beta_true,\n    bins=30,\n    title=\"ABC rejection posterior (accepted β)\",\n    xlabel=\"β\",\n)\nfig\n\n\n\n\nABC rejection: histogram of accepted \\(\\beta\\) values (dashed line = \\(\\beta_{true}\\)).\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\nThe choice of summaries and scaling in the distance is ad hoc; later sections will discuss more principled summary selection.\nAs written, ABC rejection can be inefficient: if the tolerance epsilon is too small, you may need a very large max_trials.\n\n\n\n9.1.1 3) (Optional) SMC-ABC\n\nSequence of tolerances \\(\\varepsilon_1 &gt; \\varepsilon_2 &gt; \\cdots\\).\nReweight/resample/perturb particles.\nMonitor acceptance rates and particle degeneracy.\n\n\n9.1.1.1 Demo: beta-only SMC-ABC on the same summaries\nThis is the same toy inference problem as the ABC rejection demo above, but using an SMC-ABC scheme: we run multiple rounds with decreasing tolerances, and maintain a weighted particle population.\n\nimport numpy as np\n\nfrom diff_epi_inference.abc import smc_abc\n\n# Reuse: y_obs, params_true, steps, logbeta_prior_mean, logbeta_prior_sd,\n# prior_sample, simulate, summary, distance (defined in the ABC rejection block).\n\n\ndef prior_logpdf(theta: np.ndarray) -&gt; float:\n    (logbeta,) = np.asarray(theta, dtype=float)\n    z = (logbeta - logbeta_prior_mean) / logbeta_prior_sd\n    return float(-0.5 * z**2)  # (drop constants)\n\n\nkernel_sd = 0.12\n\n\ndef perturb(theta_prev: np.ndarray, rng: np.random.Generator) -&gt; np.ndarray:\n    theta_prev = np.asarray(theta_prev, dtype=float)\n    return theta_prev + rng.normal(loc=0.0, scale=kernel_sd, size=theta_prev.shape)\n\n\ndef perturb_logpdf(theta_prop: np.ndarray, theta_prev: np.ndarray) -&gt; float:\n    theta_prop = np.asarray(theta_prop, dtype=float)\n    theta_prev = np.asarray(theta_prev, dtype=float)\n    z = (theta_prop - theta_prev) / kernel_sd\n    return float(-0.5 * np.sum(z**2))  # (drop constants)\n\n\nrng = np.random.default_rng(2)\n\nres_smc = smc_abc(\n    prior_sample=prior_sample,\n    prior_logpdf=prior_logpdf,\n    simulate=simulate,\n    distance=distance,\n    y_obs=y_obs,\n    summary=summary,\n    epsilons=np.array([2.0, 1.6, 1.3], dtype=float),\n    n_particles=150,\n    max_trials_per_round=80_000,\n    perturb=perturb,\n    perturb_logpdf=perturb_logpdf,\n    rng=rng,\n)\n\n# Report the final-round beta posterior summary (weighted particles).\nlogbeta_final = res_smc.thetas[-1, :, 0]\nw_final = res_smc.weights[-1]\n\nbeta_final = np.exp(logbeta_final)\n\n\ndef ess(w: np.ndarray) -&gt; float:\n    w = np.asarray(w, dtype=float)\n    return float(1.0 / np.sum(w**2))\n\n\ndef weighted_quantile(x: np.ndarray, w: np.ndarray, q: float) -&gt; float:\n    x = np.asarray(x, dtype=float)\n    w = np.asarray(w, dtype=float)\n    idx = np.argsort(x)\n    x = x[idx]\n    w = w[idx]\n    cdf = np.cumsum(w) / np.sum(w)\n    return float(x[np.searchsorted(cdf, q, side=\"left\")])\n\n\nprint(f\"eps schedule: {res_smc.epsilons}\")\nprint(f\"beta_true: {beta_true:.3f}\")\nprint(f\"final-round ESS: {ess(w_final):.1f} / {len(w_final)}\")\nprint(\n    \"SMC-ABC beta (final round): \"\n    f\"mean={np.sum(w_final * beta_final):.3f}, \"\n    f\"q10={weighted_quantile(beta_final, w_final, 0.1):.3f}, \"\n    f\"q90={weighted_quantile(beta_final, w_final, 0.9):.3f}\"\n)\n\neps schedule: [2.  1.6 1.3]\nbeta_true: 0.350\nfinal-round ESS: 145.1 / 150\nSMC-ABC beta (final round): mean=0.320, q10=0.273, q90=0.364\n\n\n\nfrom diff_epi_inference.plotting import plot_hist_1d\n\nfig, ax = plot_hist_1d(\n    beta_final,\n    weights=w_final,\n    true_value=beta_true,\n    bins=30,\n    title=f\"SMC-ABC posterior (final round), ESS={ess(w_final):.1f}/{len(w_final)}\",\n    xlabel=\"β\",\n)\nfig\n\n\n\n\nSMC-ABC: final-round weighted histogram for \\(\\beta\\) (dashed line = \\(\\beta_{true}\\)).\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\nThe perturbation kernel above is a simple Gaussian random walk on \\(\\log \\beta\\); in practice, you would adapt its scale using the weighted particle covariance.\nThe tolerance schedule is hand-picked for this toy example. A common approach is to set each \\(\\varepsilon_t\\) to a quantile of the previous round’s accepted distances.\n\n\n\n\n9.1.2 4) Synthetic likelihood (on summaries)\n\nAssume \\(s(y) \\mid \\theta \\approx \\mathcal{N}(\\mu_\\theta, \\Sigma_\\theta)\\).\nEstimate \\((\\mu_\\theta, \\Sigma_\\theta)\\) via repeated simulations at fixed \\(\\theta\\).\nUse the resulting approximate likelihood inside MH/HMC.\n\n\n9.1.2.1 Demo: beta-only synthetic likelihood on the same summaries\nThis is a tiny “likelihood-on-summaries” baseline:\n\nFor a proposed \\(\\theta\\) (here \\(\\theta = \\log \\beta\\)), run the stochastic simulator multiple times.\nCompute the same summary vector \\(s(y)\\) used in the ABC demo.\nFit a Gaussian \\(\\mathcal{N}(\\mu_\\theta, \\Sigma_\\theta)\\) to the simulated summaries.\nUse the resulting synthetic likelihood \\(p(s(y_{\\mathrm{obs}}) \\mid \\theta)\\) inside random-walk MH.\n\n\nimport numpy as np\n\nfrom diff_epi_inference import SEIRParams\nfrom diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings\nfrom diff_epi_inference.pipeline import simulate_seir_and_report_stochastic\nfrom diff_epi_inference.synthetic_likelihood import estimate_summary_gaussian, mvn_logpdf\n\n# Reuse the observed data / params from the ABC block above.\n# (Quarto executes code blocks in order, so y_obs and params_true exist here.)\n\ns_obs = summary(y_obs)\n\n\ndef simulate_summary_once(logbeta: float, *, rng: np.random.Generator) -&gt; np.ndarray:\n    beta = float(np.exp(logbeta))\n    params = SEIRParams(beta=beta, sigma=params_true.sigma, gamma=params_true.gamma)\n    ds = simulate_seir_and_report_stochastic(\n        params=params,\n        s0=10_000,\n        e0=3,\n        i0=2,\n        r0=0,\n        dt=1.0,\n        steps=steps,\n        reporting_rate=0.25,\n        rng=rng,\n    )\n    return summary(ds.y.astype(float))\n\n\ndef estimate_mu_cov(\n    logbeta: float,\n    *,\n    rng: np.random.Generator,\n    n_sims: int = 12,\n    cov_jitter: float = 1e-6,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Estimate (mu, cov) of summaries s(y) at fixed logbeta via repeated sims.\"\"\"\n    return estimate_summary_gaussian(\n        lambda r: simulate_summary_once(logbeta, rng=r),\n        n_sims=n_sims,\n        rng=rng,\n        cov_jitter=cov_jitter,\n    )\n\n\nrng = np.random.default_rng(1)\n\n\ndef log_prior(theta: np.ndarray) -&gt; float:\n    (logbeta,) = np.asarray(theta, dtype=float)\n    z = (logbeta - logbeta_prior_mean) / logbeta_prior_sd\n    return float(-0.5 * z**2)  # (drop constants)\n\n\ndef log_synth_lik(theta: np.ndarray, rng: np.random.Generator) -&gt; float:\n    (logbeta,) = np.asarray(theta, dtype=float)\n    mu, cov = estimate_mu_cov(float(logbeta), rng=rng)\n    return mvn_logpdf(s_obs, mu, cov)\n\n\ndef log_posterior(theta: np.ndarray, rng: np.random.Generator) -&gt; float:\n    lp = log_prior(theta)\n    if not np.isfinite(lp):\n        return -np.inf\n    ll = log_synth_lik(theta, rng)\n    return lp + ll\n\n\n# A very small chain for demonstration (synthetic likelihood is expensive per step).\nchain = random_walk_metropolis_hastings(\n    log_prob_fn=lambda theta: log_posterior(theta, rng),\n    x0=np.array([logbeta_prior_mean], dtype=float),\n    n_steps=500,\n    proposal_std=np.array([0.15], dtype=float),\n    rng=rng,\n)\n\nbeta_chain = np.exp(chain.chain[100:, 0])\n\naccept_rate = float(np.mean(chain.accepted))\nprint(f\"MH accept rate: {accept_rate:.3f}\")\nprint(\n    \"synthetic-lik beta: \"\n    f\"mean={np.mean(beta_chain):.3f}, sd={np.std(beta_chain):.3f}, \"\n    f\"q10={np.quantile(beta_chain, 0.1):.3f}, q90={np.quantile(beta_chain, 0.9):.3f}\"\n)\n\nMH accept rate: 0.222\nsynthetic-lik beta: mean=0.348, sd=0.019, q10=0.328, q90=0.373\n\n\n\nfrom diff_epi_inference.plotting import plot_trace\n\nfig, ax = plot_trace(\n    beta_chain,\n    true_value=beta_true,\n    title=\"Synthetic likelihood MH trace (β)\",\n)\nfig\n\n\n\n\nSynthetic likelihood: MH trace for \\(\\beta\\) (post burn-in).\n\n\n\n\n\n\n\n\n\n\n\n\nrng_ppc = np.random.default_rng(123)\n\n# Draw a few posterior samples of logbeta (post burn-in) for posterior predictive checks.\npost_logbeta = chain.chain[100:, 0]\nidx = rng_ppc.choice(post_logbeta.size, size=12, replace=False)\nlogbeta_draws = post_logbeta[idx]\n\n# Simulate trajectories and overlay them.\ndraws = []\n\nfor logb in logbeta_draws:\n    params = SEIRParams(beta=float(np.exp(logb)), sigma=params_true.sigma, gamma=params_true.gamma)\n    ds = simulate_seir_and_report_stochastic(\n        params=params,\n        s0=10_000,\n        e0=3,\n        i0=2,\n        r0=0,\n        dt=1.0,\n        steps=steps,\n        reporting_rate=0.25,\n        rng=rng_ppc,\n    )\n    draws.append(ds.y.astype(float))\n\nfrom diff_epi_inference.plotting import plot_ppc_overlay\n\nfig, ax = plot_ppc_overlay(\n    t=np.arange(steps, dtype=float),\n    observed=y_obs,\n    draws=np.asarray(draws),\n    title=\"Posterior predictive overlay\",\n)\nfig\n\n\n\n\nSynthetic likelihood: posterior predictive overlay (a few draws).\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\nThis is conceptually simple but computationally heavy: each MH step requires multiple simulator runs.\nIn practice you would typically (i) use common random numbers, (ii) reuse simulations across nearby \\(\\theta\\), or (iii) move to more modern amortised SBI methods.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Likelihood-free baselines</span>"
    ]
  },
  {
    "objectID": "likelihood-free-baselines.html#notes",
    "href": "likelihood-free-baselines.html#notes",
    "title": "9  Likelihood-free baselines",
    "section": "9.2 Notes",
    "text": "9.2 Notes\n\nThese baselines are intentionally simple and prioritise clarity over efficiency.\nSummary choice and calibration should always be revisited when moving to higher-dimensional settings.\n\n\n\n\n\nBeaumont, Mark A, Wenyang Zhang, and David J Balding. 2002. “Approximate Bayesian Computation in Population Genetics.” Genetics 162 (4): 2025–35.\n\n\nWood, Simon N. 2010. “Statistical Inference for Noisy Nonlinear Ecological Dynamic Systems.” Nature 466 (7310): 1102–4.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Likelihood-free baselines</span>"
    ]
  },
  {
    "objectID": "modern-sbi.html",
    "href": "modern-sbi.html",
    "title": "10  Modern simulation-based inference",
    "section": "",
    "text": "10.1 Chapter map\nThis chapter starts the modern SBI track: amortised neural posterior / likelihood / ratio estimation and conditional density models.\nThe goal is to provide a minimal, runnable baseline pipeline and show how to evaluate it (calibration, posterior predictive checks, and diagnostics).\nThis chapter follows the modern SBI framing in (Cranmer, Brehmer, and Louppe 2020; Papamakarios, Sterratt, and Murray 2019) and keeps the implementation deliberately transparent.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modern simulation-based inference</span>"
    ]
  },
  {
    "objectID": "modern-sbi.html#chapter-map",
    "href": "modern-sbi.html#chapter-map",
    "title": "10  Modern simulation-based inference",
    "section": "",
    "text": "Flow mechanics with an explicit change-of-variables example.\nEnd-to-end beta-only NPE loop (simulation bank, conditional density fit, posterior sampling).\nLightweight JAX conditional density training loop.\nPosterior predictive and amortised-vs-local diagnostics.\n\n\n10.1.0.1 A tiny flow “from scratch” (1D + 2D)\nA normalising flow builds a flexible density by transforming a simple base random variable \\(z \\sim p_Z(z)\\) (often standard normal) through an invertible map:\n\\[x = f_\\phi(z), \\qquad z = f_\\phi^{-1}(x).\\]\nThe change-of-variables formula gives\n\\[\\log p_X(x) = \\log p_Z\\bigl(f_\\phi^{-1}(x)\\bigr) + \\log\\left|\\det \\nabla_x f_\\phi^{-1}(x)\\right|.\\]\nBelow we implement two tiny examples in pure NumPy:\n\n1D affine flow (shift + scale), which is “just” a learned Gaussian.\n2D coupling flow (RealNVP-style), which introduces non-trivial dependencies.\n\n\n\n\n\n\n\nNote\n\n\n\nThese are deliberately minimal and not meant to be a performant flow library. The point is to make the invertibility and log-determinant Jacobian bookkeeping concrete.\n\n\n\nimport numpy as np\n\nrng = np.random.default_rng(0)\n\n\n10.1.0.1.1 1D affine flow\n\ndef stdnorm_logpdf(z: np.ndarray) -&gt; np.ndarray:\n    return -0.5 * (z**2 + np.log(2 * np.pi))\n\n\ndef affine_forward(z: np.ndarray, mu: float, log_sigma: float) -&gt; np.ndarray:\n    \"\"\"x = mu + exp(log_sigma) * z\"\"\"\n    return mu + np.exp(log_sigma) * z\n\n\ndef affine_inverse(x: np.ndarray, mu: float, log_sigma: float) -&gt; np.ndarray:\n    \"\"\"z = (x - mu) / exp(log_sigma)\"\"\"\n    return (x - mu) * np.exp(-log_sigma)\n\n\ndef affine_log_prob(x: np.ndarray, mu: float, log_sigma: float) -&gt; np.ndarray:\n    \"\"\"log p_X(x) induced by z~N(0,1), x = mu + sigma z.\"\"\"\n    z = affine_inverse(x, mu=mu, log_sigma=log_sigma)\n    # For 1D: log|det d/dx f^{-1}(x)| = log(1/sigma) = -log_sigma\n    return stdnorm_logpdf(z) - log_sigma\n\n\n# Toy data: non-standard normal\nx_data = rng.normal(loc=2.0, scale=0.7, size=2_000)\n\n# MLE for Gaussian = match sample mean/std.\nmu_hat = float(x_data.mean())\nlog_sigma_hat = float(np.log(x_data.std(ddof=0)))\n\nnll = lambda mu, ls: float(-affine_log_prob(x_data, mu=mu, log_sigma=ls).mean())\n\nprint({\n    \"mu_hat\": mu_hat,\n    \"sigma_hat\": float(np.exp(log_sigma_hat)),\n    \"NLL(base std normal)\": nll(mu=0.0, ls=0.0),\n    \"NLL(fitted affine flow)\": nll(mu=mu_hat, ls=log_sigma_hat),\n})\n\n{'mu_hat': 1.9803820899009006, 'sigma_hat': 0.7001382390069695, 'NLL(base std normal)': 3.124991921064692, 'NLL(fitted affine flow)': 1.0624610540641546}\n\n\n\n\n10.1.0.1.2 2D coupling flow (one coupling layer)\nA classic coupling layer keeps part of the vector unchanged and uses it to scale/shift the rest. For \\(x=(x_1,x_2)\\), one simple form is\n\\[y_1 = x_1, \\qquad y_2 = x_2\\,\\exp(s(x_1)) + t(x_1).\\]\nThis is always invertible as long as \\(\\exp(s(x_1))&gt;0\\), and its Jacobian determinant is cheap:\n\\[\\log |\\det \\nabla_x f(x)| = s(x_1).\\]\n\ndef coupling_forward(x: np.ndarray, a: float, b: float) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"One 2D coupling layer.\n\n    y1 = x1\n    y2 = x2 * exp(a*x1) + b*x1\n\n    Returns: (y, log_det_J) where log_det_J is per-sample.\n    \"\"\"\n    x1 = x[:, 0]\n    x2 = x[:, 1]\n    s = a * x1\n    t = b * x1\n    y1 = x1\n    y2 = x2 * np.exp(s) + t\n    y = np.stack([y1, y2], axis=1)\n    log_det = s  # per-sample\n    return y, log_det\n\n\ndef coupling_inverse(y: np.ndarray, a: float, b: float) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Inverse of coupling_forward.\n\n    x1 = y1\n    x2 = (y2 - b*x1) * exp(-a*x1)\n\n    Returns: (x, log_det_J_inv) where log_det_J_inv is per-sample.\n    \"\"\"\n    y1 = y[:, 0]\n    y2 = y[:, 1]\n    s = a * y1\n    t = b * y1\n    x1 = y1\n    x2 = (y2 - t) * np.exp(-s)\n    x = np.stack([x1, x2], axis=1)\n    log_det_inv = -s\n    return x, log_det_inv\n\n\ndef stdnorm2_logpdf(z: np.ndarray) -&gt; np.ndarray:\n    return -0.5 * (np.sum(z**2, axis=1) + 2 * np.log(2 * np.pi))\n\n\ndef coupling_flow_log_prob(y: np.ndarray, a: float, b: float) -&gt; np.ndarray:\n    # y = f(x), with base density on x ~ N(0, I)\n    x, log_det_inv = coupling_inverse(y, a=a, b=b)\n    return stdnorm2_logpdf(x) + log_det_inv\n\n\n# Draw samples by pushing base samples through the coupling layer\nz = rng.normal(size=(5_000, 2))\nflow_samples, log_det = coupling_forward(z, a=0.8, b=0.5)\n\nprint({\n    \"samples_mean\": flow_samples.mean(axis=0).round(3).tolist(),\n    \"samples_cov\": np.cov(flow_samples.T).round(3).tolist(),\n    \"avg_log_det\": float(log_det.mean()),\n})\n\n{'samples_mean': [0.014, -0.019], 'samples_cov': [[0.995, 0.403], [0.403, 3.771]], 'avg_log_det': 0.01122702511566227}\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(4, 4))\nax.scatter(flow_samples[::10, 0], flow_samples[::10, 1], s=5, alpha=0.3)\nax.set_title(\"Samples from a tiny 2D coupling flow\")\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\nplt.show()\n\n\n\n\n\n\n\n\nIn practice, learned coupling flows use neural nets to represent \\(s(\\cdot)\\) and \\(t(\\cdot)\\) (and stack many layers, alternating which coordinates are transformed), but the bookkeeping is the same.\n\n\n\n10.1.1 3) NPE pipeline (end-to-end)\n\nGenerate training pairs \\((\\theta_i, y_i)\\) from the simulator + prior.\nChoose an encoder for time series \\(y\\) (summaries first; later a small neural encoder).\nTrain a conditional density model for \\(\\theta\\mid y\\).\nEvaluate on held-out simulations.\n\n\n10.1.1.1 Demo: beta-only “NPE” with the package conditional-flow helper (no deep learning)\nBefore we use a full conditional flow, it helps to see the basic NPE loop in the simplest possible form. We will:\n\nSample \\(\\theta_i = \\log \\beta_i\\) from a prior.\nSimulate a stochastic SEIR observation \\(y_i\\).\nCompress \\(y_i\\) to a low-dimensional summary vector \\(s_i\\).\nFit a conditional density model \\[q_\\phi(\\log \\beta \\mid s) = \\mathcal{N}(\\mu_\\phi(s),\\,\\sigma_\\phi(s)^2).\\]\n\nHere \\(\\mu_\\phi(s)\\) is just a linear regression and \\(\\sigma_\\phi\\) is a single global scale. This is not a powerful model, but it is a runnable baseline with exactly the same control flow as neural NPE.\nIn code, we will use the package’s ConditionalAffineDiagNormal helper to fit this model in closed form.\n\nimport numpy as np\n\nfrom diff_epi_inference import SEIRParams\nfrom diff_epi_inference.pipeline import simulate_seir_and_report_stochastic\n\nrng = np.random.default_rng(0)\n\n# --- Synthetic \"observed\" dataset ---\nbeta_true = 0.35\nparams_true = SEIRParams(beta=beta_true, sigma=1 / 4.0, gamma=1 / 6.0)\nsteps = 80\n\nds_obs = simulate_seir_and_report_stochastic(\n    params=params_true,\n    s0=10_000,\n    e0=3,\n    i0=2,\n    r0=0,\n    dt=1.0,\n    steps=steps,\n    reporting_rate=0.25,\n    rng=rng,\n)\n\ny_obs = ds_obs.y.astype(float)\n\n\ndef summary(y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"A tiny hand-crafted encoder for a reported-incidence time series.\"\"\"\n    y = np.asarray(y, dtype=float)\n    peak_t = int(np.argmax(y))\n    return np.array([np.sum(y), np.max(y), peak_t], dtype=float)\n\n\ns_obs = summary(y_obs)\nprint({\"beta_true\": beta_true, \"s_obs\": s_obs.round(3).tolist()})\n\n{'beta_true': 0.35, 's_obs': [1138.0, 59.0, 77.0]}\n\n\n\n# --- Prior over log(beta) ---\nlogbeta_prior_mean = float(np.log(0.3))\nlogbeta_prior_sd = 0.35\n\n\ndef prior_sample_logbeta(rng: np.random.Generator) -&gt; float:\n    return float(rng.normal(loc=logbeta_prior_mean, scale=logbeta_prior_sd))\n\n\ndef simulate_y_from_logbeta(logbeta: float, *, rng: np.random.Generator) -&gt; np.ndarray:\n    beta = float(np.exp(logbeta))\n    params = SEIRParams(beta=beta, sigma=params_true.sigma, gamma=params_true.gamma)\n    ds = simulate_seir_and_report_stochastic(\n        params=params,\n        s0=10_000,\n        e0=3,\n        i0=2,\n        r0=0,\n        dt=1.0,\n        steps=steps,\n        reporting_rate=0.25,\n        rng=rng,\n    )\n    return ds.y.astype(float)\n\n\ndef make_training_data(n_sims: int, *, rng: np.random.Generator) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return (S, logbeta) with S shape (n_sims, d).\"\"\"\n    S = np.zeros((n_sims, 3), dtype=float)\n    logb = np.zeros((n_sims,), dtype=float)\n    for i in range(n_sims):\n        lb = prior_sample_logbeta(rng)\n        y = simulate_y_from_logbeta(lb, rng=rng)\n        S[i] = summary(y)\n        logb[i] = lb\n    return S, logb\n\n\nS_train, logb_train = make_training_data(800, rng=np.random.default_rng(1))\nS_test, logb_test = make_training_data(200, rng=np.random.default_rng(2))\n\nprint({\"S_train_shape\": S_train.shape, \"logb_train_mean\": float(logb_train.mean())})\n\n{'S_train_shape': (800, 3), 'logb_train_mean': -1.2095443448622796}\n\n\n\nfrom diff_epi_inference.flows import ConditionalAffineDiagNormal\n\n# Fit q(logbeta | s) as a *conditional affine flow* (a diagonal Gaussian)\n# in closed form.\nflow = ConditionalAffineDiagNormal.fit_closed_form(\n    contexts=S_train,\n    thetas=logb_train[:, None],\n)\n\n# Predict mean log(beta) on a test set (for a sanity check).\nmu_test = flow.mean(S_test)[:, 0]\nrmse = float(np.sqrt(np.mean((mu_test - logb_test) ** 2)))\n\nprint({\"sigma_hat\": float(np.exp(flow.log_sigma[0])), \"test_RMSE_logbeta\": rmse})\n\n{'sigma_hat': 0.13429620838685563, 'test_RMSE_logbeta': 0.13001736854755003}\n\n\n\n# Sample from the learned posterior q(logbeta | s_obs).\nlogb_post = flow.sample(s_obs, 5_000, rng=np.random.default_rng(3))[:, 0]\nbeta_post = np.exp(logb_post)\n\nprint(\n    {\n        \"beta_true\": beta_true,\n        \"beta_post_mean\": float(np.mean(beta_post)),\n        \"beta_post_q10\": float(np.quantile(beta_post, 0.1)),\n        \"beta_post_q90\": float(np.quantile(beta_post, 0.9)),\n    }\n)\n\n{'beta_true': 0.35, 'beta_post_mean': 0.3545202252634459, 'beta_post_q10': 0.2961454270383665, 'beta_post_q90': 0.418358659968864}\n\n\nThis model is intentionally crude: the posterior width here is driven mostly by the regression residuals. The same NPE workflow can be extended to more expressive conditional flows when non-Gaussian posteriors are needed.\n\n\n10.1.1.2 Tiny JAX training loop (behind modern-sbi)\nThe point of this section is not to introduce new modelling ideas. It is just a minimal, end-to-end example of the mechanics we will reuse for neural NPE:\n\ndefine a parametric model\nwrite a loss (negative log-likelihood)\ntake gradients\napply an optimiser update\n\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\n\n# Reuse the (S_train, logb_train) data defined above.\nX = jnp.asarray(S_train)\ny = jnp.asarray(logb_train)\n\n# Standardise inputs (same idea as above).\nx_mean = X.mean(axis=0)\nx_std = jnp.where(X.std(axis=0) &gt; 0, X.std(axis=0), 1.0)\nXn = (X - x_mean) / x_std\n\n\nclass CondGaussian(eqx.Module):\n    net: eqx.nn.MLP\n\n    def __call__(self, x: jnp.ndarray) -&gt; tuple[jnp.ndarray, jnp.ndarray]:\n        # Predict (mu, log_sigma) for log(beta) given summaries.\n        out = self.net(x)\n        mu = out[0]\n        log_sigma = out[1]\n        return mu, log_sigma\n\n\nkey = jax.random.PRNGKey(0)\nmodel = CondGaussian(\n    net=eqx.nn.MLP(\n        in_size=3,\n        out_size=2,\n        width_size=16,\n        depth=2,\n        activation=jax.nn.tanh,\n        key=key,\n    )\n)\n\n\ndef nll_one(params: CondGaussian, x: jnp.ndarray, y: jnp.ndarray) -&gt; jnp.ndarray:\n    mu, log_sigma = params(x)\n    # Add a small floor for numerical stability.\n    log_sigma = jnp.maximum(log_sigma, -6.0)\n    sigma2 = jnp.exp(2.0 * log_sigma)\n    return 0.5 * ((y - mu) ** 2 / sigma2 + 2.0 * log_sigma + jnp.log(2.0 * jnp.pi))\n\n\ndef loss(params: CondGaussian, xb: jnp.ndarray, yb: jnp.ndarray) -&gt; jnp.ndarray:\n    return jnp.mean(jax.vmap(nll_one, in_axes=(None, 0, 0))(params, xb, yb))\n\n\nopt = optax.adam(1e-2)\nopt_state = opt.init(eqx.filter(model, eqx.is_array))\n\n\n@eqx.filter_jit\ndef step(params: CondGaussian, opt_state, xb: jnp.ndarray, yb: jnp.ndarray):\n    l, grads = eqx.filter_value_and_grad(loss)(params, xb, yb)\n    updates, opt_state = opt.update(grads, opt_state, params)\n    params = eqx.apply_updates(params, updates)\n    return params, opt_state, l\n\n\n# Tiny SGD loop.\nbatch_size = 128\nn_steps = 200\nkey, subkey = jax.random.split(key)\nidx = jax.random.permutation(subkey, Xn.shape[0])\n\nfor t in range(n_steps):\n    i0 = (t * batch_size) % Xn.shape[0]\n    batch_idx = idx[i0 : i0 + batch_size]\n    xb = Xn[batch_idx]\n    yb = y[batch_idx]\n    model, opt_state, l = step(model, opt_state, xb, yb)\n\n# Quick sanity check: does the trained model reduce NLL vs random initialisation?\nfinal_nll = float(loss(model, Xn, y))\nprint({\"final_train_NLL\": final_nll})\n\n{'final_train_NLL': -0.9696986079216003}\n\n\n\n\n10.1.1.3 End-to-end: JAX conditional Gaussian NPE (posterior + posterior predictive)\nThis section turns the tiny training loop above into a complete, deterministic workflow:\n\ntrain a conditional Gaussian for \\(\\log\\beta \\mid s(y)\\)\nsample an approximate posterior given \\(y_{\\text{obs}}\\)\nrun a small posterior predictive check (PPC)\n\nThe goal is not state-of-the-art performance; it is a runnable template you can adapt.\n\nimport distrax\n\n# Re-train deterministically (fresh init + fixed steps).\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\nmodel2 = CondGaussian(\n    net=eqx.nn.MLP(\n        in_size=3,\n        out_size=2,\n        width_size=16,\n        depth=2,\n        activation=jax.nn.tanh,\n        key=subkey,\n    )\n)\n\nopt2 = optax.adam(1e-2)\nopt_state2 = opt2.init(eqx.filter(model2, eqx.is_array))\n\nbatch_size = 128\nn_steps = 300\nkey, subkey = jax.random.split(key)\nidx = jax.random.permutation(subkey, Xn.shape[0])\n\nfor t in range(n_steps):\n    i0 = (t * batch_size) % Xn.shape[0]\n    batch_idx = idx[i0 : i0 + batch_size]\n    xb = Xn[batch_idx]\n    yb = y[batch_idx]\n    model2, opt_state2, l = step(model2, opt_state2, xb, yb)\n\n# --- Approx posterior for log(beta) given observed summaries ---\ns_obs_j = jnp.asarray(s_obs)\ns_obs_n = (s_obs_j - x_mean) / x_std\nmu_hat, log_sigma_hat = model2(s_obs_n)\nlog_sigma_hat = jnp.maximum(log_sigma_hat, -6.0)\n\nq = distrax.Normal(loc=mu_hat, scale=jnp.exp(log_sigma_hat))\n\nkey, subkey = jax.random.split(key)\nlogb_samps = np.asarray(q.sample(seed=subkey, sample_shape=(2_000,)))\nbeta_samps = np.exp(logb_samps)\n\nprint(\n    {\n        \"beta_true\": beta_true,\n        \"beta_post_mean\": float(np.mean(beta_samps)),\n        \"beta_post_q05\": float(np.quantile(beta_samps, 0.05)),\n        \"beta_post_q95\": float(np.quantile(beta_samps, 0.95)),\n    }\n)\n\n# --- Tiny posterior predictive check on the summary space ---\nrng_ppc = np.random.default_rng(0)\nn_ppc = 80\nS_ppc = np.zeros((n_ppc, 3), dtype=float)\nfor i in range(n_ppc):\n    beta_i = float(beta_samps[i])\n    params_i = SEIRParams(beta=beta_i, sigma=params_true.sigma, gamma=params_true.gamma)\n    ds_i = simulate_seir_and_report_stochastic(\n        params=params_i,\n        s0=10_000,\n        e0=3,\n        i0=2,\n        r0=0,\n        dt=1.0,\n        steps=steps,\n        reporting_rate=0.25,\n        rng=rng_ppc,\n    )\n    S_ppc[i] = summary(ds_i.y)\n\n# Compare observed summary to PPC distribution (just a few quantiles).\nq_lo = np.quantile(S_ppc, 0.1, axis=0)\nq_hi = np.quantile(S_ppc, 0.9, axis=0)\nprint(\n    {\n        \"ppc_q10\": q_lo.round(2).tolist(),\n        \"ppc_q90\": q_hi.round(2).tolist(),\n        \"s_obs\": s_obs.round(2).tolist(),\n    }\n)\n\n{'beta_true': 0.35, 'beta_post_mean': 0.3329930901527405, 'beta_post_q05': 0.3035953938961029, 'beta_post_q95': 0.3644125163555145}\n{'ppc_q10': [247.5, 21.7, 69.0], 'ppc_q90': [1437.1, 78.1, 79.0], 's_obs': [1138.0, 59.0, 77.0]}\n\n\n\n\n\n10.1.2 3b) NLE: neural likelihood estimation\nIn NLE we learn an explicit surrogate for the likelihood,\n\\[q_\\phi(y\\mid\\theta) \\approx p(y\\mid\\theta),\\]\ntypically using a conditional density model (often a conditional flow). Once we can evaluate \\(\\log q_\\phi(y_{\\text{obs}}\\mid\\theta)\\), we can do standard Bayesian inference with MCMC:\n\\[\\log p(\\theta\\mid y_{\\text{obs}}) = \\log p(\\theta) + \\log q_\\phi(y_{\\text{obs}}\\mid\\theta) + \\text{const}.\\]\nPractical notes:\n\nIn many problems, \\(y\\) is high-dimensional; in that case NLE is often done on summaries \\(s(y)\\), learning \\(q_\\phi(s\\mid\\theta)\\).\nCompared to NPE, NLE often makes it easier to reuse a learned likelihood across different priors (since the likelihood itself is prior-independent).\n\nA minimal skeleton (pseudo-code) looks like:\n# Train on simulator output:\n#   (theta_i, y_i) ~ p(theta) p(y|theta)\n# Fit conditional density model q_phi(y|theta)\n\ndef log_posterior(theta, y_obs):\n    return log_prior(theta) + flow_like.log_prob(y_obs, context=theta)\n\n# Then run any MCMC method (Metropolis, HMC, slice, ...)\n# targeting log_posterior(., y_obs).\nThis handbook focuses first on NPE for clarity; NLE uses the same simulation bank pattern and becomes practical once you have a conditional density model for the observation space (or summary space).\n\n\n10.1.3 3c) NRE: neural ratio estimation\nIn NRE we learn the likelihood-to-evidence ratio\n\\[r(\\theta, y) = \\frac{p(y\\mid\\theta)}{p(y)},\\]\nusually by reducing ratio estimation to binary classification. A common construction is:\n\ndraw joint samples \\((\\theta, y) \\sim p(\\theta) p(y\\mid\\theta)\\) and label them 1\ndraw marginal/product samples \\((\\theta, y') \\sim p(\\theta) p(y)\\) (e.g. by shuffling \\(y\\)) and label them 0\ntrain a classifier \\(d_\\phi(\\theta, y) \\approx \\Pr(\\text{joint}=1\\mid\\theta, y)\\)\n\nThen the estimated ratio is\n\\[\\hat r_\\phi(\\theta, y) = \\frac{d_\\phi(\\theta, y)}{1 - d_\\phi(\\theta, y)}.\\]\nThis gives a posterior via\n\\[p(\\theta\\mid y_{\\text{obs}}) \\propto p(\\theta)\\,\\hat r_\\phi(\\theta, y_{\\text{obs}}).\\]\nNRE is attractive when you want a flexible posterior but do not want to model a full conditional density. As with NLE, it is common to feed the classifier summaries \\(s(y)\\) (or learned embeddings) rather than raw \\(y\\).\nA minimal skeleton (pseudo-code) looks like:\n# Make training pairs for a joint-vs-product classifier.\n# Joint: (theta, y) ~ p(theta) p(y|theta)\n# Product: (theta, y') ~ p(theta) p(y)  (e.g. shuffle y within a batch)\n\ndef log_ratio_hat(theta, y):\n    # d_phi(theta, y) is the classifier probability of \"joint\".\n    d = classifier.predict_proba(theta, y)\n    return np.log(d) - np.log1p(-d)  # logit(d)\n\n# Posterior (up to normalisation): log p(theta) + log r_hat(theta, y_obs)\n\ndef log_posterior(theta, y_obs):\n    return log_prior(theta) + log_ratio_hat(theta, y_obs)\n\n# Then sample theta from log_posterior(., y_obs) using MCMC.\n\n\n10.1.4 4) Diagnostics and calibration\n\nPosterior predictive checks (PPC).\nCoverage / calibration checks (small SBC-style smoke test).\nFailure modes: misspecification, simulation budget, overconfident posteriors.\n\n\n\n10.1.5 5) Amortised vs local inference\nThere are two common ways to spend a simulation budget when learning a conditional density model (e.g. an NPE-style estimator):\n\nAmortised: train once on samples from the prior predictive, then reuse the trained model for many observations.\nLocal / sequential: for a single observation, concentrate simulations around the region of parameter space that seems plausible for that observation (often via rounds of proposals).\n\nBelow is a tiny comparison on the beta-only running example, using the same simple conditional Gaussian model from the earlier section.\nThe goal is not to claim strong performance, but to make the trade-off concrete:\n\namortised training uses a larger up-front simulation budget, but inference per new dataset is cheap\nlocal training can use fewer simulations overall if you only care about one dataset, and can often be more accurate for that dataset when the prior is broad\n\n\nimport numpy as np\n\nfrom diff_epi_inference import plot_series_comparison\nfrom diff_epi_inference.flows import ConditionalAffineDiagNormal\n\n# Reuse from above: prior_sample_logbeta, simulate_y_from_logbeta, summary,\n# as well as the amortised `flow` trained on (S_train, logb_train).\n\n\ndef fit_local_flow(\n    s_obs: np.ndarray,\n    *,\n    flow_init: ConditionalAffineDiagNormal,\n    n_local: int = 400,\n    proposal_sd: float = 0.15,\n    rng: np.random.Generator,\n) -&gt; ConditionalAffineDiagNormal:\n    \"\"\"Fit a *local* conditional model by simulating around an initial guess.\n\n    We use the amortised flow's posterior mean as a proposal centre, then draw\n    log(beta) ~ Normal(center, proposal_sd) and simulate y, returning summaries.\n\n    This is a very small stand-in for a sequential NPE scheme.\n    \"\"\"\n    center = float(flow_init.mean(s_obs[None, :])[0, 0])\n\n    S = np.zeros((n_local, 3), dtype=float)\n    logb = np.zeros((n_local,), dtype=float)\n    for i in range(n_local):\n        lb = float(rng.normal(loc=center, scale=proposal_sd))\n        y = simulate_y_from_logbeta(lb, rng=rng)\n        S[i] = summary(y)\n        logb[i] = lb\n\n    return ConditionalAffineDiagNormal.fit_closed_form(contexts=S, thetas=logb[:, None])\n\n\ndef lognormal_sd(*, mu: float, sigma: float) -&gt; float:\n    \"\"\"SD of exp(X) when X ~ Normal(mu, sigma^2).\"\"\"\n    v = float(sigma**2)\n    return float(np.sqrt((np.exp(v) - 1.0) * np.exp(2.0 * mu + v)))\n\n\ndef ppc_summary_distance(\n    flow_model: ConditionalAffineDiagNormal,\n    s_obs: np.ndarray,\n    *,\n    n: int,\n    rng: np.random.Generator,\n) -&gt; float:\n    \"\"\"PPC score: mean L2 distance between simulated and observed summaries.\"\"\"\n    logb_samp = flow_model.sample(s_obs, n=n, rng=rng)[:, 0]\n    dists = []\n    for lb in logb_samp:\n        y = simulate_y_from_logbeta(float(lb), rng=rng)\n        s = summary(y)\n        dists.append(float(np.linalg.norm(s - s_obs)))\n    return float(np.mean(dists))\n\n\n# Create a few synthetic \"observations\" with different true betas.\nK = 8\nrng_obs = np.random.default_rng(123)\n\nlogb_true = np.array([prior_sample_logbeta(rng_obs) for _ in range(K)], dtype=float)\nbeta_true_vec = np.exp(logb_true)\n\ns_obs_list: list[np.ndarray] = []\nfor k in range(K):\n    yk = simulate_y_from_logbeta(float(logb_true[k]), rng=rng_obs)\n    s_obs_list.append(summary(yk))\n\n\nn_local = 400\nn_ppc = 120\n\n# Compare amortised vs local across point estimate error, posterior width, and PPC score.\nrows = []\nfor k, s_k in enumerate(s_obs_list):\n    mu_am = float(flow.mean(s_k[None, :])[0, 0])\n    beta_hat_am = float(np.exp(mu_am))\n    sd_beta_am = lognormal_sd(mu=mu_am, sigma=float(np.exp(flow.log_sigma[0])))\n    ppc_am = ppc_summary_distance(flow, s_k, n=n_ppc, rng=np.random.default_rng(200 + k))\n\n    flow_loc = fit_local_flow(\n        s_k,\n        flow_init=flow,\n        n_local=n_local,\n        proposal_sd=0.15,\n        rng=np.random.default_rng(10 + k),\n    )\n    mu_loc = float(flow_loc.mean(s_k[None, :])[0, 0])\n    beta_hat_loc = float(np.exp(mu_loc))\n    sd_beta_loc = lognormal_sd(mu=mu_loc, sigma=float(np.exp(flow_loc.log_sigma[0])))\n    ppc_loc = ppc_summary_distance(flow_loc, s_k, n=n_ppc, rng=np.random.default_rng(500 + k))\n\n    rows.append(\n        {\n            \"k\": k,\n            \"beta_true\": float(beta_true_vec[k]),\n            \"beta_hat_am\": beta_hat_am,\n            \"abs_err_am\": float(abs(beta_hat_am - beta_true_vec[k])),\n            \"sd_beta_am\": sd_beta_am,\n            \"ppc_dist_am\": ppc_am,\n            \"beta_hat_loc\": beta_hat_loc,\n            \"abs_err_loc\": float(abs(beta_hat_loc - beta_true_vec[k])),\n            \"sd_beta_loc\": sd_beta_loc,\n            \"ppc_dist_loc\": ppc_loc,\n        }\n    )\n\n\nabs_err_am = np.asarray([r[\"abs_err_am\"] for r in rows], dtype=float)\nabs_err_loc = np.asarray([r[\"abs_err_loc\"] for r in rows], dtype=float)\nppc_am = np.asarray([r[\"ppc_dist_am\"] for r in rows], dtype=float)\nppc_loc = np.asarray([r[\"ppc_dist_loc\"] for r in rows], dtype=float)\n\nmean_err_am = float(np.mean(abs_err_am))\nmean_err_loc = float(np.mean(abs_err_loc))\nmean_ppc_am = float(np.mean(ppc_am))\nmean_ppc_loc = float(np.mean(ppc_loc))\n\nprint(\n    {\n        \"amortised_sims_total\": int(len(S_train)),\n        \"local_sims_per_obs\": int(n_local),\n        \"local_sims_total\": int(n_local * K),\n        \"mean_abs_err_amortised\": mean_err_am,\n        \"mean_abs_err_local\": mean_err_loc,\n        \"mean_ppc_dist_amortised\": mean_ppc_am,\n        \"mean_ppc_dist_local\": mean_ppc_loc,\n    }\n)\n\nfig, ax = plot_series_comparison(\n    t=np.arange(K, dtype=float),\n    observed=abs_err_am,\n    fitted=abs_err_loc,\n    observed_label=\"amortised |error|\",\n    fitted_label=\"local |error|\",\n    title=\"Amortised vs local: absolute beta error by dataset\",\n    ylabel=\"absolute error\",\n)\nfig\n\nfig, ax = plot_series_comparison(\n    t=np.arange(K, dtype=float),\n    observed=ppc_am,\n    fitted=ppc_loc,\n    observed_label=\"amortised PPC distance\",\n    fitted_label=\"local PPC distance\",\n    title=\"Amortised vs local: PPC summary distance by dataset\",\n    ylabel=\"mean L2 summary distance\",\n)\nfig\n\n{'amortised_sims_total': 800, 'local_sims_per_obs': 400, 'local_sims_total': 3200, 'mean_abs_err_amortised': 0.03743904795563384, 'mean_abs_err_local': 0.030592436050012765, 'mean_ppc_dist_amortised': 394.10878384108923, 'mean_ppc_dist_local': 275.6685276886164}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation (for this toy setup):\n\nCompute budget: the amortised model pays an up-front simulation cost (here: len(S_train) sims), while the local approach spends n_local simulations per observation.\nPosterior width: we report an approximate posterior width via sd_beta(·) (treating \\(\\log\\beta\\) as Gaussian, so \\(\\beta\\) is log-normal).\nPPC score: ppc_dist(·) is a tiny posterior predictive check score: simulate datasets from posterior draws and report the mean L2 distance between simulated and observed summaries.\n\nIn practice, sequential NPE schemes use multiple rounds and better proposal adaptation, and the comparison depends strongly on the dimensionality of \\(\\theta\\) and on how broad the prior is.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modern simulation-based inference</span>"
    ]
  },
  {
    "objectID": "modern-sbi.html#notes",
    "href": "modern-sbi.html#notes",
    "title": "10  Modern simulation-based inference",
    "section": "10.2 Notes",
    "text": "10.2 Notes\n\nThis chapter starts with a beta-only running example so the inference mechanics stay transparent.\nThe implementation is intentionally dependency-light; richer integrations can be added without changing the core workflow.\n\n\n\n\n\nCranmer, Kyle, Johann Brehmer, and Gilles Louppe. 2020. “The Frontier of Simulation-Based Inference.” Proceedings of the National Academy of Sciences 117 (48): 30055–62.\n\n\nPapamakarios, George, David Sterratt, and Iain Murray. 2019. “Sequential Neural Posterior Estimation with Autoregressive Flows.” Proceedings of Machine Learning Research 89: 837–48.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modern simulation-based inference</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html",
    "href": "validation-and-robustness.html",
    "title": "11  Validation and robustness",
    "section": "",
    "text": "11.1 A validation workflow (checklist)\nThis chapter collects a repeatable validation workflow you can apply to any inference method in this handbook. It is intentionally method-agnostic: you can use the same checks for classical MCMC, variational inference, ABC, and modern simulation-based inference. The core workflow aligns with simulation-based calibration and modern MCMC diagnostics (Talts et al. 2020; Vehtari et al. 2021).\nWe focus on three pillars:\nWhen you build a new inference pipeline, write down answers to these explicitly:\nIn earlier chapters we already used:\nThis chapter formalises those patterns.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html#a-validation-workflow-checklist",
    "href": "validation-and-robustness.html#a-validation-workflow-checklist",
    "title": "11  Validation and robustness",
    "section": "",
    "text": "What are the scientific quantities of interest? (parameters, forecasts, counterfactuals)\nWhat would “bad” look like? (systematic bias, undercoverage, overconfidence, wrong tails)\nWhat are your minimum diagnostics?\n\nPPC on the raw data scale (or on domain-relevant summaries)\na small calibration / coverage smoke test on synthetic data\n\nWhat are your sensitivity axes? (prior width, observation model, summaries, simulator mismatch)\nWhat will you do if the checks fail? (revise model, increase simulation budget, change summaries, change inference method)\n\n\n\nPPC overlays in the classical baselines,\na tiny calibration/coverage smoke test (SBC-style) on synthetic datasets.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html#worked-diagnostics-mini-suite",
    "href": "validation-and-robustness.html#worked-diagnostics-mini-suite",
    "title": "11  Validation and robustness",
    "section": "11.2 Worked diagnostics mini-suite",
    "text": "11.2 Worked diagnostics mini-suite\nThe cells below create a compact, reproducible validation workflow on synthetic epidemic-like incidence data. The purpose is to show the diagnostics in a single place with visuals.\n\nimport numpy as np\n\nfrom diff_epi_inference import (\n    plot_ppc_overlay,\n    plot_rank_histogram,\n    plot_sensitivity_ranges,\n    plot_summary_intervals,\n)\n\n\nrng = np.random.default_rng(2026)\nt = np.arange(80, dtype=float)\n\n# Synthetic observed series with one main wave.\nmu_obs = 5.0 + 45.0 * np.exp(-0.5 * ((t - 30.0) / 9.0) ** 2)\ny_obs = rng.poisson(mu_obs)\n\n# Posterior predictive draws: allow moderate scale and local-noise variation.\ndraws = np.zeros((200, t.size), dtype=float)\nfor i in range(draws.shape[0]):\n    scale = float(rng.normal(loc=1.0, scale=0.08))\n    mu_i = np.clip(scale * mu_obs + rng.normal(0.0, 2.0, size=t.size), 0.0, None)\n    draws[i] = rng.poisson(mu_i)\n\nfig, ax = plot_ppc_overlay(\n    t=t,\n    observed=y_obs,\n    draws=draws,\n    title=\"Posterior predictive overlay (synthetic incidence)\",\n    max_draws=60,\n)\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef summary(y: np.ndarray) -&gt; np.ndarray:\n    y = np.asarray(y, dtype=float)\n    return np.array([np.sum(y), np.max(y), float(np.argmax(y))], dtype=float)\n\n\ns_obs = summary(y_obs)\ns_draws = np.asarray([summary(row) for row in draws], dtype=float)\n\nfig, ax = plot_summary_intervals(\n    observed=s_obs,\n    draws=s_draws,\n    labels=[\"total burden\", \"peak size\", \"peak timing\"],\n    q_low=0.1,\n    q_high=0.9,\n    title=\"Summary-space PPC intervals\",\n)\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Tiny SBC-style rank check in one dimension.\nn_datasets = 120\nn_post = 200\nranks = np.zeros(n_datasets, dtype=int)\n\nfor i in range(n_datasets):\n    theta_true = float(rng.normal(0.0, 1.0))\n    # Deliberately simple approximate posterior family for demonstration.\n    post_draws = rng.normal(loc=theta_true + rng.normal(0.0, 0.08), scale=0.9, size=n_post)\n    ranks[i] = int(np.sum(post_draws &lt; theta_true))\n\nfig, ax = plot_rank_histogram(\n    ranks,\n    n_ranks=n_post + 1,\n    title=\"SBC rank histogram (smoke-test scale)\",\n)\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlow = np.quantile(s_draws, 0.1, axis=0)\nbase = np.median(s_draws, axis=0)\nhigh = np.quantile(s_draws, 0.9, axis=0)\n\nfig, ax = plot_sensitivity_ranges(\n    labels=[\"total burden\", \"peak size\", \"peak timing\"],\n    low=low,\n    base=base,\n    high=high,\n    title=\"Sensitivity envelope on key summaries\",\n)\nfig",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html#posterior-predictive-checks-ppc",
    "href": "validation-and-robustness.html#posterior-predictive-checks-ppc",
    "title": "11  Validation and robustness",
    "section": "11.3 1) Posterior predictive checks (PPC)",
    "text": "11.3 1) Posterior predictive checks (PPC)\nA posterior predictive check asks:\n\nIf (^{(m)}p(y_{})) and (y{(m)}p(y{(m)})), do the replicated datasets (y^{(m)}) look like (y_{}) in the ways we care about?\n\n\n11.3.1 What PPCs can and cannot tell you\nPPCs are good at detecting:\n\ngross model mismatch (wrong scale, wrong temporal structure),\nmis-specified observation noise (too smooth / too noisy),\nmissing mechanisms that affect salient features (timing of peak, tail decay).\n\nPPCs are not a guarantee that:\n\nparameters are identifiable,\nposteriors are calibrated,\nuncertainty is correctly quantified.\n\nA model can pass a coarse PPC while still producing overconfident posteriors.\n\n\n11.3.2 Choosing PPC statistics\nA practical PPC uses a small set of interpretable features (T(y)), for example:\n\ntotal burden (_t y_t)\npeak height (_t y_t)\npeak timing (_t y_t)\ngrowth rate early in the epidemic\nautocorrelation / day-of-week effects (if relevant)\n\nThen compare (T(y_{})) to the distribution of (T(y^{(m)})).\n\n\n\n\n\n\nNote\n\n\n\nMany of the PPC plots in this handbook use simple predictive envelopes: e.g. 5–95% bands over time. That is often enough to catch glaring problems.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html#simulation-based-calibration-sbc",
    "href": "validation-and-robustness.html#simulation-based-calibration-sbc",
    "title": "11  Validation and robustness",
    "section": "11.4 2) Simulation-based calibration (SBC)",
    "text": "11.4 2) Simulation-based calibration (SBC)\nSBC evaluates the end-to-end pipeline (prior + simulator + inference + posterior). The core idea is:\n\nDraw (^p())\nDraw synthetic data (yp(y))\nRun your inference method to get draws (^{(m)} p(y^)) (or an approximation)\nCompute the rank statistic of (^) among ({^{(m)}})\n\nIf the whole pipeline is correct (and your sampler/estimator is accurate), the rank statistics are uniform.\n\n11.4.1 Rank histograms and coverage\nTwo lightweight SBC summaries:\n\nRank histograms: uniform is good; U-shaped suggests under-dispersed posteriors; dome-shaped suggests over-dispersed posteriors; skew suggests bias.\nEmpirical coverage: across synthetic datasets, does a nominal 90% interval contain (^) about 90% of the time?\n\nIn the early chapters we used a coverage smoke test as a fast proxy for full SBC. The trade-off is compute:\n\nfull SBC: stronger but expensive\ncoverage smoke tests: weaker but cheap and surprisingly effective for catching obvious bugs\n\n\n\n\n\n\n\nWarning\n\n\n\nSBC only tests calibration under the assumed model. If the simulator is misspecified relative to reality, a perfectly calibrated posterior for the wrong model can still be scientifically misleading.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html#sensitivity-analysis",
    "href": "validation-and-robustness.html#sensitivity-analysis",
    "title": "11  Validation and robustness",
    "section": "11.5 3) Sensitivity analysis",
    "text": "11.5 3) Sensitivity analysis\nSensitivity is about stress-testing your conclusions against plausible modelling choices. It is not one thing; it is a family of tests.\n\n11.5.1 3.1 Prior sensitivity\nQuestions to ask:\n\nIf I widen/narrow the prior, do posterior conclusions shift drastically?\nAre posterior intervals dominated by the prior? (a sign of weak identifiability)\nAre conclusions driven by arbitrary bounds (e.g. truncations)?\n\nPractical recipe:\n\nchoose 2–3 prior variants (e.g. base, wider, narrower)\nrerun the pipeline and compare posterior summaries for the scientific targets\nrerun PPCs: if you change priors but PPCs are unchanged, that can indicate the data are informative; if PPCs deteriorate, priors may be preventing fit\n\n\n\n11.5.2 3.2 Summary selection sensitivity (ABC / SBI)\nIf your pipeline uses summaries (s(y)), sensitivity can be severe:\n\nsmall changes in summaries can change identifiability\nsummaries can discard information needed to recover key parameters\n\nPractical recipe:\n\ntry multiple summary sets (e.g. basic moments vs adding peak timing / early growth)\ncheck (i) calibration/coverage on synthetic data and (ii) PPCs on held-out synthetic datasets\n\n\n\n11.5.3 3.3 Simulator discrepancy and misspecification\nCommon misspecification sources in epidemic models:\n\nreporting rate varies over time\ndelays are mis-modelled\ncontact patterns or interventions not captured\nstructural mismatch between the chosen compartmental model and reality\n\nPractical recipe:\n\nadd a simple discrepancy term (e.g. extra observation noise, a time-varying reporting factor)\nrepeat PPCs and check whether predictive fit improves without producing nonsensical parameter posteriors",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "validation-and-robustness.html#key-takeaways",
    "href": "validation-and-robustness.html#key-takeaways",
    "title": "11  Validation and robustness",
    "section": "11.6 Key takeaways",
    "text": "11.6 Key takeaways\n\nPPC, SBC-style checks, and sensitivity analysis should be treated as a single validation workflow.\nGood chain diagnostics do not guarantee scientific validity; calibration and misspecification checks remain essential.\nLightweight smoke diagnostics are useful in CI, but should be complemented by deeper offline validation.\n\n\n\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” arXiv Preprint arXiv:1804.06788.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved \\(\\hat{R}\\) for Assessing Convergence of MCMC.” Bayesian Analysis 16 (2): 667–718.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Validation and robustness</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "12  References",
    "section": "",
    "text": "This handbook cites primary references for inference methods, diagnostics, and differentiable modelling tools used in the chapters.\n\n\n\n\nBaydin, Atilim Gunes, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2018. “Automatic Differentiation in Machine Learning: A Survey.” Journal of Machine Learning Research 18 (153): 1–43.\n\n\nBeaumont, Mark A, Wenyang Zhang, and David J Balding. 2002. “Approximate Bayesian Computation in Population Genetics.” Genetics 162 (4): 2025–35.\n\n\nBlei, David M, Alp Kucukelbir, and Jon D McAuliffe. 2017. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association 112 (518): 859–77.\n\n\nCranmer, Kyle, Johann Brehmer, and Gilles Louppe. 2020. “The Frontier of Simulation-Based Inference.” Proceedings of the National Academy of Sciences 117 (48): 30055–62.\n\n\nGriewank, Andreas, and Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2nd ed. SIAM.\n\n\nHermans, Joeri, Alexandre Delaunoy, François Rozet, Antoine Wehenkel, and Gilles Louppe. 2021. “Averting a Crisis in Simulation-Based Inference.” arXiv Preprint arXiv:2110.06581.\n\n\nHoffman, Matthew D, and Andrew Gelman. 2014. “The No-u-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” Journal of Machine Learning Research 15 (47): 1593–623.\n\n\nJang, Eric, Shixiang Gu, and Ben Poole. 2017. “Categorical Reparameterization with Gumbel-Softmax.” International Conference on Learning Representations.\n\n\nKucukelbir, Alp, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. 2017. “Automatic Differentiation Variational Inference.” Journal of Machine Learning Research 18 (14): 1–45.\n\n\nMaddison, Chris J, Andriy Mnih, and Yee Whye Teh. 2017. “The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.” International Conference on Learning Representations.\n\n\nNeal, Radford M. 2011. MCMC Using Hamiltonian Dynamics. Edited by Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Chapman; Hall/CRC.\n\n\nPapamakarios, George, David Sterratt, and Iain Murray. 2019. “Sequential Neural Posterior Estimation with Autoregressive Flows.” Proceedings of Machine Learning Research 89: 837–48.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” arXiv Preprint arXiv:1804.06788.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved \\(\\hat{R}\\) for Assessing Convergence of MCMC.” Bayesian Analysis 16 (2): 667–718.\n\n\nWilliams, Ronald J. 1992. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.” Machine Learning 8: 229–56.\n\n\nWood, Simon N. 2010. “Statistical Inference for Noisy Nonlinear Ecological Dynamic Systems.” Nature 466 (7310): 1102–4.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>References</span>"
    ]
  }
]