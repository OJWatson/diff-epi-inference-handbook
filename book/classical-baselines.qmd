---
title: "Classical baselines"
---

This chapter introduces *amortisation-free* inference baselines for the SEIR running example.
These serve as anchors for later SBI/VI methods.

## Goals (M2)

- Provide a **Metropolis–Hastings (MH)** baseline on the (possibly) non-differentiable simulator / likelihood.
- Provide a **gradient-based HMC/NUTS** baseline on the differentiable ODE likelihood.
- Include a minimal set of **diagnostics** that we will reuse throughout the book:
  - Trace / mixing diagnostics (R-hat, ESS where available)
  - Posterior predictive checks (PPC)
  - Basic calibration checks (e.g. coverage on simulated datasets)

## Plan

### 1) Common interface

- Define a parameter vector and priors consistent with the running example.
- Define a log-likelihood `logp(y | theta)` (or unbiased estimate) plus `logp(theta)`.
- Agree on what a “draw” contains (e.g. `theta`, latent trajectories optional, log joint).

## Minimal MH demo: sampling a 1D Gaussian

Before applying MH to the SEIR model, it helps to sanity-check the implementation on a distribution where we know the answer.
Below we sample from a standard normal target density using a Gaussian random-walk proposal.

```{python}
import numpy as np

from diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings


def log_prob_standard_normal(x: np.ndarray) -> float:
    # Up to an additive constant: log N(x; 0, 1)
    return -0.5 * float(np.sum(x**2))


rng = np.random.default_rng(0)
res = random_walk_metropolis_hastings(
    log_prob_standard_normal,
    x0=np.array([5.0]),
    proposal_std=1.0,
    n_steps=5_000,
    rng=rng,
)

accept_rate = res.accept_rate
mean = float(np.mean(res.chain))
std = float(np.std(res.chain))

accept_rate, mean, std
```

The acceptance rate depends strongly on the proposal scale; in 1D, a value around 20–60% is typical for a reasonable random-walk step size.

## Minimal MH demo: infer only `beta` in the SEIR running example

As a first epidemiological baseline, we infer **only the transmission rate** `beta`, keeping the remaining parameters fixed.
This is intentionally a *small, well-posed* problem: a 1D posterior that still exercises the full simulation + observation likelihood.

We will:

- simulate a synthetic observed time series `y` from a known `beta_true`
- define a log posterior in terms of `log_beta = log(beta)` (so proposals live on ℝ)
- run random-walk MH and inspect the resulting posterior for `beta`

```{python}
import numpy as np
import matplotlib.pyplot as plt

from diff_epi_inference import (
    SEIRParams,
    discrete_gamma_delay_pmf,
    expected_reported_cases_delayed,
    incidence_from_susceptibles,
    nbinom_loglik,
    sample_nbinom_reports,
    simulate_seir_euler,
)
from diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings

# --- Fixed settings (match the running example defaults) ---

sigma_fixed = 1 / 5
gamma_fixed = 1 / 7

s0, e0, i0, r0 = 999.0, 0.0, 1.0, 0.0

dt = 0.2
steps = 200

reporting_rate = 0.3
w = discrete_gamma_delay_pmf(shape=2.0, scale=1.0, max_delay=20)
dispersion = 20.0

# --- Synthetic data ---

beta_true = 0.6
params_true = SEIRParams(beta=beta_true, sigma=sigma_fixed, gamma=gamma_fixed)

out_true = simulate_seir_euler(
    params=params_true,
    s0=s0,
    e0=e0,
    i0=i0,
    r0=r0,
    dt=dt,
    steps=steps,
)

inc_true = incidence_from_susceptibles(out_true["S"])
mu_true = expected_reported_cases_delayed(
    incidence=inc_true,
    reporting_rate=reporting_rate,
    delay_pmf=w,
)

rng = np.random.default_rng(0)
y_obs = sample_nbinom_reports(expected=mu_true, dispersion=dispersion, rng=rng)

# --- Log posterior for log(beta) ---

logbeta_prior_mean = float(np.log(0.5))
logbeta_prior_sd = 0.5


def log_post_logbeta(x: np.ndarray) -> float:
    logbeta = float(x[0])
    beta = float(np.exp(logbeta))

    # Log prior: log(beta) ~ Normal(mean, sd)  (constants dropped)
    lp = -0.5 * ((logbeta - logbeta_prior_mean) / logbeta_prior_sd) ** 2

    params = SEIRParams(beta=beta, sigma=sigma_fixed, gamma=gamma_fixed)
    out = simulate_seir_euler(
        params=params,
        s0=s0,
        e0=e0,
        i0=i0,
        r0=r0,
        dt=dt,
        steps=steps,
    )

    inc = incidence_from_susceptibles(out["S"])
    mu = expected_reported_cases_delayed(
        incidence=inc,
        reporting_rate=reporting_rate,
        delay_pmf=w,
    )

    ll = nbinom_loglik(y=y_obs, mu=mu, dispersion=dispersion)
    return float(lp + ll)


res = random_walk_metropolis_hastings(
    log_post_logbeta,
    x0=np.array([np.log(0.3)]),
    proposal_std=0.10,
    n_steps=3_000,
    rng=np.random.default_rng(1),
)

beta_chain = np.exp(res.chain[:, 0])
beta_chain = beta_chain[500:]  # crude burn-in for this demo

accept_rate = res.accept_rate
beta_mean = float(np.mean(beta_chain))
beta_q05, beta_q95 = [float(q) for q in np.quantile(beta_chain, [0.05, 0.95])]

accept_rate, beta_mean, beta_q05, beta_q95
```

A quick visual check:

```{python}
fig, ax = plt.subplots(figsize=(7, 3))
ax.plot(beta_chain, lw=0.7)
ax.axhline(beta_true, color="black", ls="--", label="beta_true")
ax.set_xlabel("MH iteration")
ax.set_ylabel("beta")
ax.set_title("MH trace for beta (others fixed)")
ax.legend()
plt.show()
```

### Posterior predictive check (MH)

A minimal posterior predictive check samples simulated observations from the posterior draws,
then overlays predictive bands against the observed time series.

```{python}
def ppc_from_beta_draws(beta_draws: np.ndarray, rng: np.random.Generator, n_draws: int = 50):
    beta_draws = np.asarray(beta_draws)
    if beta_draws.ndim != 1:
        raise ValueError("beta_draws must be 1D")

    n_draws = int(min(n_draws, beta_draws.shape[0]))
    idx = rng.choice(beta_draws.shape[0], size=n_draws, replace=False)

    y_rep = []
    for beta in beta_draws[idx]:
        params = SEIRParams(beta=float(beta), sigma=sigma_fixed, gamma=gamma_fixed)
        out = simulate_seir_euler(
            params=params,
            s0=s0,
            e0=e0,
            i0=i0,
            r0=r0,
            dt=dt,
            steps=steps,
        )
        inc = incidence_from_susceptibles(out["S"])
        mu = expected_reported_cases_delayed(
            incidence=inc,
            reporting_rate=reporting_rate,
            delay_pmf=w,
        )
        y_rep.append(sample_nbinom_reports(expected=mu, dispersion=dispersion, rng=rng))

    y_rep = np.asarray(y_rep)
    q05, q50, q95 = np.quantile(y_rep, [0.05, 0.5, 0.95], axis=0)
    return q05, q50, q95


q05_mh, q50_mh, q95_mh = ppc_from_beta_draws(
    beta_chain,
    rng=np.random.default_rng(123),
    n_draws=60,
)

t = np.arange(len(y_obs)) * dt

fig, ax = plt.subplots(figsize=(7, 3))
ax.fill_between(t, q05_mh, q95_mh, alpha=0.25, label="posterior predictive 90%")
ax.plot(t, q50_mh, lw=1.2, label="posterior predictive median")
ax.plot(t, y_obs, lw=1.0, color="black", label="observed y")
ax.set_xlabel("time")
ax.set_ylabel("reported cases")
ax.set_title("PPC (MH; beta only)")
ax.legend(loc="upper right")
plt.show()
```

### 2) Random-walk Metropolis–Hastings

- Algorithm: Gaussian random-walk proposals in unconstrained space.
- Implementation notes:
  - Transform constrained parameters (positivity, simplex) to ℝ.
  - Adapt proposal scale during warmup only; then fix.
- Outputs:
  - Acceptance rate
  - Autocorrelation / effective sample size

## Minimal HMC demo: infer only `beta` (finite-difference gradients)

For a first gradient-based baseline, we can run **HMC** on the same 1D posterior as above.
In a production setting, you would normally use **autodiff + NUTS** (e.g. NumPyro/BlackJAX)
so that gradients are exact (up to solver tolerance) and step sizes / trajectory lengths are
adapted automatically.

Here we use a deliberately minimal HMC implementation that estimates gradients by **central
finite differences**.

Limitations of this baseline (important):

- **Computational cost:** finite-difference gradients require *2 evaluations per parameter*
  *per leapfrog step*. Even in 1D this adds up; in higher dimensions it quickly becomes
  impractical.
- **Numerical sensitivity:** the finite-difference step size (`grad_eps`) must be tuned; too
  small → cancellation / noise, too large → biased gradients.
- **No adaptation / diagnostics:** this minimal HMC does not adapt step sizes or trajectory
  lengths and does not report NUTS-style diagnostics (divergences, tree depth, energy).

Despite these caveats, it is a useful pedagogical bridge between MH and a full autodiff + NUTS
workflow.

```{python}
from diff_epi_inference.mcmc.hmc import hamiltonian_monte_carlo

res_hmc = hamiltonian_monte_carlo(
    log_post_logbeta,
    x0=np.array([np.log(0.3)]),
    n_steps=2_000,
    step_size=0.02,
    n_leapfrog=25,
    grad_eps=1e-4,
    rng=np.random.default_rng(2),
)

beta_chain_hmc = np.exp(res_hmc.chain[:, 0])
beta_chain_hmc = beta_chain_hmc[500:]

accept_rate_hmc = res_hmc.accept_rate
beta_mean_hmc = float(np.mean(beta_chain_hmc))
beta_q05_hmc, beta_q95_hmc = [float(q) for q in np.quantile(beta_chain_hmc, [0.05, 0.95])]

accept_rate_hmc, beta_mean_hmc, beta_q05_hmc, beta_q95_hmc
```

A quick trace plot:

```{python}
fig, ax = plt.subplots(figsize=(7, 3))
ax.plot(beta_chain_hmc, lw=0.7)
ax.axhline(beta_true, color="black", ls="--", label="beta_true")
ax.set_xlabel("HMC iteration")
ax.set_ylabel("beta")
ax.set_title("HMC trace for beta (finite-difference gradients; others fixed)")
ax.legend()
plt.show()
```

### Posterior predictive check (HMC)

Using the same PPC helper as above:

```{python}
q05_hmc, q50_hmc, q95_hmc = ppc_from_beta_draws(
    beta_chain_hmc,
    rng=np.random.default_rng(456),
    n_draws=60,
)

t = np.arange(len(y_obs)) * dt

fig, ax = plt.subplots(figsize=(7, 3))
ax.fill_between(t, q05_hmc, q95_hmc, alpha=0.25, label="posterior predictive 90%")
ax.plot(t, q50_hmc, lw=1.2, label="posterior predictive median")
ax.plot(t, y_obs, lw=1.0, color="black", label="observed y")
ax.set_xlabel("time")
ax.set_ylabel("reported cases")
ax.set_title("PPC (HMC; beta only)")
ax.legend(loc="upper right")
plt.show()
```

### 3) HMC / NUTS on the ODE model (recommended path)

- Use **autodiff through the ODE solver + observation model**.
- Start with a small, well-posed parameterisation (avoid pathological geometry).
- Outputs:
  - Step size / tree depth (NUTS)
  - Divergences / energy diagnostics

#### Optional: autodiff + NUTS via BlackJAX

This repository keeps JAX/BlackJAX as an **optional dependency**. If installed (see
`pyproject.toml` extra `nuts`), you can run a full NUTS baseline with window adaptation.

Below is a minimal sanity check on a 1D standard normal target. When the optional
dependencies are missing, the cell prints a message and continues.

```{python}
try:
    import numpy as np
    import jax.numpy as jnp

    from diff_epi_inference.mcmc.nuts_blackjax import run_blackjax_nuts

    def log_prob_standard_normal(x):
        return -0.5 * jnp.sum(x**2)

    res_nuts = run_blackjax_nuts(
        log_prob_standard_normal,
        x0=np.array([5.0]),
        num_warmup=200,
        num_samples=500,
        seed=0,
    )

    float(res_nuts.accept_rate), float(np.mean(res_nuts.chain[:, 0])), float(np.std(res_nuts.chain[:, 0]))
except ImportError as e:
    print("Skipping BlackJAX NUTS demo (optional deps not installed):", e)
```

#### Optional: NUTS for the SEIR `beta`-only posterior (JAX reimplementation)

To use NUTS on the SEIR running example, the log posterior must be **JAX-traceable**.
The core simulator in this repo is NumPy-based, so below we re-implement the *minimal*
`beta`-only likelihood in `jax.numpy` (Euler step + delay convolution + NB likelihood).

This is intentionally scoped to the `beta`-only demo so we can keep the optional path
self-contained.

**Note:** there is also an optional *calibration/coverage smoke test* for this NUTS path in
`tests/test_blackjax_nuts_seir_calibration_optional.py`. It is skipped unless the
`nuts` extra is installed (and is meant to catch obvious regressions in the JAX
reimplementation).

```{python}
try:
    import numpy as np

    from diff_epi_inference.mcmc.nuts_blackjax import run_blackjax_nuts

    from diff_epi_inference.models.seir_jax_beta_only import make_log_post_logbeta_jax

    log_post_logbeta_jax = make_log_post_logbeta_jax(
        y_obs=np.asarray(y_obs),
        w_delay_pmf=np.asarray(w, dtype=float),
        sigma=float(sigma_fixed),
        gamma=float(gamma_fixed),
        s0=float(s0),
        e0=float(e0),
        i0=float(i0),
        r0=float(r0),
        dt=float(dt),
        steps=int(steps),
        reporting_rate=float(reporting_rate),
        dispersion=float(dispersion),
        logbeta_prior_mean=float(logbeta_prior_mean),
        logbeta_prior_sd=float(logbeta_prior_sd),
    )

    res_nuts_seir = run_blackjax_nuts(
        log_post_logbeta_jax,
        x0=np.array([np.log(0.3)]),
        num_warmup=400,
        num_samples=800,
        seed=3,
    )

    beta_chain_nuts = np.exp(res_nuts_seir.chain[:, 0])
    accept_rate_nuts = float(res_nuts_seir.accept_rate)
    beta_mean_nuts = float(np.mean(beta_chain_nuts))
    beta_q05_nuts, beta_q95_nuts = [float(q) for q in np.quantile(beta_chain_nuts, [0.05, 0.95])]

    accept_rate_nuts, beta_mean_nuts, beta_q05_nuts, beta_q95_nuts
except ImportError as e:
    print("Skipping BlackJAX NUTS SEIR demo (optional deps not installed):", e)
```

A quick posterior predictive check (if NUTS ran):

```{python}
try:
    q05_nuts, q50_nuts, q95_nuts = ppc_from_beta_draws(
        beta_chain_nuts,
        rng=np.random.default_rng(789),
        n_draws=60,
    )

    t = np.arange(len(y_obs)) * dt

    fig, ax = plt.subplots(figsize=(7, 3))
    ax.fill_between(t, q05_nuts, q95_nuts, alpha=0.25, label="posterior predictive 90%")
    ax.plot(t, q50_nuts, lw=1.2, label="posterior predictive median")
    ax.plot(t, y_obs, lw=1.0, color="black", label="observed y")
    ax.set_xlabel("time")
    ax.set_ylabel("reported cases")
    ax.set_title("PPC (NUTS via BlackJAX; beta only)")
    ax.legend(loc="upper right")
    plt.show()
except NameError:
    print("Skipping NUTS PPC (NUTS SEIR demo did not run)")
```

### 4) Diagnostics

- Trace plots for key parameters.
- Posterior predictive: simulate trajectories and compare to observed data.

#### Calibration smoke test (MH; `beta` only)

A minimal calibration check is to repeat the synthetic-data experiment many times and ask:

> If we generate data at `beta_true`, does our nominal **90% posterior interval** contain `beta_true`
> about **90% of the time**?

This is not full simulation-based calibration (SBC), but it is a fast “is anything horribly wrong?”
smoke test.

```{python}
from dataclasses import dataclass


@dataclass
class CoverageResult:
    beta_true: float
    contained: bool
    q05: float
    q95: float
    accept_rate: float


def make_log_post_logbeta_for_y(y_obs_local: np.ndarray):
    def _log_post_logbeta(x: np.ndarray) -> float:
        logbeta = float(x[0])
        beta = float(np.exp(logbeta))

        lp = -0.5 * ((logbeta - logbeta_prior_mean) / logbeta_prior_sd) ** 2

        params = SEIRParams(beta=beta, sigma=sigma_fixed, gamma=gamma_fixed)
        out = simulate_seir_euler(
            params=params,
            s0=s0,
            e0=e0,
            i0=i0,
            r0=r0,
            dt=dt,
            steps=steps,
        )

        inc = incidence_from_susceptibles(out["S"])
        mu = expected_reported_cases_delayed(
            incidence=inc,
            reporting_rate=reporting_rate,
            delay_pmf=w,
        )

        ll = nbinom_loglik(y=y_obs_local, mu=mu, dispersion=dispersion)
        return float(lp + ll)

    return _log_post_logbeta


rng = np.random.default_rng(2026)

beta_grid = np.array([0.30, 0.40, 0.50, 0.60, 0.70])
reps_per_beta = 4  # 5 * 4 = 20 datasets

results: list[CoverageResult] = []

for beta_true_i in beta_grid:
    for _ in range(reps_per_beta):
        params_true_i = SEIRParams(beta=float(beta_true_i), sigma=sigma_fixed, gamma=gamma_fixed)
        out_true_i = simulate_seir_euler(
            params=params_true_i,
            s0=s0,
            e0=e0,
            i0=i0,
            r0=r0,
            dt=dt,
            steps=steps,
        )

        inc_true_i = incidence_from_susceptibles(out_true_i["S"])
        mu_true_i = expected_reported_cases_delayed(
            incidence=inc_true_i,
            reporting_rate=reporting_rate,
            delay_pmf=w,
        )

        y_obs_i = sample_nbinom_reports(expected=mu_true_i, dispersion=dispersion, rng=rng)
        log_post_i = make_log_post_logbeta_for_y(y_obs_i)

        # Short chain: this is a smoke test, not a production run.
        res_i = random_walk_metropolis_hastings(
            log_post_i,
            x0=np.array([np.log(0.5)]),
            proposal_std=0.10,
            n_steps=800,
            rng=rng,
        )

        beta_chain_i = np.exp(res_i.chain[:, 0])[200:]
        q05_i, q95_i = [float(q) for q in np.quantile(beta_chain_i, [0.05, 0.95])]
        contained_i = (q05_i <= float(beta_true_i) <= q95_i)

        results.append(
            CoverageResult(
                beta_true=float(beta_true_i),
                contained=bool(contained_i),
                q05=q05_i,
                q95=q95_i,
                accept_rate=float(res_i.accept_rate),
            )
        )

coverage = float(np.mean([r.contained for r in results]))
coverage
```

We can also break down coverage by `beta_true`:

```{python}
from collections import defaultdict

by_beta = defaultdict(list)
for r in results:
    by_beta[r.beta_true].append(r.contained)

coverage_by_beta = {beta: float(np.mean(v)) for beta, v in sorted(by_beta.items())}
coverage_by_beta
```

#### Calibration smoke test (HMC; `beta` only)

We can repeat the same coverage experiment for the minimal HMC baseline.
Because HMC here uses finite-difference gradients, we keep the chains *very short*.

```{python}
from diff_epi_inference.mcmc.hmc import hamiltonian_monte_carlo


results_hmc: list[CoverageResult] = []

for beta_true_i in beta_grid:
    for _ in range(reps_per_beta):
        params_true_i = SEIRParams(beta=float(beta_true_i), sigma=sigma_fixed, gamma=gamma_fixed)
        out_true_i = simulate_seir_euler(
            params=params_true_i,
            s0=s0,
            e0=e0,
            i0=i0,
            r0=r0,
            dt=dt,
            steps=steps,
        )

        inc_true_i = incidence_from_susceptibles(out_true_i["S"])
        mu_true_i = expected_reported_cases_delayed(
            incidence=inc_true_i,
            reporting_rate=reporting_rate,
            delay_pmf=w,
        )

        y_obs_i = sample_nbinom_reports(expected=mu_true_i, dispersion=dispersion, rng=rng)
        log_post_i = make_log_post_logbeta_for_y(y_obs_i)

        res_i = hamiltonian_monte_carlo(
            log_post_i,
            x0=np.array([np.log(0.5)]),
            n_steps=400,
            step_size=0.02,
            n_leapfrog=15,
            grad_eps=1e-4,
            rng=rng,
        )

        beta_chain_i = np.exp(res_i.chain[:, 0])[100:]
        q05_i, q95_i = [float(q) for q in np.quantile(beta_chain_i, [0.05, 0.95])]
        contained_i = (q05_i <= float(beta_true_i) <= q95_i)

        results_hmc.append(
            CoverageResult(
                beta_true=float(beta_true_i),
                contained=bool(contained_i),
                q05=q05_i,
                q95=q95_i,
                accept_rate=float(res_i.accept_rate),
            )
        )

coverage_hmc = float(np.mean([r.contained for r in results_hmc]))
coverage_hmc
```

Break down coverage by `beta_true`:

```{python}
by_beta_hmc = defaultdict(list)
for r in results_hmc:
    by_beta_hmc[r.beta_true].append(r.contained)

coverage_by_beta_hmc = {beta: float(np.mean(v)) for beta, v in sorted(by_beta_hmc.items())}
coverage_by_beta_hmc
```

## Next coding tasks

- Add `src/diff_epi_inference/mcmc/mh.py` with a minimal MH sampler + tests.
- Add `src/diff_epi_inference/mcmc/nuts.py` (or wrapper around an existing library) + tests.
- Add plotting helpers for chains + PPC.
