---
title: "Classical baselines"
---

This chapter introduces *amortisation-free* inference baselines for the SEIR running example.
These serve as anchors for later SBI/VI methods.

## Goals (M2)

- Provide a **Metropolis–Hastings (MH)** baseline on the (possibly) non-differentiable simulator / likelihood.
- Provide a **gradient-based HMC/NUTS** baseline on the differentiable ODE likelihood.
- Include a minimal set of **diagnostics** that we will reuse throughout the book:
  - Trace / mixing diagnostics (R-hat, ESS where available)
  - Posterior predictive checks (PPC)
  - Basic calibration checks (e.g. coverage on simulated datasets)

## Plan

### 1) Common interface

- Define a parameter vector and priors consistent with the running example.
- Define a log-likelihood `logp(y | theta)` (or unbiased estimate) plus `logp(theta)`.
- Agree on what a “draw” contains (e.g. `theta`, latent trajectories optional, log joint).

## Minimal MH demo: sampling a 1D Gaussian

Before applying MH to the SEIR model, it helps to sanity-check the implementation on a distribution where we know the answer.
Below we sample from a standard normal target density using a Gaussian random-walk proposal.

```{python}
import numpy as np

from diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings


def log_prob_standard_normal(x: np.ndarray) -> float:
    # Up to an additive constant: log N(x; 0, 1)
    return -0.5 * float(np.sum(x**2))


rng = np.random.default_rng(0)
res = random_walk_metropolis_hastings(
    log_prob_standard_normal,
    x0=np.array([5.0]),
    proposal_std=1.0,
    n_steps=5_000,
    rng=rng,
)

accept_rate = res.accept_rate
mean = float(np.mean(res.chain))
std = float(np.std(res.chain))

accept_rate, mean, std
```

The acceptance rate depends strongly on the proposal scale; in 1D, a value around 20–60% is typical for a reasonable random-walk step size.

## Minimal MH demo: infer only `beta` in the SEIR running example

As a first epidemiological baseline, we infer **only the transmission rate** `beta`, keeping the remaining parameters fixed.
This is intentionally a *small, well-posed* problem: a 1D posterior that still exercises the full simulation + observation likelihood.

We will:

- simulate a synthetic observed time series `y` from a known `beta_true`
- define a log posterior in terms of `log_beta = log(beta)` (so proposals live on ℝ)
- run random-walk MH and inspect the resulting posterior for `beta`

```{python}
import numpy as np
import matplotlib.pyplot as plt

from diff_epi_inference import (
    SEIRParams,
    discrete_gamma_delay_pmf,
    expected_reported_cases_delayed,
    incidence_from_susceptibles,
    nbinom_loglik,
    sample_nbinom_reports,
    simulate_seir_euler,
)
from diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings

# --- Fixed settings (match the running example defaults) ---

sigma_fixed = 1 / 5
gamma_fixed = 1 / 7

s0, e0, i0, r0 = 999.0, 0.0, 1.0, 0.0

dt = 0.2
steps = 200

reporting_rate = 0.3
w = discrete_gamma_delay_pmf(shape=2.0, scale=1.0, max_delay=20)
dispersion = 20.0

# --- Synthetic data ---

beta_true = 0.6
params_true = SEIRParams(beta=beta_true, sigma=sigma_fixed, gamma=gamma_fixed)

out_true = simulate_seir_euler(
    params=params_true,
    s0=s0,
    e0=e0,
    i0=i0,
    r0=r0,
    dt=dt,
    steps=steps,
)

inc_true = incidence_from_susceptibles(out_true["S"])
mu_true = expected_reported_cases_delayed(
    incidence=inc_true,
    reporting_rate=reporting_rate,
    delay_pmf=w,
)

rng = np.random.default_rng(0)
y_obs = sample_nbinom_reports(expected=mu_true, dispersion=dispersion, rng=rng)

# --- Log posterior for log(beta) ---

logbeta_prior_mean = float(np.log(0.5))
logbeta_prior_sd = 0.5


def log_post_logbeta(x: np.ndarray) -> float:
    logbeta = float(x[0])
    beta = float(np.exp(logbeta))

    # Log prior: log(beta) ~ Normal(mean, sd)  (constants dropped)
    lp = -0.5 * ((logbeta - logbeta_prior_mean) / logbeta_prior_sd) ** 2

    params = SEIRParams(beta=beta, sigma=sigma_fixed, gamma=gamma_fixed)
    out = simulate_seir_euler(
        params=params,
        s0=s0,
        e0=e0,
        i0=i0,
        r0=r0,
        dt=dt,
        steps=steps,
    )

    inc = incidence_from_susceptibles(out["S"])
    mu = expected_reported_cases_delayed(
        incidence=inc,
        reporting_rate=reporting_rate,
        delay_pmf=w,
    )

    ll = nbinom_loglik(y=y_obs, mu=mu, dispersion=dispersion)
    return float(lp + ll)


res = random_walk_metropolis_hastings(
    log_post_logbeta,
    x0=np.array([np.log(0.3)]),
    proposal_std=0.10,
    n_steps=3_000,
    rng=np.random.default_rng(1),
)

beta_chain = np.exp(res.chain[:, 0])
beta_chain = beta_chain[500:]  # crude burn-in for this demo

accept_rate = res.accept_rate
beta_mean = float(np.mean(beta_chain))
beta_q05, beta_q95 = [float(q) for q in np.quantile(beta_chain, [0.05, 0.95])]

accept_rate, beta_mean, beta_q05, beta_q95
```

A quick visual check:

```{python}
fig, ax = plt.subplots(figsize=(7, 3))
ax.plot(beta_chain, lw=0.7)
ax.axhline(beta_true, color="black", ls="--", label="beta_true")
ax.set_xlabel("MH iteration")
ax.set_ylabel("beta")
ax.set_title("MH trace for beta (others fixed)")
ax.legend()
plt.show()
```

### 2) Random-walk Metropolis–Hastings

- Algorithm: Gaussian random-walk proposals in unconstrained space.
- Implementation notes:
  - Transform constrained parameters (positivity, simplex) to ℝ.
  - Adapt proposal scale during warmup only; then fix.
- Outputs:
  - Acceptance rate
  - Autocorrelation / effective sample size

## Minimal HMC demo: infer only `beta` (finite-difference gradients)

For a first gradient-based baseline, we can run **HMC** on the same 1D posterior as above.
In a production setting, you would normally use **autodiff + NUTS** (e.g. NumPyro/BlackJAX)
so that gradients are exact (up to solver tolerance) and step sizes / trajectory lengths are
adapted automatically.

Here we use a deliberately minimal HMC implementation that estimates gradients by **central
finite differences**.

Limitations of this baseline (important):

- **Computational cost:** finite-difference gradients require *2 evaluations per parameter*
  *per leapfrog step*. Even in 1D this adds up; in higher dimensions it quickly becomes
  impractical.
- **Numerical sensitivity:** the finite-difference step size (`grad_eps`) must be tuned; too
  small → cancellation / noise, too large → biased gradients.
- **No adaptation / diagnostics:** this minimal HMC does not adapt step sizes or trajectory
  lengths and does not report NUTS-style diagnostics (divergences, tree depth, energy).

Despite these caveats, it is a useful pedagogical bridge between MH and a full autodiff + NUTS
workflow.

```{python}
from diff_epi_inference.mcmc.hmc import hamiltonian_monte_carlo

res_hmc = hamiltonian_monte_carlo(
    log_post_logbeta,
    x0=np.array([np.log(0.3)]),
    n_steps=2_000,
    step_size=0.02,
    n_leapfrog=25,
    grad_eps=1e-4,
    rng=np.random.default_rng(2),
)

beta_chain_hmc = np.exp(res_hmc.chain[:, 0])
beta_chain_hmc = beta_chain_hmc[500:]

accept_rate_hmc = res_hmc.accept_rate
beta_mean_hmc = float(np.mean(beta_chain_hmc))
beta_q05_hmc, beta_q95_hmc = [float(q) for q in np.quantile(beta_chain_hmc, [0.05, 0.95])]

accept_rate_hmc, beta_mean_hmc, beta_q05_hmc, beta_q95_hmc
```

A quick trace plot:

```{python}
fig, ax = plt.subplots(figsize=(7, 3))
ax.plot(beta_chain_hmc, lw=0.7)
ax.axhline(beta_true, color="black", ls="--", label="beta_true")
ax.set_xlabel("HMC iteration")
ax.set_ylabel("beta")
ax.set_title("HMC trace for beta (finite-difference gradients; others fixed)")
ax.legend()
plt.show()
```

### 3) HMC / NUTS on the ODE model (recommended path)

- Use **autodiff through the ODE solver + observation model**.
- Start with a small, well-posed parameterisation (avoid pathological geometry).
- Outputs:
  - Step size / tree depth (NUTS)
  - Divergences / energy diagnostics

### 4) Diagnostics

- Trace plots for key parameters.
- Posterior predictive: simulate trajectories and compare to observed data.
- Calibration smoke test:
  - Generate a small set of synthetic datasets from known parameters.
  - Run inference and check nominal coverage for a few parameters.

## Next coding tasks

- Add `src/diff_epi_inference/mcmc/mh.py` with a minimal MH sampler + tests.
- Add `src/diff_epi_inference/mcmc/nuts.py` (or wrapper around an existing library) + tests.
- Add plotting helpers for chains + PPC.
