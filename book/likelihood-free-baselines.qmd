---
title: "Likelihood-free baselines"
---

This chapter introduces likelihood-free baselines for the running example.
These methods are useful when the simulator is non-differentiable and/or when a tractable likelihood is unavailable.

## Goals (M3)

- Implement and demonstrate **ABC rejection** as a minimal, robust baseline.
- (Optional) extend to **SMC-ABC** for better efficiency.
- Introduce **synthetic likelihood** / likelihood-on-summaries as a bridge between ABC and modern SBI.

## Outline

### 1) Setup: summaries and distances

- Choose summary statistics $s(y)$ for the observed time series (e.g. peak size/time, total cases, early growth rate).
- Define a distance $d(s(y), s(y_{\mathrm{sim}}))$.
- Discuss the trade-off between informativeness and dimensionality.

### 2) ABC rejection

- Sample $\theta \sim p(\theta)$.
- Simulate $y_{\mathrm{sim}} \sim p(\cdot \mid \theta)$.
- Accept if $d(s(y), s(y_{\mathrm{sim}})) \leq \varepsilon$.

Deliverables:

- A reusable `abc_rejection(...)` helper in `diff_epi_inference.abc`.
- A small demo on the running example (start with `beta`-only, then expand).

#### Demo: `beta`-only ABC on the stochastic simulator

Below we infer only $\beta$ (holding the other SEIR parameters fixed) using the **stochastic** simulator and a very small set of summary statistics.
This is intentionally minimal: the goal is to show the ABC control flow and get a rough posterior-like set of accepted $\beta$ values.

```{python}
#| echo: true
#| warning: false

import numpy as np

from diff_epi_inference import SEIRParams
from diff_epi_inference.abc import abc_rejection
from diff_epi_inference.pipeline import simulate_seir_and_report_stochastic

rng = np.random.default_rng(0)

# --- "Observed" data (synthetic) ---
beta_true = 0.35
params_true = SEIRParams(beta=beta_true, sigma=1 / 4.0, gamma=1 / 6.0)

steps = 80

ds_obs = simulate_seir_and_report_stochastic(
    params=params_true,
    s0=10_000,
    e0=3,
    i0=2,
    r0=0,
    dt=1.0,
    steps=steps,
    reporting_rate=0.25,
    rng=rng,
)

y_obs = ds_obs.y.astype(float)


# --- Prior and simulator ---
# We sample log(beta) ~ Normal, then exponentiate.
logbeta_prior_mean = float(np.log(0.3))
logbeta_prior_sd = 0.35


def prior_sample(rng: np.random.Generator) -> np.ndarray:
    logbeta = rng.normal(loc=logbeta_prior_mean, scale=logbeta_prior_sd)
    return np.array([logbeta], dtype=float)


def simulate(theta: np.ndarray, rng: np.random.Generator) -> np.ndarray:
    (logbeta,) = np.asarray(theta, dtype=float)
    beta = float(np.exp(logbeta))

    params = SEIRParams(beta=beta, sigma=params_true.sigma, gamma=params_true.gamma)
    ds = simulate_seir_and_report_stochastic(
        params=params,
        s0=10_000,
        e0=3,
        i0=2,
        r0=0,
        dt=1.0,
        steps=steps,
        reporting_rate=0.25,
        rng=rng,
    )
    return ds.y.astype(float)


# --- Summaries and distance ---
# Keep summaries low-dimensional: (total cases, peak size, peak time).

def summary(y: np.ndarray) -> np.ndarray:
    y = np.asarray(y, dtype=float)
    peak_t = int(np.argmax(y))
    return np.array([np.sum(y), np.max(y), peak_t], dtype=float)


def distance(s_sim: np.ndarray, s_obs: np.ndarray) -> float:
    s_sim = np.asarray(s_sim, dtype=float)
    s_obs = np.asarray(s_obs, dtype=float)
    # Rough scaling so "time" doesn't dominate.
    scale = np.array([1000.0, 50.0, 5.0])
    return float(np.linalg.norm((s_sim - s_obs) / scale))


res = abc_rejection(
    prior_sample=prior_sample,
    simulate=simulate,
    distance=distance,
    y_obs=y_obs,
    summary=summary,
    epsilon=1.5,
    n_accept=200,
    max_trials=50_000,
    rng=rng,
)

beta_accept = np.exp(res.thetas[:, 0])

print(f"ABC trials: {res.n_trials}  (accept rate ~ {len(beta_accept)/res.n_trials:.3f})")
print(f"beta_true: {beta_true:.3f}")
print(
    "accepted beta: "
    f"mean={np.mean(beta_accept):.3f}, sd={np.std(beta_accept):.3f}, "
    f"q10={np.quantile(beta_accept, 0.1):.3f}, q90={np.quantile(beta_accept, 0.9):.3f}"
)
```

Notes:

- The choice of summaries and scaling in the distance is ad hoc; later sections will discuss more principled summary selection.
- As written, ABC rejection can be inefficient: if the tolerance `epsilon` is too small, you may need a very large `max_trials`.

### 3) (Optional) SMC-ABC

- Sequence of tolerances $\varepsilon_1 > \varepsilon_2 > \cdots$.
- Reweight/resample/perturb particles.
- Monitor acceptance rates and particle degeneracy.

### 4) Synthetic likelihood (on summaries)

- Assume $s(y) \mid \theta \approx \mathcal{N}(\mu_\theta, \Sigma_\theta)$.
- Estimate $(\mu_\theta, \Sigma_\theta)$ via repeated simulations at fixed $\theta$.
- Use the resulting approximate likelihood inside MH/HMC.

#### Demo: `beta`-only synthetic likelihood on the same summaries

This is a tiny “likelihood-on-summaries” baseline:

1. For a proposed $\theta$ (here $\theta = \log \beta$), run the stochastic simulator multiple times.
2. Compute the same summary vector $s(y)$ used in the ABC demo.
3. Fit a Gaussian $\mathcal{N}(\mu_\theta, \Sigma_\theta)$ to the simulated summaries.
4. Use the resulting *synthetic likelihood* $p(s(y_{\mathrm{obs}}) \mid \theta)$ inside random-walk MH.

```{python}
#| echo: true
#| warning: false

import numpy as np

from diff_epi_inference import SEIRParams
from diff_epi_inference.mcmc.mh import random_walk_metropolis_hastings
from diff_epi_inference.pipeline import simulate_seir_and_report_stochastic
from diff_epi_inference.synthetic_likelihood import estimate_summary_gaussian, mvn_logpdf

# Reuse the observed data / params from the ABC block above.
# (Quarto executes code blocks in order, so y_obs and params_true exist here.)

s_obs = summary(y_obs)


def simulate_summary_once(logbeta: float, *, rng: np.random.Generator) -> np.ndarray:
    beta = float(np.exp(logbeta))
    params = SEIRParams(beta=beta, sigma=params_true.sigma, gamma=params_true.gamma)
    ds = simulate_seir_and_report_stochastic(
        params=params,
        s0=10_000,
        e0=3,
        i0=2,
        r0=0,
        dt=1.0,
        steps=steps,
        reporting_rate=0.25,
        rng=rng,
    )
    return summary(ds.y.astype(float))


def estimate_mu_cov(
    logbeta: float,
    *,
    rng: np.random.Generator,
    n_sims: int = 12,
    cov_jitter: float = 1e-6,
) -> tuple[np.ndarray, np.ndarray]:
    """Estimate (mu, cov) of summaries s(y) at fixed logbeta via repeated sims."""
    return estimate_summary_gaussian(
        lambda r: simulate_summary_once(logbeta, rng=r),
        n_sims=n_sims,
        rng=rng,
        cov_jitter=cov_jitter,
    )


rng = np.random.default_rng(1)


def log_prior(theta: np.ndarray) -> float:
    (logbeta,) = np.asarray(theta, dtype=float)
    z = (logbeta - logbeta_prior_mean) / logbeta_prior_sd
    return float(-0.5 * z**2)  # (drop constants)


def log_synth_lik(theta: np.ndarray, rng: np.random.Generator) -> float:
    (logbeta,) = np.asarray(theta, dtype=float)
    mu, cov = estimate_mu_cov(float(logbeta), rng=rng)
    return mvn_logpdf(s_obs, mu, cov)


def log_posterior(theta: np.ndarray, rng: np.random.Generator) -> float:
    lp = log_prior(theta)
    if not np.isfinite(lp):
        return -np.inf
    ll = log_synth_lik(theta, rng)
    return lp + ll


# A very small chain for demonstration (synthetic likelihood is expensive per step).
chain = random_walk_metropolis_hastings(
    log_prob_fn=lambda theta: log_posterior(theta, rng),
    x0=np.array([logbeta_prior_mean], dtype=float),
    n_steps=500,
    proposal_std=np.array([0.15], dtype=float),
    rng=rng,
)

beta_chain = np.exp(chain.chain[100:, 0])

accept_rate = float(np.mean(chain.accepted))
print(f"MH accept rate: {accept_rate:.3f}")
print(
    "synthetic-lik beta: "
    f"mean={np.mean(beta_chain):.3f}, sd={np.std(beta_chain):.3f}, "
    f"q10={np.quantile(beta_chain, 0.1):.3f}, q90={np.quantile(beta_chain, 0.9):.3f}"
)
```

Notes:

- This is conceptually simple but **computationally heavy**: each MH step requires multiple simulator runs.
- In practice you would typically (i) use common random numbers, (ii) reuse simulations across nearby $\theta$, or (iii) move to more modern amortised SBI methods.

## Notes

- These baselines are intentionally simple and prioritise clarity over efficiency.
- Later chapters will revisit summary selection, amortisation, and calibration at scale.
