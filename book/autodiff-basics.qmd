---
title: "Automatic differentiation basics"
---

## Why autodiff matters for this handbook

Much of modern inference for epidemiological models relies on gradients (or approximations to gradients).
Automatic differentiation (autodiff) is the workhorse that makes gradients practical in high-dimensional
models and pipelines.

This chapter is intentionally **conceptual-first**. Runnable demos and helper code live in the next tasks
for milestone M6.

## Learning goals

After this chapter you should be able to:

- Distinguish *symbolic differentiation*, *numerical differentiation*, and *automatic differentiation*.
- Explain the difference between **forward-mode** (JVP) and **reverse-mode** (VJP) autodiff.
- Recognise when reverse-mode is a good fit (scalar loss, many parameters) and when forward-mode helps.
- Map epidemiology modelling steps (simulator, observation model, loss) to a computational graph.

## Autodiff in one picture (computational graphs)

Autodiff works by applying the chain rule to a program represented as a graph of elementary operations.
In practice, libraries like JAX, PyTorch, and TensorFlow build and/or trace these graphs for you.

### Notation

- Parameters: $\theta$
- Latent state trajectory (e.g. compartments over time): $x_{0:T}$
- Simulator: $x_{0:T} = \mathrm{Sim}(\theta)$
- Observations: $y \sim p(y \mid x_{0:T}, \theta)$
- Objective (loss / negative log-likelihood): $\mathcal{L}(\theta)$

## Forward-mode vs reverse-mode

### Forward-mode (JVP)

Forward-mode computes derivatives by pushing *tangent* information forward through the program.
It is often efficient when the input dimension is small.

### Reverse-mode (VJP)

Reverse-mode computes derivatives by first running the program forward (saving intermediate values)
and then pulling *adjoint* information backwards.
It is often efficient for scalar objectives with many parameters, which is common in inference.

## Runnable demos (JAX; optional dependency)

This repository keeps JAX as an **optional dependency**.
If you have it installed (see `pyproject.toml` extra `jax`), the cells below will run and produce
concrete numerical output.

### Demo 1: scalar function + gradient

```{python}
try:
    import matplotlib.pyplot as plt

    from diff_epi_inference.autodiff.demos import scalar_function_and_grad_demo

    res = scalar_function_and_grad_demo(n=200)
    print({"x_min": float(res.x_grid.min()), "x_max": float(res.x_grid.max())})

    fig, ax = plt.subplots(figsize=(7, 3))
    ax.plot(res.x_grid, res.f_vals, label="f(x)")
    ax.plot(res.x_grid, res.grad_vals, label="df/dx")
    ax.set_xlabel("x")
    ax.set_title("A tiny scalar function and its autodiff gradient")
    ax.legend(loc="best")
    plt.show()
except ImportError as e:
    print("Skipping JAX autodiff demos (optional deps not installed):", e)
```

### Demo 2: forward-mode JVP for a vector-valued function

```{python}
try:
    from diff_epi_inference.autodiff.demos import forward_mode_jvp_demo

    res = forward_mode_jvp_demo(m=20, x0=0.3)
    print({"x0": res.x0, "y_shape": res.y.shape, "jvp_shape": res.jvp.shape})
    print("First 5 outputs y:", res.y[:5].round(4).tolist())
    print("First 5 derivatives dy/dx:", res.jvp[:5].round(4).tolist())
except ImportError as e:
    print("Skipping JAX forward-mode demo (optional deps not installed):", e)
```

### Demo 3: reverse-mode gradient for a scalar loss with many parameters

```{python}
try:
    from diff_epi_inference.autodiff.demos import reverse_mode_vjp_demo

    res = reverse_mode_vjp_demo(dim=200, seed=0)
    print({"dim": res.dim, "loss0": res.loss0, "grad_norm": res.grad_norm})
except ImportError as e:
    print("Skipping JAX reverse-mode demo (optional deps not installed):", e)
```

## Practical checklist (to be expanded)

- Is your objective scalar? Reverse-mode is usually the default choice.
- Are you computing derivatives w.r.t. many outputs? Consider forward-mode or batching.
- Do you have control-flow (loops/branches)? Ensure your autodiff library supports the pattern.
- Are there non-differentiable ops (thresholds, argmax, discrete sampling)? See the next chapter.

## Where this goes next

- M6.1 will add runnable JAX demos for JVP/VJP intuition and small helper utilities.
- M6.2 will cover differentiability in simulators and what to do when gradients are unavailable.
