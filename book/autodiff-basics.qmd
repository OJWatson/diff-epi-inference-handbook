---
title: "Automatic differentiation basics"
---

## Why autodiff matters for this handbook

Much of modern inference for epidemiological models relies on gradients (or approximations to gradients).
Automatic differentiation (autodiff) is the workhorse that makes gradients practical in high-dimensional
models and pipelines.

This chapter is intentionally **conceptual-first**. Runnable demos and helper code are included below.
For deeper treatment, see [@griewank2008autodiff; @baydin2018autodiff].

## Learning goals

After this chapter you should be able to:

- Distinguish *symbolic differentiation*, *numerical differentiation*, and *automatic differentiation*.
- Explain the difference between **forward-mode** (JVP) and **reverse-mode** (VJP) autodiff.
- Recognise when reverse-mode is a good fit (scalar loss, many parameters) and when forward-mode helps.
- Map epidemiology modelling steps (simulator, observation model, loss) to a computational graph.

## Autodiff in one picture (computational graphs)

Autodiff works by applying the chain rule to a program represented as a graph of elementary operations.
In practice, libraries like JAX, PyTorch, and TensorFlow build and/or trace these graphs for you.

### Notation

- Parameters: $\theta$
- Latent state trajectory (e.g. compartments over time): $x_{0:T}$
- Simulator: $x_{0:T} = \mathrm{Sim}(\theta)$
- Observations: $y \sim p(y \mid x_{0:T}, \theta)$
- Objective (loss / negative log-likelihood): $\mathcal{L}(\theta)$

## Forward-mode vs reverse-mode

### Forward-mode (JVP)

Forward-mode computes derivatives by pushing *tangent* information forward through the program.
It is often efficient when the input dimension is small.

### Reverse-mode (VJP)

Reverse-mode computes derivatives by first running the program forward (saving intermediate values)
and then pulling *adjoint* information backwards.
It is often efficient for scalar objectives with many parameters, which is common in inference.

## Runnable demos (JAX path)

The cells below use JAX and produce concrete numerical output.

### Demo 1: scalar function + gradient

```{python}
import matplotlib.pyplot as plt

from diff_epi_inference.autodiff.demos import scalar_function_and_grad_demo

res = scalar_function_and_grad_demo(n=200)
print({"x_min": float(res.x_grid.min()), "x_max": float(res.x_grid.max())})

fig, ax = plt.subplots(figsize=(7, 3))
ax.plot(res.x_grid, res.f_vals, label="f(x)")
ax.plot(res.x_grid, res.grad_vals, label="df/dx")
ax.set_xlabel("x")
ax.set_title("A tiny scalar function and its autodiff gradient")
ax.legend(loc="best")
plt.show()
```

### Demo 2: forward-mode JVP for a vector-valued function

```{python}
from diff_epi_inference.autodiff.demos import forward_mode_jvp_demo

res = forward_mode_jvp_demo(m=20, x0=0.3)
print({"x0": res.x0, "y_shape": res.y.shape, "jvp_shape": res.jvp.shape})
print("First 5 outputs y:", res.y[:5].round(4).tolist())
print("First 5 derivatives dy/dx:", res.jvp[:5].round(4).tolist())
```

### Demo 3: reverse-mode gradient for a scalar loss with many parameters

```{python}
from diff_epi_inference.autodiff.demos import reverse_mode_vjp_demo

res = reverse_mode_vjp_demo(dim=200, seed=0)
print({"dim": res.dim, "loss0": res.loss0, "grad_norm": res.grad_norm})
```

## Practical checklist

- Is your objective scalar? Reverse-mode is usually the default choice.
- Are you computing derivatives w.r.t. many outputs? Consider forward-mode or batching.
- Do you have control-flow (loops/branches)? Ensure your autodiff library supports the pattern.
- Are there non-differentiable ops (thresholds, argmax, discrete sampling)? See the next chapter.

## Key takeaways

- Forward-mode and reverse-mode both apply the chain rule; they differ in traversal direction and cost profile.
- Reverse-mode is usually the default for scalar objectives with many parameters.
- Autodiff is only as useful as the differentiability of your modeling pipeline.
