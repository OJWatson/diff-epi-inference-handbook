---
title: "Gradient estimators & relaxations"
---

Many inference and learning methods in differentiable epidemiology rely on gradients.
When your model has **discrete** components (discrete latent states, interventions, thresholds, reporting rules), naive differentiation breaks.

This chapter collects a few practical tools:

- **Score-function gradients (REINFORCE)** — general but often high-variance.
- **Pathwise / reparameterisation gradients** — low variance when available.
- **Relaxations** such as **Gumbel–Softmax** and **straight-through** estimators — heuristics that make optimisation possible even when the underlying model is discrete.

::: {.callout-warning}
These estimators and relaxations are *not magic*.
Always validate with posterior predictive checks / calibration, and expect sensitivity to temperature schedules, baselines, and parameterisation.
:::

## 0) Finite differences (fragile)

A first impulse is to approximate a gradient with finite differences:

$$
\frac{\partial}{\partial \phi}\,\mathbb{E}[f(z)]
\approx \frac{\widehat{\mathbb{E}}_{\phi+\epsilon}[f(z)]-\widehat{\mathbb{E}}_{\phi-\epsilon}[f(z)]}{2\epsilon}.
$$

This can work for smooth, low-noise problems, but it is often *fragile*:

- you must choose a step size $\epsilon$;
- Monte Carlo noise can dominate the difference;
- for discontinuous / discrete models, the estimator can be extremely unstable.

### Demo: bias/variance trade-off on a toy Bernoulli “infection event”

We estimate the gradient of $\mathbb{E}[z]$ where $z\sim \mathrm{Bernoulli}(\sigma(\phi))$.
The true gradient is $\sigma(\phi)(1-\sigma(\phi))$.

We compare three estimators:

- finite differences (MC)
- REINFORCE (score-function)
- a *soft relaxation* (Binary Concrete) with a pathwise derivative (biased at finite temperature)

```{python}
import numpy as np
import matplotlib.pyplot as plt

from diff_epi_inference.gradients import bernoulli_concrete, reinforce_grad_logit_bernoulli

rng = np.random.default_rng(0)
logit = 0.3
p = 1.0 / (1.0 + np.exp(-logit))
true_grad = p * (1 - p)

def mc_mean_z(logit_value: float, *, rng: np.random.Generator, n: int) -> float:
    p = 1.0 / (1.0 + np.exp(-logit_value))
    z = rng.binomial(n=1, p=p, size=n).astype(float)
    return float(np.mean(z))

n_samples = 400
n_rep = 300

eps = 1e-2
tau = 0.7

fd = []
reinforce = []
relaxed_pathwise = []

for _ in range(n_rep):
    # 1) finite differences with two independent MC estimates
    m_plus = mc_mean_z(logit + eps, rng=rng, n=n_samples)
    m_minus = mc_mean_z(logit - eps, rng=rng, n=n_samples)
    fd.append((m_plus - m_minus) / (2 * eps))

    # 2) REINFORCE: unbiased but high variance
    r = reinforce_grad_logit_bernoulli(lambda z: z, logit=logit, rng=rng, n_samples=n_samples)
    reinforce.append(r.grad)

    # 3) Binary Concrete relaxation: differentiable sample y in [0, 1]
    y = bernoulli_concrete(logit, temperature=tau, rng=rng)
    # pathwise derivative dy/dlogit for y = sigmoid((logit + noise)/tau)
    relaxed_pathwise.append(float(y * (1 - y) / tau))

fig, ax = plt.subplots(figsize=(7, 3.5))
ax.boxplot([fd, reinforce, relaxed_pathwise], labels=["finite diff", "REINFORCE", f"relax (tau={tau})"], showfliers=False)
ax.axhline(true_grad, color="black", linestyle="--", linewidth=1, label="true")
ax.set_ylabel("gradient estimate")
ax.set_title(f"Toy Bernoulli gradient at logit={logit} (n={n_samples}, reps={n_rep})")
ax.legend(loc="upper right")
plt.show()

np.mean(fd), np.mean(reinforce), np.mean(relaxed_pathwise), true_grad
```

In practice:

- finite differences can be *wildly noisy* unless you spend a lot of compute;
- REINFORCE is unbiased but may require careful baselines / variance reduction;
- relaxations can give stable gradients, but they change the model (bias).

## 1) Score-function gradients (REINFORCE)

Suppose we want gradients of an expectation

$$
\nabla_\phi\; \mathbb{E}_{z\sim p_\phi(z)}[f(z)]
$$

where $z$ is discrete.
We can use the identity

$$
\nabla_\phi\; \mathbb{E}[f(z)] = \mathbb{E}\bigl[f(z)\,\nabla_\phi \log p_\phi(z)\bigr].
$$

For a Bernoulli variable parameterised by a **logit** $\phi$,

$$
\nabla_\phi \log p_\phi(z) = z - \sigma(\phi).
$$

### Minimal demo: Bernoulli REINFORCE (NumPy)

```{python}
import numpy as np

from diff_epi_inference.gradients import reinforce_grad_logit_bernoulli

rng = np.random.default_rng(0)
logit = 0.3

# f(z)=z  => E[z]=p  => d/dlogit E[z] = p(1-p)
res = reinforce_grad_logit_bernoulli(lambda z: z, logit=logit, rng=rng, n_samples=50_000)

p = 1.0 / (1.0 + np.exp(-logit))
res.grad, p * (1 - p)
```

### Variance reduction: baselines

A common control variate is to subtract a baseline $b$:

$$
\mathbb{E}[(f(z)-b)\nabla_\phi\log p_\phi(z)]
$$

Any baseline that does **not** depend on $z$ keeps the estimator unbiased and can drastically reduce variance.

(Our tiny implementation uses the sample mean of $f(z)$ as a baseline option.)

## 2) Pathwise (reparameterisation) gradients

When randomness can be expressed as a deterministic transform of parameter-free noise,

$$
\epsilon\sim p(\epsilon),\qquad z = g(\phi,\epsilon),
$$

we can differentiate through $g$ and estimate

$$
\nabla_\phi\,\mathbb{E}[f(g(\phi,\epsilon))]
\approx \frac{1}{S}\sum_{s=1}^S \nabla_\phi f(g(\phi,\epsilon_s)).
$$

This usually has much lower variance than REINFORCE.

In epidemiology, this is most directly applicable when your simulator is differentiable and the noise is reparameterisable (e.g. Gaussian noise models).

## 3) Relaxations for categorical / discrete choices

### Gumbel–Softmax (Concrete)

For a categorical variable with logits $\ell\in\mathbb{R}^K$, Gumbel–Softmax draws a **soft one-hot** vector on the simplex.
As temperature $\tau\to 0$, samples become closer to one-hot.

For $K=2$ (a Bernoulli choice), this reduces to the **Binary Concrete** (a soft sample in $[0,1]$).

```{python}
import numpy as np

from diff_epi_inference.gradients import gumbel_softmax

rng = np.random.default_rng(0)
logits = np.array([0.0, 1.0, -0.5])

y = gumbel_softmax(logits, temperature=0.7, rng=rng)
float(np.sum(y)), y
```

### Straight-through estimators (heuristic)

A common practical trick is:

- **forward pass:** use a hard discrete sample (argmax / threshold)
- **backward pass:** pretend we used the soft sample

This enables optimisation but is biased.

## Practical checklist

When you reach for gradient estimators / relaxations:

1. Can you rewrite the randomness in a reparameterised form (pathwise gradient)?
2. If you must use REINFORCE, can you add an effective baseline (and maybe Rao–Blackwellise parts)?
3. If using Gumbel–Softmax:
   - choose a temperature schedule,
   - check sensitivity to $\tau$,
   - validate downstream metrics (not just loss curves).
4. Always test for *pathological gradients*: exploding variance, vanishing gradients at low temperatures, or optimisation instability.
