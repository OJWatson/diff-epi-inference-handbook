---
title: "Gradient estimators & relaxations"
---

Many inference and learning methods in differentiable epidemiology rely on gradients.
When your model has **discrete** components (discrete latent states, interventions, thresholds, reporting rules), naive differentiation breaks.

This chapter collects a few practical tools:

- **Score-function gradients (REINFORCE)** — general but often high-variance.
- **Pathwise / reparameterisation gradients** — low variance when available.
- **Relaxations** such as **Gumbel–Softmax** and **straight-through** estimators — heuristics that make optimisation possible even when the underlying model is discrete.

::: {.callout-warning}
These estimators and relaxations are *not magic*.
Always validate with posterior predictive checks / calibration, and expect sensitivity to temperature schedules, baselines, and parameterisation.
:::

## 1) Score-function gradients (REINFORCE)

Suppose we want gradients of an expectation

$$
\nabla_\phi\; \mathbb{E}_{z\sim p_\phi(z)}[f(z)]
$$

where $z$ is discrete.
We can use the identity

$$
\nabla_\phi\; \mathbb{E}[f(z)] = \mathbb{E}\bigl[f(z)\,\nabla_\phi \log p_\phi(z)\bigr].
$$

For a Bernoulli variable parameterised by a **logit** $\phi$,

$$
\nabla_\phi \log p_\phi(z) = z - \sigma(\phi).
$$

### Minimal demo: Bernoulli REINFORCE (NumPy)

```{python}
import numpy as np

from diff_epi_inference.gradients import reinforce_grad_logit_bernoulli

rng = np.random.default_rng(0)
logit = 0.3

# f(z)=z  => E[z]=p  => d/dlogit E[z] = p(1-p)
res = reinforce_grad_logit_bernoulli(lambda z: z, logit=logit, rng=rng, n_samples=50_000)

p = 1.0 / (1.0 + np.exp(-logit))
res.grad, p * (1 - p)
```

### Variance reduction: baselines

A common control variate is to subtract a baseline $b$:

$$
\mathbb{E}[(f(z)-b)\nabla_\phi\log p_\phi(z)]
$$

Any baseline that does **not** depend on $z$ keeps the estimator unbiased and can drastically reduce variance.

(Our tiny implementation uses the sample mean of $f(z)$ as a baseline option.)

## 2) Pathwise (reparameterisation) gradients

When randomness can be expressed as a deterministic transform of parameter-free noise,

$$
\epsilon\sim p(\epsilon),\qquad z = g(\phi,\epsilon),
$$

we can differentiate through $g$ and estimate

$$
\nabla_\phi\,\mathbb{E}[f(g(\phi,\epsilon))]
\approx \frac{1}{S}\sum_{s=1}^S \nabla_\phi f(g(\phi,\epsilon_s)).
$$

This usually has much lower variance than REINFORCE.

In epidemiology, this is most directly applicable when your simulator is differentiable and the noise is reparameterisable (e.g. Gaussian noise models).

## 3) Relaxations for categorical / discrete choices

### Gumbel–Softmax (Concrete)

For a categorical variable with logits $\ell\in\mathbb{R}^K$, Gumbel–Softmax draws a **soft one-hot** vector on the simplex.
As temperature $\tau\to 0$, samples become closer to one-hot.

```{python}
import numpy as np

from diff_epi_inference.gradients import gumbel_softmax

rng = np.random.default_rng(0)
logits = np.array([0.0, 1.0, -0.5])

y = gumbel_softmax(logits, temperature=0.7, rng=rng)
float(np.sum(y)), y
```

### Straight-through estimators (heuristic)

A common practical trick is:

- **forward pass:** use a hard discrete sample (argmax / threshold)
- **backward pass:** pretend we used the soft sample

This enables optimisation but is biased.

## Practical checklist

When you reach for gradient estimators / relaxations:

1. Can you rewrite the randomness in a reparameterised form (pathwise gradient)?
2. If you must use REINFORCE, can you add an effective baseline (and maybe Rao–Blackwellise parts)?
3. If using Gumbel–Softmax:
   - choose a temperature schedule,
   - check sensitivity to $\tau$,
   - validate downstream metrics (not just loss curves).
4. Always test for *pathological gradients*: exploding variance, vanishing gradients at low temperatures, or optimisation instability.
